<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free@6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"cita-cloud.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="CITAHub技术团队">
<meta property="og:url" content="https://cita-cloud.github.io/index.html">
<meta property="og:site_name" content="CITAHub技术团队">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="CITAHub">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://cita-cloud.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CITAHub技术团队</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">CITAHub技术团队</h1>
      <i class="logo-line"></i>
    </a>
      <img class="custom-logo-image" src="/uploads/citahub.jpg" alt="CITAHub技术团队">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">CITAHub</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">27</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/citahub" title="CITAHub仓库 → https:&#x2F;&#x2F;github.com&#x2F;citahub" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>CITAHub仓库</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/cita-cloud" title="CITA-Cloud仓库 → https:&#x2F;&#x2F;github.com&#x2F;cita-cloud" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>CITA-Cloud仓库</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:contact@rivtower.com" title="E-Mail → mailto:contact@rivtower.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://talk.citahub.com/" title="论坛 → https:&#x2F;&#x2F;talk.citahub.com" rel="noopener me" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>论坛</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://docs.citahub.com/zh-CN/welcome" title="CITAHub文档 → https:&#x2F;&#x2F;docs.citahub.com&#x2F;zh-CN&#x2F;welcome" rel="noopener me" target="_blank"><i class="fa fa-book fa-fw"></i>CITAHub文档</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://cita-cloud-docs.readthedocs.io/zh_CN/latest/" title="CITA-Cloud文档 → https:&#x2F;&#x2F;cita-cloud-docs.readthedocs.io&#x2F;zh_CN&#x2F;latest&#x2F;" rel="noopener me" target="_blank"><i class="fa fa-book fa-fw"></i>CITA-Cloud文档</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://cita-cloud.github.io/run-CITA-Cloud-on-object-storage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CITAHub">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/run-CITA-Cloud-on-object-storage/" class="post-title-link" itemprop="url">在对象存储上运行CITA-Cloud</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-12-30 10:12:40" itemprop="dateCreated datePublished" datetime="2022-12-30T10:12:40+08:00">2022-12-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Object-Storage/" itemprop="url" rel="index"><span itemprop="name">Object Storage</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>973</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="对象存储与生态"><a href="#对象存储与生态" class="headerlink" title="对象存储与生态"></a>对象存储与生态</h2><p>说起对象存储，不得不提Amazon的AWS S3(Simple Storage Service).</p>
<p>进入到21世纪，数据急剧增长，当时已经是电商巨头的Amazon需要一种海量的、可扩展的、支持非结构化数据且对开发友好的网络存储。2006年，S3云服务应运而生。</p>
<p>从那之后，各个云厂商也随之跟进，发布了自己的对象存储服务。</p>
<p>S3也不仅仅代表了一种云服务，也成为了对象存储业界的一种协议。</p>
<p>当前，很多云产品也都构建在S3之上，围绕对象存储基础设施，构建了庞大的生态。除了云商巨头，像Snowflake、Databricks这些明星data infra公司，更是离不开对象存储。</p>
<p>国内近年来也有基于对象存储创业的公司，如兼容S3协议的JuiceFS，新式云仓Databend。</p>
<p>从最几年的趋势来看，S3或者说对象存储俨然已经成为云上数据湖的基础。</p>
<h2 id="使用JuiceFS让CITA-Cloud跑在对象存储上"><a href="#使用JuiceFS让CITA-Cloud跑在对象存储上" class="headerlink" title="使用JuiceFS让CITA-Cloud跑在对象存储上"></a>使用JuiceFS让CITA-Cloud跑在对象存储上</h2><p>对象存储普遍用于存放图片、音视频等静态文件。</p>
<p>那么，既然作为存储的一种，CITA-Cloud能否直接运行在对象存储之上，而不占用本地空间呢？答案是肯定的，下面我们演示下怎么使用JuiceFS让CITA-Cloud跑在对象存储上。</p>
<h3 id="JuiceFS简介与使用"><a href="#JuiceFS简介与使用" class="headerlink" title="JuiceFS简介与使用"></a>JuiceFS简介与使用</h3><p>JuiceFS是为云环境设计，兼容 POSIX、HDFS 和 S3 协议的分布式文件系统，具体介绍可以查看<a target="_blank" rel="noopener" href="https://juicefs.com/zh-cn/">官网</a></p>
<p>在我看来，JuiceFS能让对象存储上的一个Bucket变成一块大的云盘，挂载在本地，让应用能像读取本地文件一样来操作对象存储上的对象。</p>
<p>JuiceFS主要分为三部分：JuiceFS客户端、数据存储(公有云对象存储&#x2F;MinIO等)、元数据引擎(Redis&#x2F;SQLite等)。</p>
<p>这里我们使用Redis作为元数据引擎。JuiceFS提供了针对Kubernetes环境的CSI，并提供了Helm的安装方式。</p>
<p>JuiceFS安装时需要一个存储来保存对应文件系统的元数据，这里我们使用Redis；后端对象存储使用MinIO。</p>
<p>Helm安装时的配置如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">storageClasses:</span><br><span class="line">- # -- `StorageClass` Name. It is important.</span><br><span class="line">  name: juicefs-sc</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">-- Default is `<span class="literal">true</span>` will create a new `StorageClass`. It will create `Secret` and `StorageClass` used by CSI Driver.</span></span><br><span class="line">  enabled: true</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">-- Either `Delete` or `Retain`. Refer to [this document](https://juicefs.com/docs/csi/guide/resource-optimization<span class="comment">#reclaim-policy) for more information.</span></span></span><br><span class="line">  reclaimPolicy: Delete</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">-- Additional annotations <span class="keyword">for</span> this `StorageClass`, e.g. make it default.</span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">annotations:</span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">  storageclass.kubernetes.io/is-default-class: <span class="string">&quot;true&quot;</span></span></span><br><span class="line">  backend:</span><br><span class="line">    # -- The JuiceFS file system name</span><br><span class="line">    name: &quot;juicefs&quot;</span><br><span class="line">    # -- Connection URL for metadata engine (e.g. Redis), for community edition use only. Refer to [this document](https://juicefs.com/docs/community/databases_for_metadata) for more information.</span><br><span class="line">    metaurl: &quot;redis://:123456@redis-service.default:6379/1&quot;</span><br><span class="line">    # -- Object storage type, such as `s3`, `gs`, `oss`, for community edition use only. Refer to [this document](https://juicefs.com/docs/community/how_to_setup_object_storage) for the full supported list.</span><br><span class="line">    storage: &quot;minio&quot;</span><br><span class="line">    # -- Bucket URL, for community edition use only. Refer to [this document](https://juicefs.com/docs/community/how_to_setup_object_storage) to learn how to setup different object storage.</span><br><span class="line">    bucket: &quot;http://minio.zhujq:9000/juicefs&quot;</span><br><span class="line">    # -- JuiceFS managed token, for cloud service use only. Refer to [this document](https://juicefs.com/docs/cloud/acl) for more details.</span><br><span class="line">    token: &quot;&quot;</span><br><span class="line">    # -- Access key for object storage</span><br><span class="line">    accessKey: &quot;minio&quot;</span><br><span class="line">    # -- Secret key for object storage</span><br><span class="line">    secretKey: &quot;minio123&quot;</span><br></pre></td></tr></table></figure>

<p>安装完成之后，可以看到多了一个名为<code>juicefs-sc</code>的StorageClass，在我的集群中如下所示：<br><img src="/run-CITA-Cloud-on-object-storage/juice_sc.jpg"></p>
<h3 id="创建一条链"><a href="#创建一条链" class="headerlink" title="创建一条链"></a>创建一条链</h3><p>有了相应的StorageClass，我们可以用Cloud-Config来创建一条链，创建时指定StorageClass参数为juicefs-sc。</p>
<p>可以看到一条4个节点的链已经运行起来了，并且能正常出块：<br><img src="/run-CITA-Cloud-on-object-storage/chain_pod.jpg"><br>同时创建了对应的PVC:<br><img src="/run-CITA-Cloud-on-object-storage/juicefs_pvc.jpg"></p>
<p>在MinIO界面上，我们能看到对应的名为juice的bucket被创建，链节点的数据文件被分割为多个chunk进行存储：</p>
<p><img src="/run-CITA-Cloud-on-object-storage/minio.jpg"></p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>对象存储作为一种相对廉价的海量存储，在强调降本增效的今天来看，越来越能体现它的重要性。</p>
<p>当然，上面的例子只是一个实验，想要在生产环境使用，需要做完善的测试和评估，相信将来会有更多能与对象存储一起结合的场景。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://cita-cloud.github.io/chat-from-controller-pattern/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CITAHub">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/chat-from-controller-pattern/" class="post-title-link" itemprop="url">聊一聊控制器模式</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-08-09 09:40:15" itemprop="dateCreated datePublished" datetime="2022-08-09T09:40:15+08:00">2022-08-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="开头"><a href="#开头" class="headerlink" title="开头"></a>开头</h2><p>容器的出现和k8s的普及已经改变了我们传统的运维方式，这些技术给我们带来了更多的资源利用率和更强的容错能力。</p>
<p>这其中控制器模式必然占据了举足轻重的作用。这种设计模式是开发和扩展Kubernetes的核心，在Kubernetes的代码仓库中能到处看到它的身影。</p>
<h2 id="声明式or命令式"><a href="#声明式or命令式" class="headerlink" title="声明式or命令式"></a>声明式or命令式</h2><p>当然，当提到控制器模式时候，你也一定会看到声明式API这种东西，这两种一定是同时搭配使用的。</p>
<p>再之，提到“声明式”，你也必然会在大量的文章中看到“命令式”与“声明式”的比较。这些文章一般会说：</p>
<blockquote>
<ol>
<li>与命令性API相比，声明性API的使用更加简洁，并且提供了更好的抽象性。</li>
<li>“命令式”强调的是“how”，你必须step-by-step告诉计算机如何完成一项工作（类似自己做菜）。“声明式”只需要告诉计算机你想要什么，声明你的”what”，计算机会为你完成具体的工作（类似于去饭店点菜吃饭）</li>
</ol>
</blockquote>
<p>还有的教程上会说：</p>
<blockquote>
<p>“声明式 API”允许有多个 API 写端，以 PATCH 的方式对 API 对象进行修改，而无需关心本地原始 YAML 文件的内容。</p>
</blockquote>
<p>在我看来，声明式能够提供所谓的更简洁的API，且有Patch的能力，<strong>这些并不是我们常用的命令式API所不能做到的</strong>。</p>
<p>如果你觉得API不够简洁抽象，那你应该去考虑你设计上是否存在问题，或者没按照规范来？</p>
<p>想要命令式API具有Patch的能力，那可以把资源的属性全部传过去，让你的接口具有类似compare and update的能力不就行了？</p>
<p>所以，不管声明式还是命令式，我觉得核心还是在背后处理API的方式上的不同，这种处理方式通常叫做”控制器模式”。</p>
<h2 id="云平台的困境"><a href="#云平台的困境" class="headerlink" title="云平台的困境"></a>云平台的困境</h2><p>在之前的工作中，我们基于IaaS平台来实现业务。与web服务不同的是，IaaS平台存在更多的耗时请求，且请求过程中出现报错、超时等错误更是家常便饭。</p>
<p>不管是自研IaaS还是二次开发，针对一个多阶段的耗时请求，为了保证发生异常时不存在垃圾资源，通常会实现与正向操作相反的方法，我们称之为rollback方法。</p>
<p>就拿创建虚拟机实例的例子来说，一个createVmInstance操作通常会有很多阶段(正向)：</p>
<p>[AllocateVolume -&gt; AllocateNic -&gt; CreateOnHypervisor…]</p>
<p>这里的每一步都会报错，那么为了报错之后保持环境干净，你必须提供相应的回滚操作(反向)：</p>
<p>[DeleteOnHypervisor -&gt; DeleteNic -&gt; DeleteVolume]</p>
<p>这里通常会设计成一个Stack，当执行一个正向操作时，将相应的反向操作压栈；当发生错误时对栈进行Pop，依次执行回滚操作。</p>
<p>有点像<strong>Saga的分布式事务</strong>。</p>
<p>但是这里会有个几个问题：</p>
<ol>
<li><p>我在执行回滚函数时又发生了报错该怎么办？这时候可能就需要定时GC相关的逻辑来处理垃圾资源。然后，用户通过重试请求createVmInstance的方式再次创建，循环往复。</p>
</li>
<li><p>假如vm创建成功，其下一个子资源被人误删(虽然很多时候会报device is busy删除失败)，那这个vm只能一直处于错误状态了。</p>
</li>
</ol>
<h2 id="控制器模式"><a href="#控制器模式" class="headerlink" title="控制器模式"></a>控制器模式</h2><p>控制器模式可以很好地解决上面解决上面这两个问题。</p>
<p>下面以k8s中的控制器来说明。</p>
<p>Kubernetes控制器会追踪一个或多个资源，并且将资源描述成如下结构：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Object <span class="keyword">struct</span> &#123;</span><br><span class="line">    metav1.TypeMeta</span><br><span class="line">    metav1.ObjectMeta</span><br><span class="line">    Spec ObjectSpec</span><br><span class="line">    Status ObjectStatus</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>除了一些必要的元数据外，还有两个字段: Spec(期望)和Status(当前状态)。控制器不断监控该资源，比较Spec的定义和当前的实际环境，在一个调谐循环中尝试将当前实际环境变换成期望的状态。伪代码如下：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">  desired := getDesiredState()</span><br><span class="line">  current := getCurrentState()</span><br><span class="line">  makeChanges(desired, current)</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<p>这也就是我们常说的最终一致性，当整个系统恢复正常时，总能给你一个你想要的状态的资源，哪怕你中间误删了一些子资源。</p>
<p>在Kubernetes中，主要通过Informer机制来实现，它作为客户端被使用在每一个Kubernetes的控制器逻辑中。<br>其中，为了保证事件不丢失，实现了list-watch机制， cache机制减少了对apiserver的请求压力，还有限速队列等保护机制等，这里不再展开叙述。</p>
<h2 id="现实中的例子"><a href="#现实中的例子" class="headerlink" title="现实中的例子"></a>现实中的例子</h2><p>控制器模式让我想起了我大学时的自动化专业，在自动控制领域中，闭环的负反馈控制系统如下所示：</p>
<p><img src="/chat-from-controller-pattern/auto_controller.png"></p>
<p>可以看到有类似的地方：通过不断地比较输入信号和反馈信号，再经过相应的算法（如经典的PID算法），以达到输出信号和输入信号趋于一致的状态。</p>
<p>这些控制器的例子在生活中比比皆是：空调温湿度调节，电机转速等。</p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>在分布式环境下，错误永远不能避免，如果你想让你的软件达到Always On的效果，可能就需要引入控制器模式。当然，基于Kubernetes之上运行的大多数软件，已经不需要再考虑这些问题了。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://cita-cloud.github.io/grpc-retry-in-rust/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CITAHub">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/grpc-retry-in-rust/" class="post-title-link" itemprop="url">在Rust中实现gRPC重试</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-10 12:53:28" itemprop="dateCreated datePublished" datetime="2022-07-10T12:53:28+08:00">2022-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Rust/" itemprop="url" rel="index"><span itemprop="name">Rust</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p><code>CITA-Cloud</code>采用了微服务架构，微服务之间以及对应用暴露的接口都采用了<code>gRPC</code>。</p>
<p><code>gRPC</code>调用时可能会返回错误，需要对错误进行处理。经过讨论之后，我们采用的<a target="_blank" rel="noopener" href="https://github.com/cita-cloud/rfcs/pull/7/files">方案</a>是将错误码分成两层。应用层面的错误用单独的<code>status_code</code>来表示；<code>gRPC</code>本身的错误会用响应中的<code>Status</code>来表示。</p>
<p>前段时间碰到了可能是网络抖动造成的客户端返回<code>UNAVAILABLE</code>的现象。因为这个是<code>gRPC</code>本身的错误，应用层没办法处理，只能靠客户端重试来解决。</p>
<h2 id="gRPC重试"><a href="#gRPC重试" class="headerlink" title="gRPC重试"></a>gRPC重试</h2><p>针对这个需求，可以使用<code>gRPC</code>本身就提供的拦截器功能。通过注入一个拦截器，检查每次调用的结果，如果返回错误（并不是每种错误都可以重试的，具体参见官方关于<a target="_blank" rel="noopener" href="https://grpc.github.io/grpc/core/md_doc_statuscodes.html">错误码的描述</a>），则再次发起调用。</p>
<p>在<code>golang</code>这样的亲儿子上，甚至已经有<a target="_blank" rel="noopener" href="https://github.com/grpc-ecosystem/go-grpc-middleware/tree/master/retry">现成的库</a>可以非常方便的做到这样的事情。</p>
<h2 id="gRPC重试-in-Rust"><a href="#gRPC重试-in-Rust" class="headerlink" title="gRPC重试 in Rust"></a>gRPC重试 in Rust</h2><p>因为<code>CITA-Cloud</code>主要使用<code>Rust</code>，结果搜索了一圈，震惊的发现竟然没有现成的库。在<code>Rust</code>这样造轮子情绪高涨的社区里，这是一个很不寻常的情况。</p>
<p>所幸一番搜索之后，发现了相应的<a target="_blank" rel="noopener" href="https://github.com/hyperium/tonic/issues/733">原因</a>，还是跟<code>Rust</code>的所有权特性有关系。</p>
<p>因为要在失败后重试，就要复制一份调用的请求参数。这个在其他语言里面根本不是个事，但是在<code>Rust</code>里就麻烦了。</p>
<p><code>tonic</code>(一个纯<code>Rust</code>的<code>gRPC</code>实现)中一个接口的客户端函数原型为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pub async fn store(</span><br><span class="line">            &amp;mut self,</span><br><span class="line">            request: impl tonic::IntoRequest&lt;super::Content&gt;,</span><br><span class="line">        ) -&gt; Result&lt;tonic::Response&lt;super::super::common::StatusCode&gt;, tonic::Status&gt;</span><br></pre></td></tr></table></figure>

<p>请求的类型是<code>tonic::IntoRequest&lt;T&gt;</code>（其中的<code>T</code>为请求中的应用层数据结构），这个类型是没有实现<code>Clone</code>的。</p>
<p>至于为什么不实现，开发者的解释是要考虑到<code>gRPC</code>的<code>stream</code>模式，<code>stream</code>中的请求是没法<code>Clone</code>的。</p>
<p>那非<code>stream</code>模式可以实现吗？答案也是不行，因为<code>gRPC</code>是基于<code>Http2</code>的，<code>Http2</code>总是<code>stream</code>的，因此单次调用模式其实就是只包含一个请求的<code>stream</code>。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>请求的类型<code>tonic::IntoRequest&lt;T&gt;</code>无法<code>Clone</code>，但是里面的<code>T</code>通常都是可以<code>Clone</code>的。</p>
<p>因此在<code>Rust</code>中像<code>golang</code>一样通过拦截器来非常优雅的实现重试是做不到了，但是用复杂一点的方法还是可以实现的。</p>
<p>其实说白了就是在应用层，按照最直接的方式来实现重试。在应用层多封装一层函数，其参数是应用层的请求类型<code>T</code>。调用接口之后，判断结果，如果是可重试的错误，则将类型<code>T</code>复制一份，重新发起调用。</p>
<p>当然这样实现的问题是重复的模式化的代码会非常多，所以具体实现还是用了一些技巧尽量让重复的代码少一点。</p>
<p>方案参考了<a target="_blank" rel="noopener" href="https://github.com/temporalio/sdk-core/tree/master/client/src">temporalio&#x2F;sdk-core</a>，具体实现参见<a target="_blank" rel="noopener" href="https://github.com/cita-cloud/cita_cloud_proto/pull/4/files">代码</a>。</p>
<p>为了复用<code>retry</code>的逻辑，单独抽象出了<code>retry</code>模块。首先定义了<code>RetryClient</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pub struct RetryClient&lt;SG&gt; &#123;</span><br><span class="line">    client: SG,</span><br><span class="line">    retry_config: RetryConfig,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>client</code>是原始的<code>gRPC client</code>，<code>retry_config</code>是重试相关的选项，比如最多重试多少次等。</p>
<p>重试的逻辑在其成员方法<code>call_with_retry</code>中，里面主要用到了<code>FutureRetry</code>，即把整个调用封装成一个<code>Future</code>闭包，退避策略则使用了<code>ExponentialBackoff</code>。</p>
<p>当然最根本的还是前面提到的，要封装一层，使闭包的参数是可以<code>Clone</code>的。这部分都是一些模式化的代码，因此使用了一个宏来自动生成相关代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">macro_rules! retry_call &#123;</span><br><span class="line">    ($myself:ident, $call_name:ident) =&gt; &#123; retry_call!($myself, $call_name,) &#125;;</span><br><span class="line">    ($myself:ident, $call_name:ident, $($args:expr),*) =&gt; &#123;&#123;</span><br><span class="line">        let call_name_str = stringify!($call_name);</span><br><span class="line">        let fact = || &#123; async &#123; $myself.get_client_clone().$call_name($($args,)*).await.map(|ret| ret.into_inner()) &#125;&#125;;</span><br><span class="line">        $myself.call_with_retry(fact, call_name_str).await</span><br><span class="line">    &#125;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>为了让<code>RetryClient</code>能够用于不同的<code>Service</code>，这里会把每个<code>Service</code>的客户端函数定义成一个<code>Trait</code>。比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#[async_trait::async_trait]</span><br><span class="line">pub trait StorageClientTrait &#123;</span><br><span class="line">    async fn store(&amp;self, content: storage::Content) -&gt; Result&lt;common::StatusCode, tonic::Status&gt;;</span><br><span class="line"></span><br><span class="line">    async fn load(&amp;self, key: storage::ExtKey) -&gt; Result&lt;storage::Value, tonic::Status&gt;;</span><br><span class="line"></span><br><span class="line">    async fn delete(&amp;self, key: storage::ExtKey) -&gt; Result&lt;common::StatusCode, tonic::Status&gt;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意这里的函数原型是封装之后的。</p>
<p>然后为<code>RetryClient</code>相对应的特化类型实现这个<code>Trait</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#[async_trait::async_trait]</span><br><span class="line">impl StorageClientTrait for RetryClient&lt;StorageServiceClient&lt;InterceptedSvc&gt;&gt; &#123;</span><br><span class="line">    async fn store(&amp;self, content: storage::Content) -&gt; Result&lt;common::StatusCode, tonic::Status&gt; &#123;</span><br><span class="line">        retry_call!(self, store, content.clone())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    async fn load(&amp;self, key: storage::ExtKey) -&gt; Result&lt;storage::Value, tonic::Status&gt; &#123;</span><br><span class="line">        retry_call!(self, load, key.clone())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    async fn delete(&amp;self, key: storage::ExtKey) -&gt; Result&lt;common::StatusCode, tonic::Status&gt; &#123;</span><br><span class="line">        retry_call!(self, delete, key.clone())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>内容是完全使用前面的宏来实现的。第一个参数是<code>RetryClient</code>的<code>self</code>，第二个参数是<code>gRPC</code>接口的名称，后面是接口的参数。</p>
<p>这样就实现了一个尽量通用的<code>RetryClient</code>，然后以尽量少的重复代码来为多个<code>Service</code>都实现了重试的功能。</p>
<p>用法可以参见里面的测试代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">let mock_client = TestClient::new(code);</span><br><span class="line">let retry_client = RetryClient::new(mock_client, Default::default());</span><br><span class="line">let result = retry_client.test(1).await;</span><br></pre></td></tr></table></figure>
<p>首先按照原有的方法获取底层的<code>Client</code>；然后将其和<code>RetryConfig</code>一起放入<code>RetryClient</code>，得到带重试功能的客户端；用这个客户端调用前述<code>Trait</code>中封装的方法就会自带重试功能了。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://cita-cloud.github.io/k8s-service-iptables/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CITAHub">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/k8s-service-iptables/" class="post-title-link" itemprop="url">Kubernetes中的负载均衡原理————以iptables模式为例</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-07 17:45:14" itemprop="dateCreated datePublished" datetime="2022-06-07T17:45:14+08:00">2022-06-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="什么是负载均衡"><a href="#什么是负载均衡" class="headerlink" title="什么是负载均衡"></a>什么是负载均衡</h2><p>负载均衡是高可用架构中的一个关键组件，他可以将请求平摊到后端服务上。</p>
<p>在单副本应用时代，你的服务负载可能并不高，这个时候负载均衡并没有多大用处。当你的应用火爆，用户请求数量多时，你需要将你的服务从单副本转变为多副本，那这个时候，是很有必要引入负载均衡的。</p>
<p>特别是当前微服务盛行的时代，一个应用多副本的高可用形态随处可见，负载均衡已经是个不可或缺的组件了。</p>
<h2 id="Kubernetes中的Service介绍"><a href="#Kubernetes中的Service介绍" class="headerlink" title="Kubernetes中的Service介绍"></a>Kubernetes中的Service介绍</h2><p>我们知道，Kubernetes中的Pod由于各种原因随时有可能被销毁和新建，且一般应用均以多副本的形式存在。</p>
<p>如果你想要访问一组Pod（或称之为微服务）时，必须有一种抽象资源，能够跟踪到其下所有的Pod，这个抽象便是Service。</p>
<p>Service主要有如下作用：</p>
<ul>
<li>服务发现：动态地将具有相同selector标志的后端Pod绑定起来</li>
<li>负载均衡：通过iptables或ipvs的负载均衡算法实现</li>
</ul>
<p>这里我们主要来讲下Service的负载均衡。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>从 <a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">官网</a> 可知，Service有三种代理模式：</p>
<ul>
<li>userspace代理模式</li>
<li>iptables代理模式</li>
<li>IPVS代理模式</li>
</ul>
<p>以iptables模式为例：</p>
<p><img src="/k8s-service-iptables/iptables-svc.png"></p>
<p>可以从图上看到，有一个重要的组件————kube-proxy，它以DaemonSet的形式存在，通过访问apiserver并watch相应资源(Service对象和Endpoint对象)来动态生成各自节点上的iptables规则。</p>
<p>当用户想要访问Pod服务时，iptables会通过NAT(网络地址转化)等方式<strong>随机</strong>请求到任意一个Pod。</p>
<p>在iptables模式下，kube-proxy通过在目标Node节点上的iptables配置中的NAT表的PREROUTIN和POSTROUTING链中创建一系列的自定义链(这些自定义链主要是”KUBE-SERVICE”链， “KUBE-POSTROUTING”链，每个服务对应的”KUBE-SVC-XXXXXX”链和”KUBE-SEP-XXXX”链)，然后通过这些自定义链对流经到该Node的数据包做DNAT和SNAT操作从而实现路由，负载均衡和地址转化。</p>
<h2 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h2><p>iptables是Linux平台下的包过滤防火墙，相关的四表五链知识可以去网上学习了解。</p>
<p>当设置了iptables规则后，每个数据包都要通过iptables的过滤，不同流向的数据包会通过不同的链：</p>
<ul>
<li>到本机某进程的报文：PREROUTING –&gt; INPUT</li>
<li>由本机转发的报文：PREROUTING –&gt; FORWARD –&gt; POSTROUTING</li>
<li>由本机的某进程发出报文：OUTPUT –&gt; POSTROUTING</li>
</ul>
<p>每个链上会有一些规则去过滤数据包进行操作，这些规则在大体上又可以分为4类，分别存在4张table中：</p>
<ul>
<li>filter表：负责过滤功能，防火墙；内核模块：iptables_filter</li>
<li>nat表：network address translation，网络地址转换功能；内核模块：iptable_nat</li>
<li>mangle表：拆解报文，做出修改，并重新封装 的功能；内核模块：iptable_mangle</li>
<li>raw表：关闭nat表上启用的连接追踪机制；内核模块：iptable_raw</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>下面我们来做个实验，具体看下<code>kube-proxy</code>生成的<code>iptables</code>规则是怎样请求到其中一个<code>Pod</code>里的</p>
<h3 id="1-准备"><a href="#1-准备" class="headerlink" title="1. 准备"></a>1. 准备</h3><p>在集群中应用如下创建一个名为nginx-service的Service和副本数为3的nginx Deployment：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建Deployment</span></span><br><span class="line">kubectl create deployment nginx --image=nginx --replicas=3</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建Service</span></span><br><span class="line">kubectl expose deployment nginx --port=80 --target-port=80</span><br></pre></td></tr></table></figure>
<p>我们可以看到3个Pod的ip分别为:</p>
<ul>
<li>10.244.1.14</li>
<li>10.244.1.16</li>
<li>10.244.1.17</li>
</ul>
<p>给Service分配的ip为: 10.97.54.248</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pod -owide</span></span><br><span class="line"></span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox                  1/1     Running   0          24s   10.244.1.20   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-6799fc88d8-8jnxd   1/1     Running   0          39h   10.244.1.14   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-6799fc88d8-g4d5x   1/1     Running   0          39h   10.244.1.16   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-6799fc88d8-q45nc   1/1     Running   0          44m   10.244.1.17   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get svc</span></span><br><span class="line"></span><br><span class="line">NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   175d</span><br><span class="line">nginx-svc    ClusterIP   10.97.54.248   &lt;none&gt;        80/TCP    3m1s</span><br></pre></td></tr></table></figure>

<p>我们可以看到nginx Service其下已有一组Endpoint暴露出来，对应的便是3个Pod的ip地址</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe svc nginx-svc</span></span><br><span class="line"></span><br><span class="line">Name:              nginx-svc</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            app=nginx</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          app=nginx</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP Family Policy:  SingleStack</span><br><span class="line">IP Families:       IPv4</span><br><span class="line">IP:                10.97.54.248</span><br><span class="line">IPs:               10.97.54.248</span><br><span class="line">Port:              &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         10.244.1.14:80,10.244.1.16:80,10.244.1.17:80</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h3 id="2-ClusterIP"><a href="#2-ClusterIP" class="headerlink" title="2. ClusterIP"></a>2. ClusterIP</h3><p>此外，我还建了一个busybox的镜像作为请求的发起方，他的地址是：10.244.1.20</p>
<p>所以当前的请求路径是：10.244.1.20(busybox) –&gt; 10.97.54.248:80(nginx-svc)</p>
<p>根据前文知识（由本机的某进程发出报文），我们先来看主机上的OUTPUT链上的nat表</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL OUTPUT</span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT 878 packets, 52888 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">  37M 2597M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span><br><span class="line">  11M  673M DOCKER     all  --  *      *       0.0.0.0/0           !127.0.0.0/8          ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure>
<p>可以看到流量指向了KUBE-SERVICES的链</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-SERVICES</span><br><span class="line">Chain KUBE-SERVICES (2 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-SVC-HL5LMXD5JFHQZ6LN  tcp  --  *      *       0.0.0.0/0            10.97.54.248         /* default/nginx-svc cluster IP */ tcp dpt:80</span><br><span class="line">    0     0 KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  *      *       0.0.0.0/0            10.96.0.1            /* default/kubernetes:https cluster IP */ tcp dpt:443</span><br><span class="line">    0     0 KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns cluster IP */ udp dpt:53</span><br><span class="line">    0     0 KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53</span><br><span class="line">    0     0 KUBE-SVC-JD5MR3NA4I4DYORP  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153</span><br><span class="line"> 1756  106K KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure>
<p>由于目标是Service的ip(10.97.54.248)，所以这边又匹配到了KUBE-SVC-HL5LMXD5JFHQZ6LN这条链。</p>
<p>也可以从后面的注释中看到下面需要走哪条链</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-SVC-HL5LMXD5JFHQZ6LN</span><br><span class="line">Chain KUBE-SVC-HL5LMXD5JFHQZ6LN (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.244.0.0/16        10.97.54.248         /* default/nginx-svc cluster IP */ tcp dpt:80</span><br><span class="line">    0     0 KUBE-SEP-U46YXJIMXXUGWXXH  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc */ statistic mode random probability 0.33333333349</span><br><span class="line">    0     0 KUBE-SEP-DUL3TOEKR4Q7XNNH  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc */ statistic mode random probability 0.50000000000</span><br><span class="line">    0     0 KUBE-SEP-OJQRYVIILJUTFXOB  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc */</span><br></pre></td></tr></table></figure>
<p>到这里，若源端不是10.244.0.0&#x2F;16的话，会被打上标记；由于我们的busybox是该网段的，这条规则略过。</p>
<p>然后会随机匹配KUBE-SEP-U46YXJIMXXUGWXXH，KUBE-SEP-DUL3TOEKR4Q7XNNH，KUBE-SEP-OJQRYVIILJUTFXOB这三条链的其中一条。</p>
<p>其意思是：会有1&#x2F;3的概率命中KUBE-SEP-U46YXJIMXXUGWXXH这条链，如果没命中的话，会有2&#x2F;3 * 1&#x2F;2 &#x3D; 1&#x2F;3 的概率命中第二条链KUBE-SEP-DUL3TOEKR4Q7XNNH，最后还有1&#x2F;3的概率命中最后一条链KUBE-SEP-OJQRYVIILJUTFXOB。</p>
<p>可以看出，这边是在做负载均衡。</p>
<p>我们选择其中一条链KUBE-SEP-U46YXJIMXXUGWXXH继续走下去</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-SEP-U46YXJIMXXUGWXXH</span><br><span class="line">Chain KUBE-SEP-U46YXJIMXXUGWXXH (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  all  --  *      *       10.244.1.14          0.0.0.0/0            /* default/nginx-svc */</span><br><span class="line">    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc */ tcp to:10.244.1.14:80</span><br></pre></td></tr></table></figure>
<p>KUBE-MARK-MASQ自己Pod内访问，打上标记，可以先不看。</p>
<p>看DNAT那条链，可以看到这里做了目标地址转化，最终我们的请求从：<br>10.244.1.20(busybox) –&gt; 10.97.54.248:80(nginx-svc)<br>变成了<br>10.244.1.20(busybox) –&gt; 10.244.1.14:80(nginx-6799fc88d8-8jnxd)</p>
<p>OUTPUT链走完之后还会经过POSTROUTING链：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL POSTROUTING</span><br><span class="line">Chain POSTROUTING (policy ACCEPT 5321 packets, 321K bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">  41M 2939M KUBE-POSTROUTING  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */</span><br><span class="line">    0     0 MASQUERADE  all  --  *      !docker0  172.17.0.0/16        0.0.0.0/0</span><br><span class="line">6994K  525M RETURN     all  --  *      *       10.244.0.0/16        10.244.0.0/16</span><br><span class="line"> 758K   67M MASQUERADE  all  --  *      *       10.244.0.0/16       !224.0.0.0/4          random-fully</span><br><span class="line">    0     0 RETURN     all  --  *      *      !10.244.0.0/16        10.244.0.0/24</span><br><span class="line">    0     0 MASQUERADE  all  --  *      *      !10.244.0.0/16        10.244.0.0/16        random-fully</span><br><span class="line">root@master:~# iptables -t nat -nvL KUBE-POSTROUTING</span><br><span class="line">Chain KUBE-POSTROUTING (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line"> 5396  325K RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0            mark match ! 0x4000/0x4000</span><br><span class="line">    0     0 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK xor 0x4000</span><br><span class="line">    0     0 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ random-fully</span><br></pre></td></tr></table></figure>
<p>KUBE-POSTROUTING会对数据包进行判断，如果发现它有0x4000&#x2F;0x4000标记，就会跳到MASQUERADE规则，由于我们并没有被打上标记，直接RETURN。</p>
<h3 id="3-NodePort"><a href="#3-NodePort" class="headerlink" title="3. NodePort"></a>3. NodePort</h3><p>NodePort是集群外访问集群内服务的一种方式，从iptables规则来看，NodePort是ClusterIP的超集，额外比ClusterIP多了一些规则。</p>
<p>现在我把原来的ClusterIP删了之后创建了一个名为nginx-svc-nodeport的NodePort类型的service。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zjq@master:~$ kubectl get svc</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kubernetes           ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        175d</span><br><span class="line">nginx-svc-nodeport   NodePort    10.109.235.134   &lt;none&gt;        80:31082/TCP   10s</span><br></pre></td></tr></table></figure>
<p>该Service对集群外暴露的端口是31082，这个端口是由每个节点上的kube-proxy打开的，可以用如下命令查看：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@master:~# netstat -anp | grep 31082</span><br><span class="line">tcp        0      0 0.0.0.0:31082           0.0.0.0:*               LISTEN      1231199/kube-proxy</span><br><span class="line"></span><br><span class="line">root@node1:~# netstat -anp | grep 31082</span><br><span class="line">tcp        0      0 0.0.0.0:31082           0.0.0.0:*               LISTEN      1986768/kube-proxy</span><br></pre></td></tr></table></figure>
<p>这样，你便能通过任意节点+port的方式访问到微服务了。</p>
<p>现在我们来看下NodePort类型的iptables。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL PREROUTING</span><br><span class="line">Chain PREROUTING (policy ACCEPT 20 packets, 980 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">6621K  791M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span><br><span class="line">2918K  426M DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-SERVICES</span><br><span class="line">Chain KUBE-SERVICES (2 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-SVC-XKWTKZBTDCMU3FHC  tcp  --  *      *       0.0.0.0/0            10.109.235.134       /* default/nginx-svc-nodeport cluster IP */ tcp dpt:80</span><br><span class="line">    0     0 KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  *      *       0.0.0.0/0            10.96.0.1            /* default/kubernetes:https cluster IP */ tcp dpt:443</span><br><span class="line">    0     0 KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns cluster IP */ udp dpt:53</span><br><span class="line">    0     0 KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53</span><br><span class="line">    0     0 KUBE-SVC-JD5MR3NA4I4DYORP  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153</span><br><span class="line"> 1978  119K KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure>
<p>从上述看到，会先跳转掉KUBE-NODEPORTS这条链，来看下KUBE-NODEPORTS这条链之后的路径</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-NODEPORTS</span><br><span class="line">Chain KUBE-NODEPORTS (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-SVC-XKWTKZBTDCMU3FHC  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc-nodeport */ tcp dpt:31082</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-SVC-XKWTKZBTDCMU3FHC</span><br><span class="line">Chain KUBE-SVC-XKWTKZBTDCMU3FHC (2 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.244.0.0/16        10.109.235.134       /* default/nginx-svc-nodeport cluster IP */ tcp dpt:80</span><br><span class="line">    0     0 KUBE-MARK-MASQ  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc-nodeport */ tcp dpt:31082</span><br><span class="line">    0     0 KUBE-SEP-VLHANZGCXJXNRTPY  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc-nodeport */ statistic mode random probability 0.33333333349</span><br><span class="line">    0     0 KUBE-SEP-L66MBC5WQIY6TV6O  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc-nodeport */ statistic mode random probability 0.50000000000</span><br><span class="line">    0     0 KUBE-SEP-EAEOYSPXP66WOJLO  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc-nodeport */</span><br></pre></td></tr></table></figure>
<p>先会匹配到第二个KUBE-MARK-MASQ链，用于打上标记(之后有用)。</p>
<p>再往下，通过随机负载均衡和ClusterIP的逻辑一致。</p>
<p>这里需要注意的一点是，这时候执行KUBE-POSTROUTING链时，由于匹配到之前做的标记0x4000，会做一个SNAT操作。</p>
<p>为什么要做SNAT转化呢？这边假设一个场景，如下图：</p>
<p><img src="/k8s-service-iptables/snat.png"></p>
<p>当一个外部的client通过node2的地址访问一个Service的时候，node2上的负载均衡规则，就可能把这个IP包转发给一个在node1上的Pod。这里没有任何问题。</p>
<p>而当node1上的这个Pod处理完请求之后，它就会按照这个IP包的源地址发出回复。</p>
<p>可是，如果没有做SNAT操作的话，这时候，被转发来的IP包的源地址就是client的IP地址。所以此时，Pod就会直接将回复发给client。对于client来说，它的请求明明发给了node2，收到的回复却来自node1，这个client很可能会报错。</p>
<h3 id="4-LoadBalance"><a href="#4-LoadBalance" class="headerlink" title="4. LoadBalance"></a>4. LoadBalance</h3><p>LoadBalance是NodePort的超集，这边不再分析。</p>
<h2 id="原理分析"><a href="#原理分析" class="headerlink" title="原理分析"></a>原理分析</h2><p>通过分析kube-proxy的实现能够更好地理解Service的实现。</p>
<p>新建proxyServer对象newProxyServer方法，会根据不同的模式来初始化proxier对象。</p>
<p>如果你的节点未开启ipvs，则自动降级为iptables模式。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newProxyServer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">config *proxyconfigapi.KubeProxyConfiguration,</span></span></span><br><span class="line"><span class="params"><span class="function">cleanupAndExit <span class="type">bool</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">master <span class="type">string</span>)</span></span> (*ProxyServer, <span class="type">error</span>) &#123;</span><br><span class="line">	...</span><br><span class="line">	<span class="keyword">if</span> proxyMode == proxyModeIPTables &#123;</span><br><span class="line">        klog.V(<span class="number">0</span>).InfoS(<span class="string">&quot;Using iptables Proxier&quot;</span>)</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> dualStack &#123;</span><br><span class="line">            ...</span><br><span class="line">            <span class="comment">// TODO this has side effects that should only happen when Run() is invoked.</span></span><br><span class="line">            proxier, err = iptables.NewDualStackProxier(</span><br><span class="line">            ipt,</span><br><span class="line">            utilsysctl.New(),</span><br><span class="line">            execer,</span><br><span class="line">            config.IPTables.SyncPeriod.Duration,</span><br><span class="line">            config.IPTables.MinSyncPeriod.Duration,</span><br><span class="line">            config.IPTables.MasqueradeAll,</span><br><span class="line">            <span class="type">int</span>(*config.IPTables.MasqueradeBit),</span><br><span class="line">            localDetectors,</span><br><span class="line">            hostname,</span><br><span class="line">            nodeIPTuple(config.BindAddress),</span><br><span class="line">            recorder,</span><br><span class="line">            healthzServer,</span><br><span class="line">            config.NodePortAddresses,</span><br><span class="line">            )</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			...</span><br><span class="line">            <span class="comment">// TODO this has side effects that should only happen when Run() is invoked.</span></span><br><span class="line">            proxier, err = iptables.NewProxier(</span><br><span class="line">                iptInterface,</span><br><span class="line">                utilsysctl.New(),</span><br><span class="line">                execer,</span><br><span class="line">                config.IPTables.SyncPeriod.Duration,</span><br><span class="line">                config.IPTables.MinSyncPeriod.Duration,</span><br><span class="line">                config.IPTables.MasqueradeAll,</span><br><span class="line">                <span class="type">int</span>(*config.IPTables.MasqueradeBit),</span><br><span class="line">                localDetector,</span><br><span class="line">                hostname,</span><br><span class="line">                nodeIP,</span><br><span class="line">                recorder,</span><br><span class="line">                healthzServer,</span><br><span class="line">                config.NodePortAddresses,</span><br><span class="line">            )</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;unable to create proxier: %v&quot;</span>, err)</span><br><span class="line">        &#125;</span><br><span class="line">        proxymetrics.RegisterMetrics()</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> proxyMode == proxyModeIPVS &#123;</span><br><span class="line">		...</span><br><span class="line">		klog.V(<span class="number">0</span>).InfoS(<span class="string">&quot;Using ipvs Proxier&quot;</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		...</span><br><span class="line">        klog.V(<span class="number">0</span>).InfoS(<span class="string">&quot;Using userspace Proxier&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ProxyServer结构体最主要的是Run()方法：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *ProxyServer)</span></span> Run() <span class="type">error</span> &#123;</span><br><span class="line">	...</span><br><span class="line">    <span class="comment">// 暴露/healthz接口</span></span><br><span class="line">    <span class="comment">// Start up a healthz server if requested</span></span><br><span class="line">    serveHealthz(s.HealthzServer, errCh)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 暴露指标信息</span></span><br><span class="line">    <span class="comment">// Start up a metrics server if requested</span></span><br><span class="line">    serveMetrics(s.MetricsBindAddress, s.ProxyMode, s.EnableProfiling, errCh)</span><br><span class="line">	</span><br><span class="line">	...</span><br><span class="line">    <span class="comment">// 新建informerFactory</span></span><br><span class="line">    <span class="comment">// Make informers that filter out objects that want a non-default service proxy.</span></span><br><span class="line">    informerFactory := informers.NewSharedInformerFactoryWithOptions(s.Client, s.ConfigSyncPeriod,</span><br><span class="line">        informers.WithTweakListOptions(<span class="function"><span class="keyword">func</span><span class="params">(options *metav1.ListOptions)</span></span> &#123;</span><br><span class="line">        options.LabelSelector = labelSelector.String()</span><br><span class="line">        &#125;))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// kube-proxy主要watch了service和endpoint(或endpointSlices)资源的变动，</span></span><br><span class="line">    <span class="comment">// 当它们有变动时，对应节点上的iptables规则也会相相应地变动</span></span><br><span class="line">    <span class="comment">// Create configs (i.e. Watches for Services and Endpoints or EndpointSlices)</span></span><br><span class="line">    <span class="comment">// Note: RegisterHandler() calls need to happen before creation of Sources because sources</span></span><br><span class="line">    <span class="comment">// only notify on changes, and the initial update (on process start) may be lost if no handlers</span></span><br><span class="line">    <span class="comment">// are registered yet.</span></span><br><span class="line">    serviceConfig := config.NewServiceConfig(informerFactory.Core().V1().Services(), s.ConfigSyncPeriod)</span><br><span class="line">    serviceConfig.RegisterEventHandler(s.Proxier)</span><br><span class="line">    <span class="keyword">go</span> serviceConfig.Run(wait.NeverStop)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> endpointsHandler, ok := s.Proxier.(config.EndpointsHandler); ok &amp;&amp; !s.UseEndpointSlices &#123;</span><br><span class="line">        endpointsConfig := config.NewEndpointsConfig(informerFactory.Core().V1().Endpoints(), s.ConfigSyncPeriod)</span><br><span class="line">        endpointsConfig.RegisterEventHandler(endpointsHandler)</span><br><span class="line">        <span class="keyword">go</span> endpointsConfig.Run(wait.NeverStop)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        endpointSliceConfig := config.NewEndpointSliceConfig(informerFactory.Discovery().V1().EndpointSlices(), s.ConfigSyncPeriod)</span><br><span class="line">        endpointSliceConfig.RegisterEventHandler(s.Proxier)</span><br><span class="line">        <span class="keyword">go</span> endpointSliceConfig.Run(wait.NeverStop)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 启动informer</span></span><br><span class="line">    <span class="comment">// This has to start after the calls to NewServiceConfig and NewEndpointsConfig because those</span></span><br><span class="line">    <span class="comment">// functions must configure their shared informer event handlers first.</span></span><br><span class="line">    informerFactory.Start(wait.NeverStop)</span><br><span class="line">	</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// Birth Cry after the birth is successful</span></span><br><span class="line">    s.birthCry()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 进入定时循环</span></span><br><span class="line">    <span class="keyword">go</span> s.Proxier.SyncLoop()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &lt;-errCh</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里我们来看下serviceConfig中的Run()方法，这里面根据注册的eventHandlers动作，均会执行OnServiceSynced()方法，</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Run waits for cache synced and invokes handlers after syncing.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *ServiceConfig)</span></span> Run(stopCh &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;) &#123;</span><br><span class="line">    klog.InfoS(<span class="string">&quot;Starting service config controller&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> !cache.WaitForNamedCacheSync(<span class="string">&quot;service config&quot;</span>, stopCh, c.listerSynced) &#123;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i := <span class="keyword">range</span> c.eventHandlers &#123;</span><br><span class="line">        klog.V(<span class="number">3</span>).InfoS(<span class="string">&quot;Calling handler.OnServiceSynced()&quot;</span>)</span><br><span class="line">        <span class="comment">// 注册的事件动作执行相应的OnServiceSynced()</span></span><br><span class="line">        c.eventHandlers[i].OnServiceSynced()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在OnServiceSynced中，我们可以看到核心方法syncProxyRules()</p>
<p>在十分冗长的syncProxyRules方法中(大约800行)，里面就会应用iptables rule到当前的节点。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// OnServiceSynced is called once all the initial event handlers were</span></span><br><span class="line"><span class="comment">// called and the state is fully propagated to local cache.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(proxier *Proxier)</span></span> OnServiceSynced() &#123;</span><br><span class="line">    proxier.mu.Lock()</span><br><span class="line">    proxier.servicesSynced = <span class="literal">true</span></span><br><span class="line">    proxier.setInitialized(proxier.endpointSlicesSynced)</span><br><span class="line">    proxier.mu.Unlock()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Sync unconditionally - this is called once per lifetime.</span></span><br><span class="line">    <span class="comment">// 应用iptables rule到节点上</span></span><br><span class="line">    proxier.syncProxyRules()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文简单介绍了负载均衡的作用和Kubernetes中负载均衡的原理，并通过一些实验例子来展示请求是如何被转发到Pod中的，最后通过分析核心源码的方式来了解具体实现。</p>
<p>但是，iptables也存在许多问题：</p>
<ol>
<li><p>iptables规则多了之后性能下降。按照 <a target="_blank" rel="noopener" href="https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/#why-ipvs-for-kubernetes">官方说法</a>:</p>
<blockquote>
<p>尽管 Kubernetes 在版本1.6中已经支持5000个节点，但是使用 iptables 的 kube-proxy 实际上是将集群扩展到5000个节点的一个瓶颈。一个例子是在一个包含5000个节点的集群中使用 NodePort Service，如果我们有2000个服务，每个服务有10个 pods，这将导致每个工作节点上至少有20000个 iptable 记录，这会使内核非常繁忙。</p>
</blockquote>
</li>
<li><p>iptables使用的负载均衡算法简单，不支持复杂场景。相反，ipvs能够支持更多的负载均衡算法，且性能更好</p>
</li>
</ol>
<p>另外，基于eBPF技术实现的CNI插件cilium可以完全替换kube-proxy，感兴趣的同学可以试一下。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://cita-cloud.github.io/k8s-service-account/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CITAHub">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/k8s-service-account/" class="post-title-link" itemprop="url">在 Pod 中，访问 Kubernetes API 接口，并控制访问权限</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-13 12:06:40" itemprop="dateCreated datePublished" datetime="2022-05-13T12:06:40+08:00">2022-05-13</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>901</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在实际的应用场景中，我们的应用程序运行在集群中（以 Pod 的形式存在），并且该应用程序将在集群中进行创建资源、修改资源、删除资源等等操作。</p>
<p>在 Pod 中，访问 Kubernetes API 的方法有很多，而通过 Client Libraries（程序类库）是官方推荐的做法，也是我们接下来将要学习的方法。</p>
<p>该笔记将记录：在 Pod 中，通过 Client Libraries 访问 Kubernetes API（管理 Kubernetes 集群）的方法，以及相关问题的解决办法。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>接下来我们将介绍例如，如果创建 ServiceAccount 对应用程序进行访问控制，只允许其查看 Pod 资源（即查看 Pod 列表和详细信息）</p>
<h3 id="第-1-步、创建-ServiceAccount-资源"><a href="#第-1-步、创建-ServiceAccount-资源" class="headerlink" title="第 1 步、创建 ServiceAccount 资源"></a>第 1 步、创建 ServiceAccount 资源</h3><p>而这其中最大的问题是，如何进行合理的授权，即对于既定的用户或应用程序，如何允许或拒绝特定的操作？—— 通过 ServiceAccount 实现。</p>
<pre><code># kubectl create serviceaccount myappsa
</code></pre>
<h3 id="第-2-步、引用-ServiceAccount-资源"><a href="#第-2-步、引用-ServiceAccount-资源" class="headerlink" title="第 2 步、引用 ServiceAccount 资源"></a>第 2 步、引用 ServiceAccount 资源</h3><p>定义一个 Pod，使用为 myappsa 的 ServiceAccount 资源：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f - &lt;&lt;EOF</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp</span><br><span class="line">spec:</span><br><span class="line">  serviceAccountName: myappsa</span><br><span class="line">  containers:</span><br><span class="line">  - name: main</span><br><span class="line">    image: bitnami/kubectl:latest</span><br><span class="line">    command:</span><br><span class="line">    - &quot;sleep&quot;</span><br><span class="line">    - &quot;infinity&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>ServiceAccount 是种身份，而在 Pod 中引用 ServiceAccount 则是将这种身份赋予 Pod 实例。而接下来的任务是给 ServiceAccount 这种身份赋予各种权限 —— 具体的做法便是将各种角色（Role）绑定（RoleBinding）到这个身份（ServiceAccount）上。</p>
<h3 id="第-3-步、创建-Role-资源"><a href="#第-3-步、创建-Role-资源" class="headerlink" title="第 3 步、创建 Role 资源"></a>第 3 步、创建 Role 资源</h3><p>定义名为 podreader 的 Role 资源，并定义其能够进行的访问操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f - &lt;&lt;EOF</span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: podreader</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 或，直接使用命令创建</span><br><span class="line"># kubectl create role podreader --verb=get --verb=list --resource=pods -n default</span><br></pre></td></tr></table></figure>

<h3 id="第-4-步、将-Role-与-ServiceAccount-绑定"><a href="#第-4-步、将-Role-与-ServiceAccount-绑定" class="headerlink" title="第 4 步、将 Role 与 ServiceAccount 绑定"></a>第 4 步、将 Role 与 ServiceAccount 绑定</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f - &lt;&lt;EOF</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: podreaderbinding</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: podreader</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: myappsa</span><br><span class="line">    namespace: default</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 或，从从命令行直接创建：</span><br><span class="line"># kubectl create rolebinding podreaderbinding --role=default:podreader --serviceaccount=default:myappsa --namesepace default -n default</span><br></pre></td></tr></table></figure>

<h3 id="第-5-步、访问-Kubernetes-API-测试"><a href="#第-5-步、访问-Kubernetes-API-测试" class="headerlink" title="第 5 步、访问 Kubernetes API 测试"></a>第 5 步、访问 Kubernetes API 测试</h3><p>通过 Service Account 相关信息来访问资源：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 通过我们运行的 kubectl 容器访问</span><br><span class="line">kubectl get pods                                    # 这里是为了体现 kubectl 并未通过 kubeconfig 的信息来访问集群，</span><br><span class="line">                                                    # 而是通过 /var/run/secrets/kubernetes.io/serviceaccount 来访问集群</span><br><span class="line">                                                    # kubectl 手册介绍该命令是如何查找访问信息的；</span><br><span class="line"></span><br><span class="line"># 通过 curl API 访问</span><br><span class="line">APISERVER=https://kubernetes.default.svc</span><br><span class="line">SERVICEACCOUNT=/var/run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">NAMESPACE=$(cat $&#123;SERVICEACCOUNT&#125;/namespace)</span><br><span class="line">TOKEN=$(cat $&#123;SERVICEACCOUNT&#125;/token)</span><br><span class="line">CACERT=$&#123;SERVICEACCOUNT&#125;/ca.crt</span><br><span class="line">curl --cacert $&#123;CACERT&#125; --header &quot;Authorization: Bearer $&#123;TOKEN&#125;&quot; -X GET $&#123;APISERVER&#125;/api/v1/namespaces/default/pods</span><br></pre></td></tr></table></figure>

<h3 id="第-6-步、通过客户端类库访问集群"><a href="#第-6-步、通过客户端类库访问集群" class="headerlink" title="第 6 步、通过客户端类库访问集群"></a>第 6 步、通过客户端类库访问集群</h3><p>以 Java 客户端为例：</p>
<p>如果需要在集群内部访问集群：<br>1）按照如上示例，定义 ServiceAccount 资源，<br>2）并参照 <a target="_blank" rel="noopener" href="https://github.com/kubernetes-client/java/blob/master/examples/examples-release-15/src/main/java/io/kubernetes/client/examples/InClusterClientExample.java">InClusterClientExample.java</a> 示例代码</p>
<p>如果需要在集群外部访问集群：<br>1）按照如上示例，在远端集群定义 ServiceAccount 资源，<br>2）并参照 <a target="_blank" rel="noopener" href="https://github.com/kubernetes-client/java/blob/master/examples/examples-release-15/src/main/java/io/kubernetes/client/examples/KubeConfigFileClientExample.java%20">java&#x2F;KubeConfigFileClientExample.java</a> 示例代码</p>
<h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><h3 id="访问集群级别的资源"><a href="#访问集群级别的资源" class="headerlink" title="访问集群级别的资源"></a>访问集群级别的资源</h3><p>上面是命名空间内的访问控制设置，因为上述使用的是 Role 和 RoleBinding 资源，而对于集群范围的访问控制应该使用 ClusterRole 和ClusterRoleBinding 命令。</p>
<p>替换 kind 为 ClusterRole 及 ClusterRoleBinding 即可，其他部分与 Role、RoleBinding 类似，这里不再赘述。</p>
<h3 id="通过-ServiceAccount-生成-Kubeconfig-的方法"><a href="#通过-ServiceAccount-生成-Kubeconfig-的方法" class="headerlink" title="通过 ServiceAccount 生成 Kubeconfig 的方法"></a>通过 ServiceAccount 生成 Kubeconfig 的方法</h3><p>如果需要使用 kubeconfig 文件，可通过 ServiceAccount 资源来创建，参考 <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/47770676/how-to-create-a-kubectl-config-file-for-serviceaccount%20">How to create a kubectl config file for serviceaccount</a> 讨论。</p>
<p>简而言之，kubeconfig 的 user 部分为 token，而非 client-certificate-data 与 client-key-data 参数。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/%20">Accessing the Kubernetes API from a Pod | Kubernetes</a><br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/59353819/how-to-bind-roles-with-service-accounts-kubernetes%20">dashboard - How to bind roles with service accounts - Kubernetes - Stack Overflow</a><br><a target="_blank" rel="noopener" href="https://www.baeldung.com/kubernetes-java-client%20">A Quick Intro to the Kubernetes Java Client | Baeldung</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://cita-cloud.github.io/blockchain-with-gitops/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CITAHub">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blockchain-with-gitops/" class="post-title-link" itemprop="url">区块链与GitOps</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-11 09:36:02" itemprop="dateCreated datePublished" datetime="2022-05-11T09:36:02+08:00">2022-05-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/" itemprop="url" rel="index"><span itemprop="name">区块链</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>15 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>曾经有用户在论坛里反馈过区块链系统启动过程比较复杂。</p>
<p>首先区块系统是一个对等网络，而传统系统一般都是<code>Client/Server</code>或者<code>Master/Slave</code>形态。</p>
<p>所以在区块链设计中就有一个常见的模式，把非对等形态的软件变为对等形态。比较简单的做法就是所有节点都是<code>Server</code>，同时又是其他节点的<code>Client</code>，可以认为是把常见的网络库提供的<code>Server/Client</code>形态的功能转换为对等网络。</p>
<p>这个方案麻烦的是生成配置文件，以及后续增加删除节点时修改配置文件。</p>
<p>需要提供已知的所有节点的网络信息。遍历所有的节点，为每个节点生成一个相关配置。找到本节点的网络信息，确定本节点的监听端口，用于启动<code>Server</code>；使用除自己之外的其他节点的信息，用于本节点作为<code>Client</code>去连接其他节点。</p>
<p>相当于<code>N * (N - 1)</code>个<code>Client/Server</code>的配置，且需要所有节点信息才能生成对应的配置文件，所以需要集中统一生成。</p>
<p>同样，增加删除节点的时候，也涉及到所有配置文件的修改，需要集中修改，生成新的配置文件，然后下发到所有的节点。</p>
<p>其次它是一个去中心化的系统，实际生产部署的时候是有多个参与方的，每个参与方负责一个节点，相互之间的协调和配合工作量比较大。且需要考虑参与方的机密信息不能泄露，因此需要分成多个步骤，将机密信息生成和协作产生区块链配置分开，进一步增加了操作的复杂程度。</p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol>
<li>参与方之间交互比较多，需要传递信息也比较多。不管是通过发邮件，还是通过其他方式传递，管理都比较困难。<br>比如，需要认为回信息的方式来确认对方已经收到，如果没有及时反馈，发送方重复发送，且信息与之前的不一致，如何处理？如果有人冒充参与方发送了假冒的信息，如何甄别？</li>
<li>因为区块链系统多方协作的特点，上线运营之后可能还会持续有参与方加入和退出，导致节点的配置不断变化。<br>有新增节点，有节点退出，节点的ip地址或者端口可能发生变动。如何应用这些变更？如何同步到其他节点？如何记录历史上的变更？手工操作进行配置升级容易出错，且消耗时间比较长。如果不能做到配置变更时，及时完成节点配置的更新部署，导致节点配置落后或者错误，可能会让整个区块链系统存在安全风险。比如一个参与方已经退出了，但是其他参与方没有及时更新这个信息，导致已经退出的参与方仍然能够访问系统的信息。</li>
</ol>
<h1 id="GitOps"><a href="#GitOps" class="headerlink" title="GitOps"></a>GitOps</h1><p><code>Git</code>是一个开源的分布式版本控制系统，分布式相比集中式的最大区别是<code>Git</code>没有中央版本库，每一位开发者都可以通过克隆远程代码库，在本地机器上初始化一个完整的代码版本，开发者可以把代码的修改提交到本地代码库，也可以把本地的代码库同步到远程的代码库。</p>
<p><code>GitOps</code>是一种持续交付的方式。它的核心思想是将应用系统的配置和部署以声明性的方式存放在<code>Git</code>版本库中。将<code>Git</code>作为交付流水线的核心，开发人员只需要将修改提到至<code>Git</code>，使用<code>Git</code>来加速和简化应用程序部署和运维任务。通过<code>GitOps</code>，当使用<code>Git</code>提交应用系统的配置更改时，自动化的交付流水线会将这些更改应用到实际系统中。</p>
<p>将<code>GitOps</code>方法应用在持续交付流水线上，有诸多优势和特点：</p>
<ul>
<li>自动保证实际的应用系统和<code>Git</code>仓库中的配置是一致的。</li>
<li>更快的部署时间和恢复时间。</li>
<li>稳定且可重现的回滚。<code>Git</code>中保存有历史的配置信息，有问题可以随时切回历史版本。</li>
</ul>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>通过<code>Git</code>来管理系统配置，使用<code>GitOps</code>实现持续交付，在传统系统中已经非常流行。</p>
<p>但是区块链系统比传统系统更加适合这种配置管理方式。因为其配置的产生是一个协作的过程，更能发挥<code>Git</code>作为团队协作工具的特点；参与方本地拥有完整配置，但是只把部分非机密信息共享给其他参与方也符合<code>Git</code>在分布式上的特点。</p>
<p>链的配置涉及到多个参与方，以及相互之间的信息交互，并就此对链的配置进行相应的修改。所以这是一个多方协作的场景，使用<code>Git</code>管理链的配置可以很好的解决现有方案的问题。</p>
<ol>
<li><code>Git</code>可以设置权限，只允许参与方查看和修改存放链配置的仓库。</li>
<li>通过<code>push</code>命令推送本地修改到远程，安全且有回馈。</li>
<li>可以通过<code>PR</code>等功能对修改进行审查，一个或者多个参与方都确认之后才能合并。</li>
<li>其他参与方可以通过<code>git pull</code>拉取最新的配置。</li>
<li><code>Git</code>本身可以记录修改的历史，何时何人做了什么修改都有记录。</li>
</ol>
<p>注意：</p>
<ol>
<li>因为配置里面有<code>ip</code>，端口等比较的敏感信息，可以通过建立私有仓库来避免信息泄露的风险。</li>
<li>可以通过<code>Github</code>&#x2F;<code>Gitlab</code>&#x2F;<code>Gitee</code>等提供的图形界面和扩展功能更加方便的对配置修改进行管理。</li>
</ol>
<h3 id="区块链声明式的配置方式"><a href="#区块链声明式的配置方式" class="headerlink" title="区块链声明式的配置方式"></a>区块链声明式的配置方式</h3><p>要实施<code>GitOps</code>，核心的一点是要将区块链的配置方式改造为声明式。</p>
<p>如果配置方式仍然是命令式的，比如，增加一个节点，删除一个节点等。因为每个节点实施的顺序不同，可能会得出不同的结果，导致不同节点的配置不一致。</p>
<p>因此我们对区块链系统的配置项进行梳理：</p>
<p><img src="/blockchain-with-gitops/configurations.png"></p>
<p>并据此制定一个声明式的配置数据结构：</p>
<p><img src="/blockchain-with-gitops/struct.png"></p>
<p>配置变更时，不再下发配置变更动作，而是直接重新下发所有的配置信息，节点根据这个配置重新生成节点本地的配置文件。</p>
<h3 id="链和节点Git仓库分离"><a href="#链和节点Git仓库分离" class="headerlink" title="链和节点Git仓库分离"></a>链和节点Git仓库分离</h3><p>理论上可以将一个区块链系统所有的配置信息都放在一个仓库中。</p>
<p>但是区块链去中心化的特性，链的配置需要公开，至少在参与方之间公开，以方便多个参与方都可以修改链的配置。但是节点配置中有很多机密信息，比如节点的私钥等，如果公开会造成安全方面的隐患。</p>
<p>因此，将链的配置信息和节点配置信息分开放在两个<code>Git</code>仓库中。</p>
<p>链的配置信息只包含可以公开的信息，比如账户地址等，可以放在一个公开的<code>Git</code>仓库中；而私钥等机密信息存在放节点配置中，放在参与方内部私有<code>Git</code>仓库中。</p>
<h3 id="交付流水线"><a href="#交付流水线" class="headerlink" title="交付流水线"></a>交付流水线</h3><p>以增加一个节点为例。</p>
<ul>
<li>新的参与方首先申请公开的存放链的配置信息的仓库的访问权限。</li>
<li>拉取最新的链的配置，并提交要增加的节点的公开信息，然后以<code>PR</code>的形式提交对链的配置的修改。</li>
<li>增加节点信息的PR经过审批之后，合并进最新的链的配置。</li>
<li>已有节点通过设置链的配置<code>Git</code>仓库的<code>Webhook</code>感知链的配置的变化。</li>
<li>通过本地的配置工具更新本地节点的配置文件，并提交至存放节点配置的<code>Git</code>仓库。</li>
<li>应用部署系统同样通过设置存放节点配置的<code>Git</code>仓库的<code>Webhook</code>感知节点配置的变化。</li>
<li>停掉已经存在的节点应用，并拉取最新的节点配置，重启节点应用，完成整个配置变更。</li>
</ul>
<p><img src="/blockchain-with-gitops/workflow.jpg"></p>
<h1 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h1><p>这里在<code>gitee</code>上创建<a target="_blank" rel="noopener" href="https://gitee.com/cita-cloud/gitops-test-chain">链级配置的仓库</a>。</p>
<p>配置工具为<a target="_blank" rel="noopener" href="https://github.com/cita-cloud/cloud-config">cloud-config</a>。</p>
<p>在公司内部的<code>Gitlab</code>上创建节点级配置仓库。</p>
<p>使用<code>Jekins</code>，并在相应的<code>Git</code>仓库设置<code>Webhook</code>来自动触发流水线。</p>
<p>区块链系统运行环境为<code>k8s</code>，并且集群中安装<a target="_blank" rel="noopener" href="https://argo-cd.readthedocs.io/en/stable/getting_started/">ArgoCD</a>，用于支持<code>GitOps</code>操作。</p>
<h2 id="链级别配置"><a href="#链级别配置" class="headerlink" title="链级别配置"></a>链级别配置</h2><h3 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h3><p>在<code>gitee</code>上创建仓库<code>gitops-test-chain</code>，保存链级配置。设置<code>master</code>分支不能直接<code>push</code>，只能<code>PR</code>的方式合入，并需要多个人的审批才能合入。</p>
<h3 id="参与方加入"><a href="#参与方加入" class="headerlink" title="参与方加入"></a>参与方加入</h3><p>各个参与方申请该仓库的权限，并进行相关的设置，比如设置<code>SSH Key</code>。</p>
<h3 id="链的发起方初始化链级配置"><a href="#链的发起方初始化链级配置" class="headerlink" title="链的发起方初始化链级配置"></a>链的发起方初始化链级配置</h3><p>初始化链级配置并提交至<code>gitops-test-chain</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ cloud-config init-chain --chain-name gitops-test-chain</span><br><span class="line">$ cloud-config init-chain-config --chain-name gitops-test-chain --consensus_tag v6.4.0 --controller_tag v6.4.0 --executor_tag v6.4.0 --kms_tag v6.4.0 --network_tag v6.4.0 --storage_tag v6.4.0</span><br><span class="line">$ cd gitops-test-chain/</span><br><span class="line">$ git init</span><br><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Untracked files:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</span><br><span class="line">        .gitignore</span><br><span class="line">        accounts/</span><br><span class="line">        ca_cert/</span><br><span class="line">        certs/</span><br><span class="line">        chain_config.toml</span><br><span class="line"></span><br><span class="line">nothing added to commit but untracked files present (use &quot;git add&quot; to track)</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;init chain config&quot;</span><br><span class="line">$ git remote add origin git@gitee.com:cita-cloud/gitops-test-chain.git</span><br><span class="line">$ git push -u origin master</span><br></pre></td></tr></table></figure>

<h3 id="设置超级管理员"><a href="#设置超级管理员" class="headerlink" title="设置超级管理员"></a>设置超级管理员</h3><p>超级管理员拉取最新配置，生成自己的账号，并将地址设置为为链的<code>admin</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 拉取最新的链的配置</span><br><span class="line">$ git clone git@gitee.com:cita-cloud/gitops-test-chain.git</span><br><span class="line">// 切换到新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout -b set-admin</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 生成admin账户并设置到链级配置中</span><br><span class="line">$ cloud-config new-account --chain-name gitops-test-chain</span><br><span class="line">key_id:1, address:4dafebf719f2a0439387a231977e5209fabb0cca</span><br><span class="line">$ cloud-config set-admin --chain-name gitops-test-chain --admin 4dafebf719f2a0439387a231977e5209fabb0cca</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;set admin&quot;</span><br><span class="line">$ git push --set-upstream origin set-admin</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/set-admin.png"></p>
<p>经过评审之后合并。</p>
<h3 id="参与方1设置共识账户"><a href="#参与方1设置共识账户" class="headerlink" title="参与方1设置共识账户"></a>参与方1设置共识账户</h3><p>参与方1拉取最新配置，生成自己的共识账号，并将地址添加为链的<code>validator</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 拉取最新的链的配置</span><br><span class="line">$ git clone git@gitee.com:cita-cloud/gitops-test-chain.git</span><br><span class="line"></span><br><span class="line">// 切换到新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout -b add-validator1</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 生成共识账户并设置到链级配置中</span><br><span class="line">$ cloud-config new-account --chain-name gitops-test-chain</span><br><span class="line">key_id:1, address:a746b04e30709203d3c8aadcca31bb024bbcf5df</span><br><span class="line">$ cloud-config append-validator --chain-name gitops-test-chain --validator a746b04e30709203d3c8aadcca31bb024bbcf5df</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;append validator1&quot;</span><br><span class="line">$ git push --set-upstream origin add-validator1</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/append-validator.png"></p>
<p>经过评审之后合并。</p>
<h3 id="参与方2设置共识账户"><a href="#参与方2设置共识账户" class="headerlink" title="参与方2设置共识账户"></a>参与方2设置共识账户</h3><p>参与方2拉取最新配置，生成自己的共识账号，并将地址添加为链的<code>validator</code>。</p>
<p>操作同参与方1，这里不再赘述。</p>
<h3 id="链的发起方关闭链级配置"><a href="#链的发起方关闭链级配置" class="headerlink" title="链的发起方关闭链级配置"></a>链的发起方关闭链级配置</h3><p>所有共识参与方都已经添加过共识账户。</p>
<p>链的参与方将链级配置的<code>stage</code>设置为<code>finalize</code>。</p>
<p>此后将无法添加共识账户。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ git checkout -b set-stage-finalize</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 将链级配置的stage设置为finalize</span><br><span class="line">$ cloud-config set-stage --chain-name gitops-test-chain</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;set stage finalize&quot;</span><br><span class="line">$ git push --set-upstream origin set-stage-finalize</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/final.png"></p>
<p>经过评审之后合并。</p>
<h3 id="参与方1添加节点网络信息"><a href="#参与方1添加节点网络信息" class="headerlink" title="参与方1添加节点网络信息"></a>参与方1添加节点网络信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ git checkout -b set-node0</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 添加节点0的网络信息</span><br><span class="line">$ cloud-config append-node --chain-name gitops-test-chain --node www.node0.com:40000:node0:k8s</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;set node0&quot;</span><br><span class="line">$ git push --set-upstream origin set-node0</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/set-node.png"></p>
<p>经过评审之后合并。</p>
<h3 id="参与方2添加节点网络信息"><a href="#参与方2添加节点网络信息" class="headerlink" title="参与方2添加节点网络信息"></a>参与方2添加节点网络信息</h3><p>操作同参与方1，这里不再赘述。</p>
<h3 id="超级管理员创建CA证书"><a href="#超级管理员创建CA证书" class="headerlink" title="超级管理员创建CA证书"></a>超级管理员创建CA证书</h3><p>因为网络采用<code>network_tls</code>，因此需要创建链的<code>CA</code>证书，并为每个节点创建证书。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ git checkout -b create-ca</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 创建CA证书</span><br><span class="line">$ cloud-config create-ca --chain-name gitops-test-chain</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;create ca&quot;</span><br><span class="line">$ git push --set-upstream origin create-ca</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/create-ca.png"></p>
<p>经过评审之后合并。</p>
<h3 id="参与方1创建CSR"><a href="#参与方1创建CSR" class="headerlink" title="参与方1创建CSR"></a>参与方1创建CSR</h3><p>为了不暴露参与方证书的私钥信息，这里采用<code>Certificate Signing Request</code>的方式申请证书。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ git checkout -b create-csr-node0</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 创建CA证书</span><br><span class="line">$ cloud-config create-csr --chain-name gitops-test-chain --domain node0</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;create csr for node0&quot;</span><br><span class="line">$ git push --set-upstream origin create-csr-node0</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/create-csr.png"></p>
<p>评审后合并。</p>
<h3 id="参与方2创建CSR"><a href="#参与方2创建CSR" class="headerlink" title="参与方2创建CSR"></a>参与方2创建CSR</h3><p>操作同参与方1，这里不再赘述。</p>
<h3 id="超级管理员处理CSR"><a href="#超级管理员处理CSR" class="headerlink" title="超级管理员处理CSR"></a>超级管理员处理CSR</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ git checkout -b sign-csr</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 处理CSR，签名</span><br><span class="line">$ cloud-config sign-csr --chain-name gitops-test-chain --domain node0</span><br><span class="line">$ cloud-config sign-csr --chain-name gitops-test-chain --domain node1</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;sign csr&quot;</span><br><span class="line">$ git push --set-upstream origin sign-csr</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/sign-csr.png"></p>
<p>评审后合并。</p>
<h2 id="节点配置"><a href="#节点配置" class="headerlink" title="节点配置"></a>节点配置</h2><h3 id="参与方1初始化节点"><a href="#参与方1初始化节点" class="headerlink" title="参与方1初始化节点"></a>参与方1初始化节点</h3><p>在内部的<code>GitLab</code>上创建节点配置仓库<code>gitops-test-chain-node0</code>。</p>
<p>创建<code>read_write_access token</code>，记录<code>token</code>和同时创建的<code>bot</code>账户，方便后续在流水线中拉取和提交代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// 拉取最新的链的配置</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 初始化node0节点配置</span><br><span class="line">$ cloud-config init-node --chain-name gitops-test-chain --domain node0 --account a746b04e30709203d3c8aadcca31bb024bbcf5df</span><br><span class="line">// 生成node0节点配置文件</span><br><span class="line">$ cloud-config update-node --chain-name gitops-test-chain --domain node0</span><br><span class="line">// 生成node0资源清单</span><br><span class="line">$ cloud-config update-yaml --chain-name gitops-test-chain --domain node0 --storage-class nfs-client</span><br><span class="line"></span><br><span class="line">// 提交节点初始化配置</span><br><span class="line">$ cd gitops-test-chain-node0</span><br><span class="line">$ git init</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;init node config&quot;</span><br><span class="line">$ git remote add origin https://project_xxx_bot:xxxxxxxx@git.XXX.com/xxx/gitops-test-chain-node0.git</span><br><span class="line">$ git push -u origin master</span><br></pre></td></tr></table></figure>

<h3 id="参与方1设置流水线"><a href="#参与方1设置流水线" class="headerlink" title="参与方1设置流水线"></a>参与方1设置流水线</h3><p>在<code>Jekins</code>中创建新的流水线，设置源码为链级配置的仓库: <code>https://gitee.com/cita-cloud/gitops-test-chain.git</code>的<code>master</code>分支，并设置检出到子目录<code>gitops-test-chain</code>。</p>
<p>设置<code>WebHook</code>的<code>token</code>并设置过滤条件，只在<code>master</code>分支有更新的时候才触发流水线。</p>
<p>执行脚本类似前面的初始化节点配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">set +e</span><br><span class="line">rm -rf gitops-test-chain-node0</span><br><span class="line">git clone https://project_xxx_bot:xxxxxxxx@git.XXX.com/xxx/gitops-test-chain-node0.git</span><br><span class="line">docker run -i --rm -v `pwd`:`pwd` -w `pwd` citacloud/cloud-config:v6.4.0 cloud-config init-node --chain-name gitops-test-chain --domain node0 --account 6b9ac59d83e9d0744f6231d453e2b883b1819358</span><br><span class="line">docker run -i --rm -v `pwd`:`pwd` -w `pwd` citacloud/cloud-config:v6.4.0 cloud-config update-node --chain-name gitops-test-chain --domain node0</span><br><span class="line">docker run -i --rm -v `pwd`:`pwd` -w `pwd` citacloud/cloud-config:v6.4.0 cloud-config update-yaml --chain-name gitops-test-chain --domain node0 --storage-class nfs-client</span><br><span class="line">cd gitops-test-chain-node0</span><br><span class="line">git add .</span><br><span class="line">git commit -m &quot;update node0 config&quot;</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure>

<p>在链级配置的仓库中设置<code>WebHook</code>:</p>
<p><img src="/blockchain-with-gitops/webhook.png"></p>
<h3 id="参与方2初始化节点并设置流水线"><a href="#参与方2初始化节点并设置流水线" class="headerlink" title="参与方2初始化节点并设置流水线"></a>参与方2初始化节点并设置流水线</h3><p>操作同参与方1，这里不再赘述。</p>
<h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><p>节点配置仓库到<code>k8s</code>集群的同步使用<code>ArgoCD</code>。</p>
<p>安装和使用方法参见<a target="_blank" rel="noopener" href="https://argo-cd.readthedocs.io/en/stable/getting_started/">文档</a>。</p>
<p>创建节点应用，并设置自动同步配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">argocd app create gitops-test-chain-node0 --repo https://project_xxx_bot:xxxxxxxx@git.XXX.com/xxx/gitops-test-chain-node0.git --path yamls --dest-server https://xxx.xxx.xxx.xxx:6443 --dest-namespace default --sync-policy auto</span><br><span class="line"></span><br><span class="line">argocd app create gitops-test-chain-node1 --repo https://project_yyy_bot:yyyyyyyy@git.YYY.com/yyy/gitops-test-chain-node1.git --path yamls --dest-server https://yyy.yyy.yyy.yyy:6443 --dest-namespace default --sync-policy auto</span><br></pre></td></tr></table></figure>

<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="参与方3添加节点网络信息"><a href="#参与方3添加节点网络信息" class="headerlink" title="参与方3添加节点网络信息"></a>参与方3添加节点网络信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ git clone git@gitee.com:cita-cloud/gitops-test-chain.git</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout -b set-node2</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 添加节点0的网络信息</span><br><span class="line">$ cloud-config append-node --chain-name gitops-test-chain --node www.node2.com:60000:node2:k8s</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;set node2&quot;</span><br><span class="line">$ git push --set-upstream origin set-node2</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/add-node2.png"></p>
<p>审核后合并。</p>
<p>此后<code>node0</code>和<code>node1</code>会经由<code>WebHook</code>得知链的配置发生变更，并通过<code>Jenkins</code>流水线自动更新节点配置文件，再通过<code>Argocd</code>，自动将新配置应用到集群中。</p>
<p>因为我们并没有真正的运行<code>node2</code>，所以<code>node0</code>和<code>node1</code>的<code>network</code>微服务应该会报连接错误，如下：</p>
<pre><code> 2022-04-25T12:59:31.881922Z  INFO network::peer: connecting.. peer=gitops-test-chain-node2 host=gitops-test-chain-node2-nodeport port=40000                                                                │
│ 2022-04-25T12:59:36.885651Z  INFO network::peer: connecting.. peer=gitops-test-chain-node2 host=gitops-test-chain-node2-nodeport port=40000
</code></pre>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://cita-cloud.github.io/use-loadbalancer-in-self-built-k8s-cluster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CITAHub">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/use-loadbalancer-in-self-built-k8s-cluster/" class="post-title-link" itemprop="url">在自建Kubernetes集群中创建LoadBalancer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-09 14:17:50" itemprop="dateCreated datePublished" datetime="2022-05-09T14:17:50+08:00">2022-05-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Service的分类"><a href="#Service的分类" class="headerlink" title="Service的分类"></a>Service的分类</h2><p>在Kubernetes中，经常用到的有这些类型的Service：ClusterIP，NodePort，LoadBalancer</p>
<h3 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a>ClusterIP</h3><p>ClusterIP主要用来集群内应用的互相访问，比如你有个数据库服务暴露了一个名为db-service的Service ，那么你的应用可以使用如下的方式连接：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">&lt;service&gt;.&lt;namespace&gt;:&lt;service-port&gt;</span></span><br><span class="line">spring.datasource.url=jdbc:mysql://db-service.default:3306/test?characterEncoding=utf8&amp;useSSL=true</span><br></pre></td></tr></table></figure>

<h3 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h3><p>NodePort为我们给集群外暴露服务提供了一种方式。NodePort顾名思义，这种Service在每个Kubernetes的节点暴露一个Port(默认为30000-32767)。</p>
<p>注意，这里是每个节点都会暴露一个Port。</p>
<p>所以，你可以在集群外所用任意节点的IP+Port连接到集群内的服务。</p>
<p>一般情况下，这种方式只会在测试阶段使用，所以不建议在生产环境中使用NodePort方式对外暴露服务，有如下缺点：</p>
<ul>
<li>端口范围有限制，只能是30000-32767 </li>
<li>直接对外暴露云主机的IP并不安全 </li>
<li>由于客户端固定使用其中一个云主机实例的IP连接，若该云主机故障，则整个多副本应用都不能对外提供服务，除非自己在客户端层面实现故障检测机制。 </li>
<li>一些厂商的云主机的公有IP会变，如AWS的EC2，关闭后再重启，可能导致IP不一致</li>
</ul>
<h3 id="LoadBalancer"><a href="#LoadBalancer" class="headerlink" title="LoadBalancer"></a>LoadBalancer</h3><p>为解决以上问题，传统云环境下，我们都会用云厂商的LoadBalancer来实现负载均衡和服务的高可用。如：AWS的ELB，阿里云的SLB等。顾名思义，LoadBalancer可以将流量导入后端多个实例，实现请求服务的负载均衡。</p>
<p>Kubernetes作为云原生时代的基础设施，必然会用到一些公有云资源，如LoadBalancer、云盘等，来适应云环境下的各种场景（当然私有云环境也会有与公有云相对应的资源或服务，这里不在讨论范围）。</p>
<p>Kubernetes中的LoadBalancer类型的Service为我们创建云上LoadBalancer的服务提供了便利，你可以使用如下配置：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-service-lb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx-net</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>
<p>如上，我在我自己的集群上使用了名为nginx-service-lb的LoadBalancer类型的Service来暴露后端的Nginx服务。不过会遇到如下错误状态:<br><img src="/use-loadbalancer-in-self-built-k8s-cluster/pending-service.png"></p>
<p>可以看到，nginx-service-lb会一直处于pending状态。不过，仔细一想也会知道，我的自建集群并未告知Kubernetes相关云服务的账号信息，怎么会为我们创建对应的云资源呢。</p>
<h2 id="Cloud-Provider与Cloud-Controller-Manager"><a href="#Cloud-Provider与Cloud-Controller-Manager" class="headerlink" title="Cloud Provider与Cloud Controller Manager"></a>Cloud Provider与Cloud Controller Manager</h2><p>其实，在Kubernetes创建公有云资源，还需要告诉Kubernetes所处于的云环境，Kubernetes把创建云资源的事交给了叫做Cloud Provider或Cloud Controller Manager的组件来做。</p>
<p>以上提及的组件存在两种形式：</p>
<ul>
<li>in-tree方式：Kubernetes v1.6之前的做法，也就是Cloud Provider。该方式各Providers的代码集成在Kubernetes主代码中，各核心组件通过–cloud-provider参数来启动对应的Provider（如AWS&#x2F;Azure&#x2F;GCE等），由于和Kubernetes代码耦合，当前这种方法已被官方弃用。</li>
<li>out-of-tree方式：需要在集群中安装一个独立组件，也叫Cloud Controller Manager，并通过设置–cloud-provider&#x3D;external的方式来告诉各核心组件通过外部组件来完成云资源的创建和管理，如AWS的 <a target="_blank" rel="noopener" href="https://github.com/kubernetes/cloud-provider-aws">cloud-provider-aws</a> ，阿里云的 <a target="_blank" rel="noopener" href="https://github.com/kubernetes/cloud-provider-alibaba-cloud">cloud-provider-alibaba-cloud</a> ，它们均被托管在Kubernetes官方仓库下。这种方式独立于Kubernetes核心代码的开发、构建和发布，是官方推荐的做法。</li>
</ul>
<h3 id="In-tree方式的Cloud-Provider初始化分析"><a href="#In-tree方式的Cloud-Provider初始化分析" class="headerlink" title="In-tree方式的Cloud Provider初始化分析"></a>In-tree方式的Cloud Provider初始化分析</h3><blockquote>
<p>虽说已弃用，但又不是不能用</p>
</blockquote>
<p>现在，我在AWS的EC2实例上自建了Kubernetes集群，并在初始化集群的时候设置–cloud-provider&#x3D;aws参数（in-tree方式，生产环境并不建议用，此处做实验用）来告诉Kubernetes初始化AWS的Provider:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 你可以使用kubeadm init --config=如下配置来初始化集群</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta3</span></span><br><span class="line"><span class="attr">bootstrapTokens:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">groups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system:bootstrappers:kubeadm:default-node-token</span></span><br><span class="line">  <span class="attr">token:</span> <span class="string">abcdef.0123456789abcdef</span></span><br><span class="line">  <span class="attr">ttl:</span> <span class="string">24h0m0s</span></span><br><span class="line">  <span class="attr">usages:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">signing</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">authentication</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">InitConfiguration</span></span><br><span class="line"><span class="attr">localAPIEndpoint:</span></span><br><span class="line">  <span class="attr">advertiseAddress:</span> <span class="number">10.0</span><span class="number">.0</span><span class="number">.216</span></span><br><span class="line">  <span class="attr">bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line">  <span class="attr">criSocket:</span> <span class="string">/var/run/dockershim.sock</span></span><br><span class="line">  <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ip-10-0-0-216.ap-east-1.compute.internal</span></span><br><span class="line">  <span class="attr">taints:</span> <span class="literal">null</span></span><br><span class="line">  <span class="attr">kubeletExtraArgs:</span></span><br><span class="line">    <span class="attr">cloud-provider:</span> <span class="string">&quot;aws&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line">  <span class="attr">timeoutForControlPlane:</span> <span class="string">4m0s</span></span><br><span class="line">  <span class="attr">extraArgs:</span></span><br><span class="line">    <span class="attr">cloud-provider:</span> <span class="string">&quot;aws&quot;</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta3</span></span><br><span class="line"><span class="attr">certificatesDir:</span> <span class="string">/etc/kubernetes/pki</span></span><br><span class="line"><span class="attr">clusterName:</span> <span class="string">kubernetes</span></span><br><span class="line"><span class="attr">controllerManager:</span></span><br><span class="line">  <span class="attr">extraArgs:</span></span><br><span class="line">    <span class="attr">cloud-provider:</span> <span class="string">&quot;aws&quot;</span></span><br><span class="line"><span class="attr">dns:</span> &#123;&#125;</span><br><span class="line"><span class="attr">etcd:</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">dataDir:</span> <span class="string">/var/lib/etcd</span></span><br><span class="line"><span class="attr">imageRepository:</span> <span class="string">k8s.gcr.io</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterConfiguration</span></span><br><span class="line"><span class="attr">kubernetesVersion:</span> <span class="number">1.23</span><span class="number">.0</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="attr">dnsDomain:</span> <span class="string">cluster.local</span></span><br><span class="line">  <span class="attr">serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span><span class="string">/12</span></span><br><span class="line">  <span class="attr">podSubnet:</span> <span class="number">10.244</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line"><span class="attr">scheduler:</span> &#123;&#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeletConfiguration</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubelet.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">cgroupDriver:</span> <span class="string">cgroupfs</span></span><br></pre></td></tr></table></figure>

<p>我们可以在Kubernetes的代码中(以v1.23.3为例)看到初始化Cloud Provider的相关逻辑：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">createCloudProvider</span><span class="params">(cloudProvider <span class="type">string</span>, externalCloudVolumePlugin <span class="type">string</span>, cloudConfigFile <span class="type">string</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                         allowUntaggedCloud <span class="type">bool</span>, sharedInformers informers.SharedInformerFactory)</span></span> (cloudprovider.Interface, ControllerLoopMode, <span class="type">error</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> cloud cloudprovider.Interface</span><br><span class="line">    <span class="keyword">var</span> loopMode ControllerLoopMode</span><br><span class="line">    <span class="keyword">var</span> err <span class="type">error</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> utilfeature.DefaultFeatureGate.Enabled(features.DisableCloudProviders) &amp;&amp; cloudprovider.IsDeprecatedInternal(cloudProvider) &#123;</span><br><span class="line">        cloudprovider.DisableWarningForProvider(cloudProvider)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, ExternalLoops, fmt.Errorf(</span><br><span class="line">            <span class="string">&quot;cloud provider %q was specified, but built-in cloud providers are disabled. Please set --cloud-provider=external and migrate to an external cloud provider&quot;</span>,</span><br><span class="line">            cloudProvider)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 判断是否是external参数</span></span><br><span class="line">    <span class="keyword">if</span> cloudprovider.IsExternal(cloudProvider) &#123;</span><br><span class="line">        loopMode = ExternalLoops</span><br><span class="line">        <span class="keyword">if</span> externalCloudVolumePlugin == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">            <span class="comment">// externalCloudVolumePlugin is temporary until we split all cloud providers out.</span></span><br><span class="line">            <span class="comment">// So we just tell the caller that we need to run ExternalLoops without any cloud provider.</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span>, loopMode, <span class="literal">nil</span></span><br><span class="line">        &#125;</span><br><span class="line">        cloud, err = cloudprovider.InitCloudProvider(externalCloudVolumePlugin, cloudConfigFile)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 输出弃用信息</span></span><br><span class="line">        cloudprovider.DeprecationWarningForProvider(cloudProvider)</span><br><span class="line">        </span><br><span class="line">        loopMode = IncludeCloudLoops</span><br><span class="line">        <span class="comment">// 初始化相应的云厂商provider</span></span><br><span class="line">        cloud, err = cloudprovider.InitCloudProvider(cloudProvider, cloudConfigFile)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, loopMode, fmt.Errorf(<span class="string">&quot;cloud provider could not be initialized: %v&quot;</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> cloud != <span class="literal">nil</span> &amp;&amp; !cloud.HasClusterID() &#123;</span><br><span class="line">        <span class="keyword">if</span> allowUntaggedCloud &#123;</span><br><span class="line">            klog.Warning(<span class="string">&quot;detected a cluster without a ClusterID.  A ClusterID will be required in the future.  Please tag your cluster to avoid any future issues&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span>, loopMode, fmt.Errorf(<span class="string">&quot;no ClusterID Found.  A ClusterID is required for the cloud provider to function properly.  This check can be bypassed by setting the allow-untagged-cloud option&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 设置Informoer</span></span><br><span class="line">    <span class="keyword">if</span> informerUserCloud, ok := cloud.(cloudprovider.InformerUser); ok &#123;</span><br><span class="line">        informerUserCloud.SetInformers(sharedInformers)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> cloud, loopMode, err</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="创建LoadBalancer"><a href="#创建LoadBalancer" class="headerlink" title="创建LoadBalancer"></a>创建LoadBalancer</h2><p>现在我们可以在集群中创建Nginx的Deployment和Service:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">hello</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="comment"># ELB&#x27;s port</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">hello</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">hello</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure>

<p>你可以在的控制台上看到，一个LoadBalancer资源被自动创建了，且负载均衡的80端口对应的后端实例端口为31977。<br><img src="/use-loadbalancer-in-self-built-k8s-cluster/ec2-elb.png"></p>
<p>我们再来看下刚才创建的LoadBalancer Service：<br><img src="/use-loadbalancer-in-self-built-k8s-cluster/get-service.png"></p>
<p><img src="/use-loadbalancer-in-self-built-k8s-cluster/des-service.png"></p>
<p>和NodePort的Service类似，LoadBalancer类型的Service也会有一个NodePort端口暴露，这块的逻辑对于这两种Service是一样的，也就是kube-proxy在每个节点开放了31977这个端口，这里不在赘述。</p>
<p>登录浏览器，你可以使用External-IP访问到Nginx主页面。</p>
<p>以上，我们通过创建LoadBalancer类型的Service来暴露我们集群内部的服务。创建LoadBalancer时Kubernetes主要为我们做了这些事情：</p>
<ul>
<li>在所有节点上开放一个相同的端口，并做流量的负载均衡（由kube-proxy完成）</li>
<li>自动为我们创建云上LoadBalancer资源，并且完成对应的实例端口映射（由service-controller完成）</li>
</ul>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>Cloud Provider中的service-controller的新建代码:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// New returns a new service controller to keep cloud provider service resources</span></span><br><span class="line"><span class="comment">// (like load balancers) in sync with the registry.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">	cloud cloudprovider.Interface,</span></span></span><br><span class="line"><span class="params"><span class="function">	kubeClient clientset.Interface,</span></span></span><br><span class="line"><span class="params"><span class="function">	serviceInformer coreinformers.ServiceInformer,</span></span></span><br><span class="line"><span class="params"><span class="function">	nodeInformer coreinformers.NodeInformer,</span></span></span><br><span class="line"><span class="params"><span class="function">	clusterName <span class="type">string</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	featureGate featuregate.FeatureGate,</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span> (*Controller, <span class="type">error</span>) &#123;</span><br><span class="line">	broadcaster := record.NewBroadcaster()</span><br><span class="line">	broadcaster.StartStructuredLogging(<span class="number">0</span>)</span><br><span class="line">	broadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeClient.CoreV1().Events(<span class="string">&quot;&quot;</span>)&#125;)</span><br><span class="line">	recorder := broadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: <span class="string">&quot;service-controller&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> kubeClient != <span class="literal">nil</span> &amp;&amp; kubeClient.CoreV1().RESTClient().GetRateLimiter() != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">if</span> err := ratelimiter.RegisterMetricAndTrackRateLimiterUsage(subSystemName, kubeClient.CoreV1().RESTClient().GetRateLimiter()); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	registerMetrics()</span><br><span class="line">	s := &amp;Controller&#123;</span><br><span class="line">		cloud:            cloud,</span><br><span class="line">		knownHosts:       []*v1.Node&#123;&#125;,</span><br><span class="line">		kubeClient:       kubeClient,</span><br><span class="line">		clusterName:      clusterName,</span><br><span class="line">		cache:            &amp;serviceCache&#123;serviceMap: <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="type">string</span>]*cachedService)&#125;,</span><br><span class="line">		eventBroadcaster: broadcaster,</span><br><span class="line">		eventRecorder:    recorder,</span><br><span class="line">		nodeLister:       nodeInformer.Lister(),</span><br><span class="line">		nodeListerSynced: nodeInformer.Informer().HasSynced,</span><br><span class="line">		queue:            workqueue.NewNamedRateLimitingQueue(workqueue.NewItemExponentialFailureRateLimiter(minRetryDelay, maxRetryDelay), <span class="string">&quot;service&quot;</span>),</span><br><span class="line">		<span class="comment">// nodeSyncCh has a size 1 buffer. Only one pending sync signal would be cached.</span></span><br><span class="line">		nodeSyncCh: <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">interface</span>&#123;&#125;, <span class="number">1</span>),</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	serviceInformer.Informer().AddEventHandlerWithResyncPeriod(</span><br><span class="line">		cache.ResourceEventHandlerFuncs&#123;</span><br><span class="line">            <span class="comment">// 添加事件回调函数</span></span><br><span class="line">			AddFunc: <span class="function"><span class="keyword">func</span><span class="params">(cur <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">				svc, ok := cur.(*v1.Service)</span><br><span class="line">				<span class="comment">// Check cleanup here can provide a remedy when controller failed to handle</span></span><br><span class="line">				<span class="comment">// changes before it exiting (e.g. crashing, restart, etc.).</span></span><br><span class="line">				<span class="keyword">if</span> ok &amp;&amp; (wantsLoadBalancer(svc) || needsCleanup(svc)) &#123;</span><br><span class="line">					s.enqueueService(cur)</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;,</span><br><span class="line">            <span class="comment">// 更新事件回调函数</span></span><br><span class="line">			UpdateFunc: <span class="function"><span class="keyword">func</span><span class="params">(old, cur <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">				oldSvc, ok1 := old.(*v1.Service)</span><br><span class="line">				curSvc, ok2 := cur.(*v1.Service)</span><br><span class="line">				<span class="keyword">if</span> ok1 &amp;&amp; ok2 &amp;&amp; (s.needsUpdate(oldSvc, curSvc) || needsCleanup(curSvc)) &#123;</span><br><span class="line">					s.enqueueService(cur)</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;,</span><br><span class="line">			<span class="comment">// No need to handle deletion event because the deletion would be handled by</span></span><br><span class="line">			<span class="comment">// the update path when the deletion timestamp is added.</span></span><br><span class="line">		&#125;,</span><br><span class="line">		serviceSyncPeriod,</span><br><span class="line">	)</span><br><span class="line">	s.serviceLister = serviceInformer.Lister()</span><br><span class="line">	s.serviceListerSynced = serviceInformer.Informer().HasSynced</span><br><span class="line"></span><br><span class="line">	...</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> err := s.init(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> s, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LoadBalance类型</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">wantsLoadBalancer</span><span class="params">(service *v1.Service)</span></span> <span class="type">bool</span> &#123;</span><br><span class="line">	<span class="comment">// if LoadBalancerClass is set, the user does not want the default cloud-provider Load Balancer</span></span><br><span class="line">	<span class="keyword">return</span> service.Spec.Type == v1.ServiceTypeLoadBalancer &amp;&amp; service.Spec.LoadBalancerClass == <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，在Cloud Provider中的serviceInformer只关注了LoadBalancer类型的Service的动作事件。当相对应的事件入队时，控制循环会执行相应的调谐逻辑。</p>
<p>客户端到我们的业务实例请求流程如下：<br><img src="/use-loadbalancer-in-self-built-k8s-cluster/loadbanlance-arch.png"></p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>本文简单介绍了暴露服务的主要几种方式，并着重介绍了Cloud Provider，并通过AWS的Cloud Provider创建了一个LoadBalancer。当然还有以下要注意的点：</p>
<ul>
<li>LoadBalancer是Kubernetes在NodePort类型Service上的功能加成，他们的基础均是ClusterIP</li>
<li>公有云上的容器服务一般都会使用自家的out-of-tree的cloud-controller-manager组件</li>
<li>微服务架构下，每个服务使用都使用一个LoadBalancer反而带来了额外的开销和管理成本，可以搭配Ingress等7层LoadBalancer一起使用</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://cita-cloud.github.io/edge-dc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CITAHub">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/edge-dc/" class="post-title-link" itemprop="url">可移动数据中心的构想与实现</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-04-28 15:25:38" itemprop="dateCreated datePublished" datetime="2022-04-28T15:25:38+08:00">2022-04-28</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>现在，在我们的工作环境中，以笔记本为办公平台，在其上完成例如 编码、远程、文档处理 等等工作。对于其他数据资源，则保存在远程文件托管服务（例如 NAS 或 云盘服务 等等）中，通过网络进行远程访问及数据备份。</p>
<p>但是，该方案的最大问题是我们无法保证网络总是可用或高速，在某些特殊工作环境中，或无法访问网络，或网络速度受限，或网络质量不稳定，导致我们无法访问远程文件托管服务中的资源。</p>
<p>所以，我们尝试将经常访问的__数据资源本地化__，即将常用访问的数据直接保存在笔记本电脑上，并在其上运行__备份服务__以将我们的数据备份到远程文件托管服务。鉴于此，即使网络资源成为瓶颈，也不会影响我们对数据资源的访问，而远程文件托管服务则作为 <strong>极低频访问资源的存储</strong> 及 <strong>笔记本数据备份的后端存储</strong> 而存在。</p>
<p>但是，随着办公平台的扩大，我们平时不得不以 Linux 系统为主要办公环境，但是有的时候会用到 Windows 系统，尤其是需要运行仅支持 Windows 平台的软件（例如 企业微信、钉钉、微信 等等）。我们尝试更换 Macbook，但并不能解决问题；两个笔记本，携带也不方便；通过 Wine 方案，仅能解决部分软件的运行问题，仍旧存在部分软件无法通过 Wine 来运行。</p>
<p>所以，我们尝试在虚拟机中运行操作系统，将我们的办公环境迁移到虚拟机。我们在宿主机中运行 Linux 操作系统，并在其中部署桌面虚拟化（例如 VirtualBox 等等），并在虚拟机中运行 Windows 操作系统。同时，借助虚拟机的 Guest Additions 组件，实现宿主机与虚拟机之间互操作（例如 文件共享、复制粘贴 等等）</p>
<p>但是，很多时候两个操作系统（Windows&#x2F;Linux）都要使用相同的服务。例如：或为了提供网络质量，两个操作系统都需要使用网络加速服务；或为了访问办公网络，两个操作系统都要接入企业 VPN 服务；</p>
<p>所以，我们开始思考，既然虚拟机的流量是通过 NAT 进行网络访问，那能不能在虚拟化中运行路由器操作系统，然后所有的虚拟机操作系统将数据包发往路由器操作系统，而路由器操作系统的其他接口负责将数据包发送到外部网络。这样的网络模型就更加贴近于现实环境的终端网络，事情也变得越来越疯狂。</p>
<p>关键词：可移动数据中心、边缘数据中心、便携式数据中心、虚拟数据中心</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>根据我们的想法进行描绘，整个系统原型类似如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+-------------------------------------------------+</span><br><span class="line">|   OpenWrt   | Windows VM | Linux VM |    ...    |</span><br><span class="line">+-------------------------------------------------+</span><br><span class="line">|   LINUX + KVM + Storage                         |</span><br><span class="line">+-------------------------------------------------+</span><br><span class="line">|                     LAPTOP                      |</span><br><span class="line">+-------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>LAPTOP</strong>：最底层是笔记本物理硬件本身，所有的环境将运行在其上。这其中既有配电设施，又有冷却设施，运维管理中心则为本机。至于机柜和布线系统，鉴于整个系统的基于多种虚拟化技术实现，所以完全不需要机柜和布线系统。</p>
<p><strong>LINUX + KVM + Storage</strong>：向上则是虚拟化环境，运行 Linux 操作系统，并在其中部署 KVM 虚拟化，最后将物理存储加入到 KVM 虚拟化的存储池中，以向上层虚拟机提供存储服务。</p>
<p><strong>OpenWrt</strong>：作为虚拟机访问外部网络的网关，所有虚拟机流量将通过 OpenWrt 路由到外部网络。对于 KVM 环境，运行 <a target="_blank" rel="noopener" href="https://openwrt.org/docs/guide-user/virtualization/qemu">OpenWrt in QEMU</a> 路由，网络加速也在 OpenWrt 中实现，通过全局流量检查来同时实现Linux 和 Windows 的网络加速，如此便无需在两个系统中安装客户端。</p>
<p><strong>Windows VM、Linux VM</strong>：作为实际我们办公的操作系统，且所有虚拟机网卡与 OpenWrt 网卡位于相同的二层网络，所有的虚拟机流量将发送到 OpenWrt，并由 OpenWrt 的另张网卡发送到外部网络。</p>
<p><strong>虚拟机的数据共享</strong>：鉴于多个虚拟机间的很多数据、文件、程序是需要共享的，我们通过用 NFS 或 CIFS 来实现。但是存储还是落在主机上，所以需要使用 KVM 隔离网络来允许虚拟机访问宿主机，这导致但个虚拟机至少有两块网卡。</p>
<p><strong>虚拟机的桌面访问</strong>：宿主机具备图形化桌面，我们使用 virt-manager 对 KVM 虚拟机进行管理及桌面访问，对于虚拟机间的复制粘贴，通过 SPICE Agent 能够解决。</p>
<p>如此，我们便能将数据中心装在包里，随身携带。硬件配置要求，取决于个人工作负载，我们平时运行 Linux 及相关的应用就要 16G 的内存，主要是应用程序开的多。</p>
<h2 id="首个实现"><a href="#首个实现" class="headerlink" title="首个实现"></a>首个实现</h2><p>我们已实现该技术方案的首个版本，但是多少有些出入（但差别并不大）：</p>
<p><img src="/edge-dc/pasted_image.png"></p>
<p>共 4 层（从上到下，L4、L3、L2、L1）</p>
<p>虚拟化选用 VirtalBox 的原因是：<br>1）鉴于是移动数据中心，所以涉及桌面环境事件响应（例如 休眠处理 等等），而桌面虚拟化软件处理的更好；<br>2）VritualBox 免费；</p>
<p>鉴于虚拟化（L2）及物理层（L1）暂无特殊配置，所以不再详细说明。该部分的后续内容将概述网络层（L3）与客户机层（L4）的实现，及相关问题处理。</p>
<h3 id="第一步、创建-OpenWrt-实例"><a href="#第一步、创建-OpenWrt-实例" class="headerlink" title="第一步、创建 OpenWrt 实例"></a>第一步、创建 OpenWrt 实例</h3><p>在 VirtualBox 中，部署 OpenWrt 服务：参考 <a target="_blank" rel="noopener" href="https://openwrt.org/docs/guide-user/virtualization/virtualbox-vm%20">[OpenWrt Wiki] OpenWrt on VirtualBox HowTo</a> 文档，获取官方配置说明。</p>
<p>我们需要三张不同网络网络类型的网卡（尽量依序创建）：<br>1）NAT：负责网络访问，否则 OpoenWrt 将无法上网；（相当于 WAN 接口）<br>2）Internal Network：实现 OpenWrt、Linux 的二层互联；（相当于 LAN 接口）<br>3）Host-only Adapter：能与主机通信，用于从主机连接和管理 OpenWrt 服务；</p>
<h3 id="第二步、配置-OpenWrt-服务"><a href="#第二步、配置-OpenWrt-服务" class="headerlink" title="第二步、配置 OpenWrt 服务"></a>第二步、配置 OpenWrt 服务</h3><p>我们需要配置 OpenWrt 服务，使其成为路由设备：<br><a target="_blank" rel="noopener" href="https://openwrt.org/docs/guide-user/network/openwrt_as_routerdevice%20">[OpenWrt Wiki] OpenWrt as router device</a></p>
<p>如果遇到问题，或许需要使用 TCPDump 抓包：<br><a target="_blank" rel="noopener" href="https://openwrt.org/docs/guide-user/firewall/misc/tcpdump_wireshark%20">[OpenWrt Wiki] How to capture, filter and inspect packets using tcpdump or wireshark tools</a></p>
<h3 id="第三步、配置-Linux-Guest-实例"><a href="#第三步、配置-Linux-Guest-实例" class="headerlink" title="第三步、配置 Linux Guest 实例"></a>第三步、配置 Linux Guest 实例</h3><p>创建 Linux Guest 主机，并添加 Internal Network 类型网络，使其与 OpenWrt 二层互联；</p>
<p>然后，启动 Linux Guest 实例；</p>
<p>如果配置正确，Linux Guest 能够通过 OpenWrt 的 DHCP 获取 IP 地址；</p>
<h3 id="后续的改进工作"><a href="#后续的改进工作" class="headerlink" title="后续的改进工作"></a>后续的改进工作</h3><p>首个版本已能够运行，但是还有很多改进工作：</p>
<p>1）<strong>继续使用 VirtualBox 桌面虚拟化</strong>：KVM 非桌面虚拟化，对桌面场景处理不是很好。比如 当 KVM 休眠恢复后，时间跳跃导致 Guest CPU Usage 飙升；</p>
<p>2）<strong>替换 Host 系统为 Linux 发行版</strong>：Window 10 + VirtualBox 的问题是，网卡插拔后（间隔久一点），Guest Bridage Network 无法就再也无法访问网络；Linux + VirtualBox 经过测试暂无问题。桥接网络是为了 OpenWrt 与 另个数据中心运行动态路由协议，如果用 NAT Network 就无法运行动态路由协议。另外，笔记本的有线网卡&#x2F;无线网卡可以进行 主&#x2F;备&#x2F;负载。</p>
<p>3）<strong>引入存储（待定）</strong>：该改进的目的是使得整个模型更加贴近于终端环境，并通过 FCoE 或 iSCSI 的方式提供块存储。但并无必要性，虚拟机的磁盘扩容也是在文件系统上扩容的，没必要引入独立的存储服务。后来（04&#x2F;27&#x2F;2022）我们觉得还是要引入存储服务，为多个 Guest 提供共享的网络文件系统存储，实现文件共享，同时使得整个模型更加贴近于数据中心。</p>
<h3 id="改进工作的推进"><a href="#改进工作的推进" class="headerlink" title="改进工作的推进"></a>改进工作的推进</h3><p>我们确实对整个模型进行改进</p>
<p><img src="/edge-dc/pasted_image001.png"></p>
<p>1）<strong>替换 Host 系统为 Linux 发行版</strong>：Ubuntu 20.04 LTS + VirtualBox 6.1.32</p>
<p>2）<strong>引入存储</strong>：使用 OpenMediaVault 作为存储，目前主要是将其作为网络文件系统来使用：（1）在多个 Guest 间通过网络分享文件和数据；（2）将数据从 Linux Guest 脱离出来，数据备份工作转移到存储服务层面；</p>
<p>3）<strong>替换 OpenWrt 组件</strong>：我们引入 pfSense 防火墙，以利用其中的 FRR 模块与 L2TP 模块；</p>
<h2 id="低廉的分布式数据中心"><a href="#低廉的分布式数据中心" class="headerlink" title="低廉的分布式数据中心"></a>低廉的分布式数据中心</h2><p>为了能从办公室访问家里的网络（我们没有出口路由的控制权），而且是任意访问（我们希望直接访问 80 端口，但是受限于网络环境，80 端口默认被运营商屏蔽），所以我们通过部署 L2TP VPN 解决。通过十分服务器部署 LNS 服务，家里的路由器和笔记都作为 LAC 拨号到 LNS，形成二层网络，我们便能通过内网地址直接访问家里的服务。但缺点也很明显，双端需要配置静态路由，否则网络无法互通。虽然需要静态路由，但也不影响使用。而在思考改进该缺点的时候，我们得到创建分布式的低廉数据中心的灵感。</p>
<p>我们的目标很简单：利用当前公网架构，将分散在各地理位置的资源集结到一起，形成大型逻辑网络。于是便产生如下拓扑（相关的技术细节不再赘述，该图已披露出关键的技术要点）</p>
<p><img src="/edge-dc/pasted_image002.png"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://cita-cloud.github.io/cert-manager/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CITAHub">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/cert-manager/" class="post-title-link" itemprop="url">在 Kubernetes 中，免费 HTTPS 证书（cert-manager）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-04-01 19:28:06" itemprop="dateCreated datePublished" datetime="2022-04-01T19:28:06+08:00">2022-04-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/" itemprop="url" rel="index"><span itemprop="name">云计算</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>我们使用 Certbot 工具向 Let’s Encrypt 免费申请并自动续期证书。</p>
<p>在 Kubernetes Cluster 中，我们使用 cert-manager 组件来实现。</p>
<p>该笔记将记录：在 Kubernetes Cluster 1.22 中，部署 cert-manager 1.7 组件，以及相关问题解决办法。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>集群版本：Kubernetes Cluster v1.22.3-aliyun.1<br>组件版本：cert-manager v1.7（Supported Kubernetes versions: 1.18-1.23）</p>
<h3 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h3><p>1）作为系列部署资源，cert-manager 运行在 Kubernetes Cluster 中，并利用 CRD 来配置 CA 并请求证书；<br>2）部署方式：我们使用官方文档中推荐的 cmctl 命令，不再使用原始的 YAML 清单文件；<br>3）在部署 cert-manager 组件之后，需要创建代表 CA 的 Issuer 或 ClusterIssuer 资源；<br>4）在集群中部署多个 cert-manager 实例会出现意外行为（以前 v1.3 文档提到过，该版本不清楚是否存在该限制）；</p>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">OS=$(go <span class="built_in">env</span> GOOS); ARCH=$(go <span class="built_in">env</span> GOARCH); curl -sSL -o cmctl.tar.gz https://github.com/cert-manager/cert-manager/releases/download/v1.7.2/cmctl-<span class="variable">$OS</span>-<span class="variable">$ARCH</span>.tar.gz</span><br><span class="line">tar xzf cmctl.tar.gz</span><br><span class="line">sudo <span class="built_in">mv</span> cmctl /usr/local/bin</span><br></pre></td></tr></table></figure>

<h3 id="第一步、部署组件"><a href="#第一步、部署组件" class="headerlink" title="第一步、部署组件"></a>第一步、部署组件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmctl x install --dry-run</span><br></pre></td></tr></table></figure>

<h3 id="第二步、验证安装"><a href="#第二步、验证安装" class="headerlink" title="第二步、验证安装"></a>第二步、验证安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cmctl check api --wait=2m</span></span><br><span class="line">The cert-manager API is ready</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pods --namespace cert-manager</span></span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">cert-manager-75cf8df6b6-t2xns              1/1     Running   0          2m37s</span><br><span class="line">cert-manager-cainjector-857f5bd88c-5xzd7   1/1     Running   0          2m37s</span><br><span class="line">cert-manager-webhook-5cd99556d6-s6jf5      1/1     Running   0          2m37s</span><br></pre></td></tr></table></figure>

<h3 id="第三步、签发测试"><a href="#第三步、签发测试" class="headerlink" title="第三步、签发测试"></a>第三步、签发测试</h3><p>验证方法：通过创建自签名证书，并检查证书是否能够自动签发（参考 <a target="_blank" rel="noopener" href="https://cert-manager.io/docs/installation/verify/">Verifying the Installation</a> 文档，以获取具体细节）</p>
<p>我们有使用手工方式来验证（我们通过文档中提到的社区工具来验证，但是失败）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"># cat &gt; test-resources.yaml  &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">    name: cert-manager-test</span><br><span class="line">---</span><br><span class="line">apiVersion: cert-manager.io/v1</span><br><span class="line">kind: Issuer</span><br><span class="line">metadata:</span><br><span class="line">    name: test-selfsigned</span><br><span class="line">    namespace: cert-manager-test</span><br><span class="line">spec:</span><br><span class="line">    selfSigned: &#123;&#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: cert-manager.io/v1</span><br><span class="line">kind: Certificate</span><br><span class="line">metadata:</span><br><span class="line">    name: selfsigned-cert</span><br><span class="line">    namespace: cert-manager-test</span><br><span class="line">spec:</span><br><span class="line">    dnsNames:</span><br><span class="line">    - example.com</span><br><span class="line">    secretName: selfsigned-cert-tls</span><br><span class="line">    issuerRef:</span><br><span class="line">    name: test-selfsigned</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># kubectl apply -f test-resources.yaml</span><br><span class="line">...</span><br><span class="line">Status:</span><br><span class="line">    Conditions:</span><br><span class="line">    Last Transition Time:  2022-04-01T09:51:50Z</span><br><span class="line">    Message:               Certificate is up to date and has not expired</span><br><span class="line">    Observed Generation:   1</span><br><span class="line">    Reason:                Ready</span><br><span class="line">    Status:                True</span><br><span class="line">    Type:                  Ready</span><br><span class="line">    Not After:               2022-06-30T09:51:50Z</span><br><span class="line">    Not Before:              2022-04-01T09:51:50Z</span><br><span class="line">    Renewal Time:            2022-05-31T09:51:50Z</span><br><span class="line">    Revision:                1</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># kubectl delete -f test-resources.yaml</span><br><span class="line">namespace &quot;cert-manager-test&quot; deleted</span><br><span class="line">issuer.cert-manager.io &quot;test-selfsigned&quot; deleted</span><br><span class="line">certificate.cert-manager.io &quot;selfsigned-cert&quot; deleted</span><br></pre></td></tr></table></figure>

<p>至此，已完成 cert-manager 部署，接下来便是使用 cert-manager 来申请 Let’s Encrypt 证书（结合我们的需求）。</p>
<h2 id="通过-cert-manager-申请-Let’s-Encrypt-证书"><a href="#通过-cert-manager-申请-Let’s-Encrypt-证书" class="headerlink" title="通过 cert-manager 申请 Let’s Encrypt 证书"></a>通过 cert-manager 申请 Let’s Encrypt 证书</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><p>前面步骤演示如何部署 cert-manager 组件，并成功申请自签名证书，但这并非我们的实际应用场景。</p>
<p>我们希望通过 cert-manager 组件，在集群内完成 Let’s Encrypt 证书申请和管理：<br>1）我们使用 阿里云 DNS，并通过 DNS01 完成域名所有权认证（部分集群在内网，无法使用 HTTP01 认证）；</p>
<h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><p>需要阅读如下文档，以了解相关内容：<br><a target="_blank" rel="noopener" href="https://cert-manager.io/docs/configuration/acme/">cert-manager&#x2F;Configuration&#x2F;ACME</a><br>—- <a target="_blank" rel="noopener" href="https://cert-manager.io/docs/configuration/acme/dns01/">cert-manager&#x2F;Configuration&#x2F;ACME&#x2F;DNS01</a><br>——– <a target="_blank" rel="noopener" href="https://cert-manager.io/docs/configuration/acme/dns01/webhook/">cert-manager&#x2F;Configuration&#x2F;ACME&#x2F;DNS01&#x2F;Webhook</a></p>
<p>我们这里使用 <a target="_blank" rel="noopener" href="https://github.com/DEVmachine-fr/cert-manager-alidns-webhook">DEVmachine-fr&#x2F;cert-manager-alidns-webhook</a> 来完成证书申请。</p>
<p>安装 Webhook 部分：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 网络原因，所以我们提前下载 helm chart 文件</span><br><span class="line">wget https://github.com/DEVmachine-fr/cert-manager-alidns-webhook/releases/download/alidns-webhook-0.6.1/alidns-webhook-0.6.1.tgz</span><br><span class="line"></span><br><span class="line"># 查看相关变量</span><br><span class="line">helm show values ./alidns-webhook-0.6.1.tgz</span><br><span class="line"></span><br><span class="line"># 大多数变量不需要修改，除了 groupName 参数</span><br><span class="line">helm -n cert-manager install alidns-webhook ./alidns-webhook-0.6.1.tgz \</span><br><span class="line">    --set groupName=your-company.example.com</span><br></pre></td></tr></table></figure>

<p>创建 ClusterIssuer 资源：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># 完成 DNS 质询需要访问阿里云接口来修改 DNS 记录，所以需要使用 KEY 与 TOKEN 来认证</span><br><span class="line">kubectl create secret generic                      \</span><br><span class="line">    alidns-secrets                                 \</span><br><span class="line">    --from-literal=&quot;access-token=your-token&quot;       \</span><br><span class="line">    --from-literal=&quot;secret-key=your-secret-key&quot;</span><br><span class="line"></span><br><span class="line"># 创建 ClusterIssuer 资源</span><br><span class="line">kubectl apply -f &lt;&lt;EOF</span><br><span class="line">apiVersion: cert-manager.io/v1</span><br><span class="line">kind: ClusterIssuer</span><br><span class="line">metadata:</span><br><span class="line">    name: letsencrypt</span><br><span class="line">spec:</span><br><span class="line">    acme:</span><br><span class="line">    # 修改，邮箱地址</span><br><span class="line">    email: contact@example.com</span><br><span class="line">    # 修改，生产地址：https://acme-v02.api.letsencrypt.org/directory</span><br><span class="line">    server: https://acme-staging-v02.api.letsencrypt.org/directory</span><br><span class="line">    privateKeySecretRef:</span><br><span class="line">        name: letsencrypt</span><br><span class="line">    solvers:</span><br><span class="line">    - dns01:</span><br><span class="line">        webhook:</span><br><span class="line">            # 修改，应用我们刚才创建的 Secret 资源</span><br><span class="line">            config:</span><br><span class="line">                accessTokenSecretRef:</span><br><span class="line">                key: access-token</span><br><span class="line">                name: alidns-secrets</span><br><span class="line">                regionId: cn-beijing</span><br><span class="line">                secretKeySecretRef:</span><br><span class="line">                key: secret-key</span><br><span class="line">                name: alidns-secrets</span><br><span class="line">            # 修改，需要填写在安装时指定的 groupName 信息</span><br><span class="line">            groupName: example.com</span><br><span class="line">            solverName: alidns-solver</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h1 id="在-Ingress-中，使用-HTTPS-证书："><a href="#在-Ingress-中，使用-HTTPS-证书：" class="headerlink" title="在 Ingress 中，使用 HTTPS 证书："></a>在 Ingress 中，使用 HTTPS 证书：</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># https://cert-manager.io/docs/usage/ingress/</span><br><span class="line">kubectl apply -f &lt;&lt;EOF</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">    annotations:</span><br><span class="line">    # 修改，暗示要使用的 issuer 资源，由管理员提供</span><br><span class="line">    cert-manager.io/cluster-issuer: nameOfClusterIssuer</span><br><span class="line">    name: myIngress</span><br><span class="line">    namespace: myIngress</span><br><span class="line">spec:</span><br><span class="line">    rules:</span><br><span class="line">    ...</span><br><span class="line">    tls:</span><br><span class="line">    - hosts:</span><br><span class="line">    # 修改，需要签发证书的域名</span><br><span class="line">    - example.com</span><br><span class="line">    # 修改，保存证书的 Secret 资源（cert-manger 负责创建）</span><br><span class="line">    secretName: myingress-cert</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://github.com/DEVmachine-fr/cert-manager-alidns-webhook">GitHub - DEVmachine-fr&#x2F;cert-manager-alidns-webhook</a><br><a target="_blank" rel="noopener" href="https://cert-manager.io/docs/installation/">Installation | cert-manager</a><br><a target="_blank" rel="noopener" href="https://cert-manager.io/docs/installation/cmctl/">cmctl | cert-manager</a><br><a target="_blank" rel="noopener" href="https://cert-manager.io/docs/usage/ingress/">Securing Ingress Resources | cert-manager</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://cita-cloud.github.io/lvm-snapshot/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="CITAHub">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | CITAHub技术团队">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/lvm-snapshot/" class="post-title-link" itemprop="url">LVM 快照与恢复（备份与恢复的快速方法）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-19 21:34:24" itemprop="dateCreated datePublished" datetime="2022-03-19T21:34:24+08:00">2022-03-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/" itemprop="url" rel="index"><span itemprop="name">云计算</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>899</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在进行某些验证性操作时，我们需要创建测试数据，并在验证操作的过程中修改数据。但是如果验证操作失败，那么我们又需要重新创建测试数据。为了避免重新创建数据，我们常见的做法是备份测试数据，以在验证失败时能够从备份数据中快速进行恢复。</p>
<p>还有种场景是服务升级的时候：为了能够在升级失败时回滚，需要对服务数据进行备份，否则数据被破坏之后，服务回滚后也无法运行。但是由于服务数据较多，导致备份周期长，服务停机时间长。而且在升级过程中，并非所有的数据都需要备份，因为并非所有的数据都会被破坏。</p>
<p>该笔记将记录：在 LVM 中，使用 Snapshot 快照的方法（对数据进行快速的备份与恢复），以及常见问题的解决办法。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>当创建快照后，如果不小心删除任何文件，也不必担心，因为快照具有我们已删除的原始文件。</p>
<p>注意事项：<br>1）快照不能用于持久的备份策略 —— 备份是某些数据文件的主副本，而快照是块级别，所以不能使用快照作为备份选项；<br>2）不要更改快照卷，保持原样，而快照用于快速恢复。</p>
<h3 id="环境概述"><a href="#环境概述" class="headerlink" title="环境概述"></a>环境概述</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pvcreate /dev/sdb                                                               # 10G</span><br><span class="line">vgcreate vgdt /dev/sdb</span><br><span class="line">lvcreate -n source --size 3G vgdt</span><br><span class="line"></span><br><span class="line">mkfs.ext4 /dev/vgdt/source</span><br><span class="line">mount /dev/vgdt/source /mnt/</span><br><span class="line">echo 123456 &gt; /mnt/foo.txt</span><br><span class="line">md5sum /mnt/foo.txt                                                             # f447b20a7fcbf53a5d5be013ea0b15af</span><br></pre></td></tr></table></figure>

<h3 id="第一步、创建快照"><a href="#第一步、创建快照" class="headerlink" title="第一步、创建快照"></a>第一步、创建快照</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># lvcreate --size  1G --snapshot --name backup4source /dev/vgdt/source</span><br><span class="line">  Logical volume &quot;backup4source&quot; created.</span><br><span class="line"></span><br><span class="line"># lvextend --size +1G /dev/vgdt/backup4source                                   # 再额外增加 1G 空间</span><br><span class="line">  Size of logical volume vgdt/backup4source changed from 1.00 GiB (256 extents) to 2.00 GiB (512 extents).</span><br><span class="line">  Logical volume vgdt/backup4source successfully resized.</span><br><span class="line"></span><br><span class="line"># lvs /dev/vgdt/backup4source                                                   # 查看快照信息</span><br><span class="line">  LV            VG   Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  backup4source vgdt swi-a-s--- 2.00g      source 0.01 </span><br><span class="line"></span><br><span class="line"># lvdisplay /dev/vgdt/backup4source </span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/vgdt/backup4source</span><br><span class="line">  ...</span><br><span class="line">  LV snapshot status     active destination for source                          # 该快照所属的 LV</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<h3 id="第二步、数据修改"><a href="#第二步、数据修改" class="headerlink" title="第二步、数据修改"></a>第二步、数据修改</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># lvs</span><br><span class="line">  LV            VG        Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  ubuntu-lv     ubuntu-vg -wi-ao---- &lt;9.00g                                                    </span><br><span class="line">  backup4source vgdt      swi-a-s---  2.00g      source 0.01                                   </span><br><span class="line">  source        vgdt      owi-aos---  3.00g</span><br><span class="line"></span><br><span class="line"># dd if=/dev/zero of=/mnt/foo.txt bs=1M count=1024 conv=fdatasync </span><br><span class="line">1024+0 records in</span><br><span class="line">1024+0 records out</span><br><span class="line">1073741824 bytes (1.1 GB, 1.0 GiB) copied, 9.26228 s, 116 MB/s</span><br><span class="line"></span><br><span class="line"># lvs</span><br><span class="line">  LV            VG        Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  ubuntu-lv     ubuntu-vg -wi-ao---- &lt;9.00g                                                    </span><br><span class="line">  backup4source vgdt      swi-a-s---  2.00g      source 50.21                   # 原始数据被写入快照分区，以占用 50.21%</span><br><span class="line">  source        vgdt      owi-aos---  3.00g </span><br></pre></td></tr></table></figure>

<p>关于 Snapshot 大小：<br>1）如果数据变更的总量超过 Snapshot 大小，则会产生 Input&#x2F;output error 错误，进而导致 Snapshot 不可用（解释扩容也无法恢复）；<br>2）如果要避免该问题，可以创建相同大小的 Snapshot，或者自动扩容 Snapshot 分区（这里不再展开详细说明）；</p>
<h3 id="第三步、恢复快照"><a href="#第三步、恢复快照" class="headerlink" title="第三步、恢复快照"></a>第三步、恢复快照</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># umount /mnt</span><br><span class="line"></span><br><span class="line"># lvconvert --merge /dev/vgdt/backup4source</span><br><span class="line">  Merging of volume vgdt/backup4source started.</span><br><span class="line">  vgdt/source: Merged: 50.29%</span><br><span class="line">...</span><br><span class="line">  vgdt/source: Merged: 100.00%</span><br><span class="line"></span><br><span class="line"># mount /dev/vgdt/source /mnt/</span><br><span class="line"></span><br><span class="line"># md5sum /mnt/foo.txt</span><br><span class="line">f447b20a7fcbf53a5d5be013ea0b15af  /mnt/foo.txt</span><br></pre></td></tr></table></figure>

<p>补充说明：<br>1）当 merge 后，Snapshot 会被自动删除；</p>
<h2 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h2><h3 id="删除快照"><a href="#删除快照" class="headerlink" title="删除快照"></a>删除快照</h3><p>如果没有必要保留快照，则可以删除：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># lvremove /dev/vgdt/backup4source</span><br></pre></td></tr></table></figure>

<h3 id="自动扩容"><a href="#自动扩容" class="headerlink" title="自动扩容"></a>自动扩容</h3><p>该特性是为了让 Snapshot 自动扩容，而不需要分配足够的空间，且当空间不足时不需要人工介入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/lvm/lvm.conf</span><br><span class="line">...</span><br><span class="line">snapshot_autoextend_threshold = 70                                              # 当用量超过 70% 时，</span><br><span class="line">snapshot_autoextend_percent = 20                                                # 自动扩容 20%</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.tecmint.com/take-snapshot-of-logical-volume-and-restore-in-lvm/%20">How to Take ‘Snapshot of Logical Volume and Restore’ in LVM - Part III</a><br><a target="_blank" rel="noopener" href="https://kerneltalks.com/disk-management/how-to-guide-lvm-snapshot/%20">How-to guide: LVM snapshot - Kernel Talks</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">CITAHub</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">43k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:38</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://unpkg.com/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/pangu@4.0.7/dist/browser/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://unpkg.com/hexo-generator-searchdb@1.4.1/dist/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  





</body>
</html>
