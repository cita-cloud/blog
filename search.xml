<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>区块链与GitOps</title>
    <url>/blockchain-with-gitops/</url>
    <content><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>曾经有用户在论坛里反馈过区块链系统启动过程比较复杂。</p>
<p>首先区块系统是一个对等网络，而传统系统一般都是<code>Client/Server</code>或者<code>Master/Slave</code>形态。</p>
<p>所以在区块链设计中就有一个常见的模式，把非对等形态的软件变为对等形态。比较简单的做法就是所有节点都是<code>Server</code>，同时又是其他节点的<code>Client</code>，可以认为是把常见的网络库提供的<code>Server/Client</code>形态的功能转换为对等网络。</p>
<p>这个方案麻烦的是生成配置文件，以及后续增加删除节点时修改配置文件。</p>
<p>需要提供已知的所有节点的网络信息。遍历所有的节点，为每个节点生成一个相关配置。找到本节点的网络信息，确定本节点的监听端口，用于启动<code>Server</code>；使用除自己之外的其他节点的信息，用于本节点作为<code>Client</code>去连接其他节点。</p>
<p>相当于<code>N * (N - 1)</code>个<code>Client/Server</code>的配置，且需要所有节点信息才能生成对应的配置文件，所以需要集中统一生成。</p>
<p>同样，增加删除节点的时候，也涉及到所有配置文件的修改，需要集中修改，生成新的配置文件，然后下发到所有的节点。</p>
<p>其次它是一个去中心化的系统，实际生产部署的时候是有多个参与方的，每个参与方负责一个节点，相互之间的协调和配合工作量比较大。且需要考虑参与方的机密信息不能泄露，因此需要分成多个步骤，将机密信息生成和协作产生区块链配置分开，进一步增加了操作的复杂程度。</p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol>
<li>参与方之间交互比较多，需要传递信息也比较多。不管是通过发邮件，还是通过其他方式传递，管理都比较困难。<br>比如，需要认为回信息的方式来确认对方已经收到，如果没有及时反馈，发送方重复发送，且信息与之前的不一致，如何处理？如果有人冒充参与方发送了假冒的信息，如何甄别？</li>
<li>因为区块链系统多方协作的特点，上线运营之后可能还会持续有参与方加入和退出，导致节点的配置不断变化。<br>有新增节点，有节点退出，节点的ip地址或者端口可能发生变动。如何应用这些变更？如何同步到其他节点？如何记录历史上的变更？手工操作进行配置升级容易出错，且消耗时间比较长。如果不能做到配置变更时，及时完成节点配置的更新部署，导致节点配置落后或者错误，可能会让整个区块链系统存在安全风险。比如一个参与方已经退出了，但是其他参与方没有及时更新这个信息，导致已经退出的参与方仍然能够访问系统的信息。</li>
</ol>
<h1 id="GitOps"><a href="#GitOps" class="headerlink" title="GitOps"></a>GitOps</h1><p><code>Git</code>是一个开源的分布式版本控制系统，分布式相比集中式的最大区别是<code>Git</code>没有中央版本库，每一位开发者都可以通过克隆远程代码库，在本地机器上初始化一个完整的代码版本，开发者可以把代码的修改提交到本地代码库，也可以把本地的代码库同步到远程的代码库。</p>
<p><code>GitOps</code>是一种持续交付的方式。它的核心思想是将应用系统的配置和部署以声明性的方式存放在<code>Git</code>版本库中。将<code>Git</code>作为交付流水线的核心，开发人员只需要将修改提到至<code>Git</code>，使用<code>Git</code>来加速和简化应用程序部署和运维任务。通过<code>GitOps</code>，当使用<code>Git</code>提交应用系统的配置更改时，自动化的交付流水线会将这些更改应用到实际系统中。</p>
<p>将<code>GitOps</code>方法应用在持续交付流水线上，有诸多优势和特点：</p>
<ul>
<li>自动保证实际的应用系统和<code>Git</code>仓库中的配置是一致的。</li>
<li>更快的部署时间和恢复时间。</li>
<li>稳定且可重现的回滚。<code>Git</code>中保存有历史的配置信息，有问题可以随时切回历史版本。</li>
</ul>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>通过<code>Git</code>来管理系统配置，使用<code>GitOps</code>实现持续交付，在传统系统中已经非常流行。</p>
<p>但是区块链系统比传统系统更加适合这种配置管理方式。因为其配置的产生是一个协作的过程，更能发挥<code>Git</code>作为团队协作工具的特点；参与方本地拥有完整配置，但是只把部分非机密信息共享给其他参与方也符合<code>Git</code>在分布式上的特点。</p>
<p>链的配置涉及到多个参与方，以及相互之间的信息交互，并就此对链的配置进行相应的修改。所以这是一个多方协作的场景，使用<code>Git</code>管理链的配置可以很好的解决现有方案的问题。</p>
<ol>
<li><code>Git</code>可以设置权限，只允许参与方查看和修改存放链配置的仓库。</li>
<li>通过<code>push</code>命令推送本地修改到远程，安全且有回馈。</li>
<li>可以通过<code>PR</code>等功能对修改进行审查，一个或者多个参与方都确认之后才能合并。</li>
<li>其他参与方可以通过<code>git pull</code>拉取最新的配置。</li>
<li><code>Git</code>本身可以记录修改的历史，何时何人做了什么修改都有记录。</li>
</ol>
<p>注意：</p>
<ol>
<li>因为配置里面有<code>ip</code>，端口等比较的敏感信息，可以通过建立私有仓库来避免信息泄露的风险。</li>
<li>可以通过<code>Github</code>&#x2F;<code>Gitlab</code>&#x2F;<code>Gitee</code>等提供的图形界面和扩展功能更加方便的对配置修改进行管理。</li>
</ol>
<h3 id="区块链声明式的配置方式"><a href="#区块链声明式的配置方式" class="headerlink" title="区块链声明式的配置方式"></a>区块链声明式的配置方式</h3><p>要实施<code>GitOps</code>，核心的一点是要将区块链的配置方式改造为声明式。</p>
<p>如果配置方式仍然是命令式的，比如，增加一个节点，删除一个节点等。因为每个节点实施的顺序不同，可能会得出不同的结果，导致不同节点的配置不一致。</p>
<p>因此我们对区块链系统的配置项进行梳理：</p>
<p><img src="/blockchain-with-gitops/configurations.png"></p>
<p>并据此制定一个声明式的配置数据结构：</p>
<p><img src="/blockchain-with-gitops/struct.png"></p>
<p>配置变更时，不再下发配置变更动作，而是直接重新下发所有的配置信息，节点根据这个配置重新生成节点本地的配置文件。</p>
<h3 id="链和节点Git仓库分离"><a href="#链和节点Git仓库分离" class="headerlink" title="链和节点Git仓库分离"></a>链和节点Git仓库分离</h3><p>理论上可以将一个区块链系统所有的配置信息都放在一个仓库中。</p>
<p>但是区块链去中心化的特性，链的配置需要公开，至少在参与方之间公开，以方便多个参与方都可以修改链的配置。但是节点配置中有很多机密信息，比如节点的私钥等，如果公开会造成安全方面的隐患。</p>
<p>因此，将链的配置信息和节点配置信息分开放在两个<code>Git</code>仓库中。</p>
<p>链的配置信息只包含可以公开的信息，比如账户地址等，可以放在一个公开的<code>Git</code>仓库中；而私钥等机密信息存在放节点配置中，放在参与方内部私有<code>Git</code>仓库中。</p>
<h3 id="交付流水线"><a href="#交付流水线" class="headerlink" title="交付流水线"></a>交付流水线</h3><p>以增加一个节点为例。</p>
<ul>
<li>新的参与方首先申请公开的存放链的配置信息的仓库的访问权限。</li>
<li>拉取最新的链的配置，并提交要增加的节点的公开信息，然后以<code>PR</code>的形式提交对链的配置的修改。</li>
<li>增加节点信息的PR经过审批之后，合并进最新的链的配置。</li>
<li>已有节点通过设置链的配置<code>Git</code>仓库的<code>Webhook</code>感知链的配置的变化。</li>
<li>通过本地的配置工具更新本地节点的配置文件，并提交至存放节点配置的<code>Git</code>仓库。</li>
<li>应用部署系统同样通过设置存放节点配置的<code>Git</code>仓库的<code>Webhook</code>感知节点配置的变化。</li>
<li>停掉已经存在的节点应用，并拉取最新的节点配置，重启节点应用，完成整个配置变更。</li>
</ul>
<p><img src="/blockchain-with-gitops/workflow.jpg"></p>
<h1 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h1><p>这里在<code>gitee</code>上创建<a href="https://gitee.com/cita-cloud/gitops-test-chain">链级配置的仓库</a>。</p>
<p>配置工具为<a href="https://github.com/cita-cloud/cloud-config">cloud-config</a>。</p>
<p>在公司内部的<code>Gitlab</code>上创建节点级配置仓库。</p>
<p>使用<code>Jekins</code>，并在相应的<code>Git</code>仓库设置<code>Webhook</code>来自动触发流水线。</p>
<p>区块链系统运行环境为<code>k8s</code>，并且集群中安装<a href="https://argo-cd.readthedocs.io/en/stable/getting_started/">ArgoCD</a>，用于支持<code>GitOps</code>操作。</p>
<h2 id="链级别配置"><a href="#链级别配置" class="headerlink" title="链级别配置"></a>链级别配置</h2><h3 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h3><p>在<code>gitee</code>上创建仓库<code>gitops-test-chain</code>，保存链级配置。设置<code>master</code>分支不能直接<code>push</code>，只能<code>PR</code>的方式合入，并需要多个人的审批才能合入。</p>
<h3 id="参与方加入"><a href="#参与方加入" class="headerlink" title="参与方加入"></a>参与方加入</h3><p>各个参与方申请该仓库的权限，并进行相关的设置，比如设置<code>SSH Key</code>。</p>
<h3 id="链的发起方初始化链级配置"><a href="#链的发起方初始化链级配置" class="headerlink" title="链的发起方初始化链级配置"></a>链的发起方初始化链级配置</h3><p>初始化链级配置并提交至<code>gitops-test-chain</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cloud-config init-chain --chain-name gitops-test-chain</span><br><span class="line">$ cloud-config init-chain-config --chain-name gitops-test-chain --consensus_tag v6.4.0 --controller_tag v6.4.0 --executor_tag v6.4.0 --kms_tag v6.4.0 --network_tag v6.4.0 --storage_tag v6.4.0</span><br><span class="line">$ cd gitops-test-chain/</span><br><span class="line">$ git init</span><br><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Untracked files:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</span><br><span class="line">        .gitignore</span><br><span class="line">        accounts/</span><br><span class="line">        ca_cert/</span><br><span class="line">        certs/</span><br><span class="line">        chain_config.toml</span><br><span class="line"></span><br><span class="line">nothing added to commit but untracked files present (use &quot;git add&quot; to track)</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;init chain config&quot;</span><br><span class="line">$ git remote add origin git@gitee.com:cita-cloud/gitops-test-chain.git</span><br><span class="line">$ git push -u origin master</span><br></pre></td></tr></table></figure>

<h3 id="设置超级管理员"><a href="#设置超级管理员" class="headerlink" title="设置超级管理员"></a>设置超级管理员</h3><p>超级管理员拉取最新配置，生成自己的账号，并将地址设置为为链的<code>admin</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 拉取最新的链的配置</span><br><span class="line">$ git clone git@gitee.com:cita-cloud/gitops-test-chain.git</span><br><span class="line">// 切换到新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout -b set-admin</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 生成admin账户并设置到链级配置中</span><br><span class="line">$ cloud-config new-account --chain-name gitops-test-chain</span><br><span class="line">key_id:1, address:4dafebf719f2a0439387a231977e5209fabb0cca</span><br><span class="line">$ cloud-config set-admin --chain-name gitops-test-chain --admin 4dafebf719f2a0439387a231977e5209fabb0cca</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;set admin&quot;</span><br><span class="line">$ git push --set-upstream origin set-admin</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/set-admin.png"></p>
<p>经过评审之后合并。</p>
<h3 id="参与方1设置共识账户"><a href="#参与方1设置共识账户" class="headerlink" title="参与方1设置共识账户"></a>参与方1设置共识账户</h3><p>参与方1拉取最新配置，生成自己的共识账号，并将地址添加为链的<code>validator</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 拉取最新的链的配置</span><br><span class="line">$ git clone git@gitee.com:cita-cloud/gitops-test-chain.git</span><br><span class="line"></span><br><span class="line">// 切换到新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout -b add-validator1</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 生成共识账户并设置到链级配置中</span><br><span class="line">$ cloud-config new-account --chain-name gitops-test-chain</span><br><span class="line">key_id:1, address:a746b04e30709203d3c8aadcca31bb024bbcf5df</span><br><span class="line">$ cloud-config append-validator --chain-name gitops-test-chain --validator a746b04e30709203d3c8aadcca31bb024bbcf5df</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;append validator1&quot;</span><br><span class="line">$ git push --set-upstream origin add-validator1</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/append-validator.png"></p>
<p>经过评审之后合并。</p>
<h3 id="参与方2设置共识账户"><a href="#参与方2设置共识账户" class="headerlink" title="参与方2设置共识账户"></a>参与方2设置共识账户</h3><p>参与方2拉取最新配置，生成自己的共识账号，并将地址添加为链的<code>validator</code>。</p>
<p>操作同参与方1，这里不再赘述。</p>
<h3 id="链的发起方关闭链级配置"><a href="#链的发起方关闭链级配置" class="headerlink" title="链的发起方关闭链级配置"></a>链的发起方关闭链级配置</h3><p>所有共识参与方都已经添加过共识账户。</p>
<p>链的参与方将链级配置的<code>stage</code>设置为<code>finalize</code>。</p>
<p>此后将无法添加共识账户。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ git checkout -b set-stage-finalize</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 将链级配置的stage设置为finalize</span><br><span class="line">$ cloud-config set-stage --chain-name gitops-test-chain</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;set stage finalize&quot;</span><br><span class="line">$ git push --set-upstream origin set-stage-finalize</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/final.png"></p>
<p>经过评审之后合并。</p>
<h3 id="参与方1添加节点网络信息"><a href="#参与方1添加节点网络信息" class="headerlink" title="参与方1添加节点网络信息"></a>参与方1添加节点网络信息</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ git checkout -b set-node0</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 添加节点0的网络信息</span><br><span class="line">$ cloud-config append-node --chain-name gitops-test-chain --node www.node0.com:40000:node0:k8s</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;set node0&quot;</span><br><span class="line">$ git push --set-upstream origin set-node0</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/set-node.png"></p>
<p>经过评审之后合并。</p>
<h3 id="参与方2添加节点网络信息"><a href="#参与方2添加节点网络信息" class="headerlink" title="参与方2添加节点网络信息"></a>参与方2添加节点网络信息</h3><p>操作同参与方1，这里不再赘述。</p>
<h3 id="超级管理员创建CA证书"><a href="#超级管理员创建CA证书" class="headerlink" title="超级管理员创建CA证书"></a>超级管理员创建CA证书</h3><p>因为网络采用<code>network_tls</code>，因此需要创建链的<code>CA</code>证书，并为每个节点创建证书。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ git checkout -b create-ca</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 创建CA证书</span><br><span class="line">$ cloud-config create-ca --chain-name gitops-test-chain</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;create ca&quot;</span><br><span class="line">$ git push --set-upstream origin create-ca</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/create-ca.png"></p>
<p>经过评审之后合并。</p>
<h3 id="参与方1创建CSR"><a href="#参与方1创建CSR" class="headerlink" title="参与方1创建CSR"></a>参与方1创建CSR</h3><p>为了不暴露参与方证书的私钥信息，这里采用<code>Certificate Signing Request</code>的方式申请证书。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ git checkout -b create-csr-node0</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 创建CA证书</span><br><span class="line">$ cloud-config create-csr --chain-name gitops-test-chain --domain node0</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;create csr for node0&quot;</span><br><span class="line">$ git push --set-upstream origin create-csr-node0</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/create-csr.png"></p>
<p>评审后合并。</p>
<h3 id="参与方2创建CSR"><a href="#参与方2创建CSR" class="headerlink" title="参与方2创建CSR"></a>参与方2创建CSR</h3><p>操作同参与方1，这里不再赘述。</p>
<h3 id="超级管理员处理CSR"><a href="#超级管理员处理CSR" class="headerlink" title="超级管理员处理CSR"></a>超级管理员处理CSR</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ git checkout -b sign-csr</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 处理CSR，签名</span><br><span class="line">$ cloud-config sign-csr --chain-name gitops-test-chain --domain node0</span><br><span class="line">$ cloud-config sign-csr --chain-name gitops-test-chain --domain node1</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;sign csr&quot;</span><br><span class="line">$ git push --set-upstream origin sign-csr</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/sign-csr.png"></p>
<p>评审后合并。</p>
<h2 id="节点配置"><a href="#节点配置" class="headerlink" title="节点配置"></a>节点配置</h2><h3 id="参与方1初始化节点"><a href="#参与方1初始化节点" class="headerlink" title="参与方1初始化节点"></a>参与方1初始化节点</h3><p>在内部的<code>GitLab</code>上创建节点配置仓库<code>gitops-test-chain-node0</code>。</p>
<p>创建<code>read_write_access token</code>，记录<code>token</code>和同时创建的<code>bot</code>账户，方便后续在流水线中拉取和提交代码。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 拉取最新的链的配置</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git pull</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 初始化node0节点配置</span><br><span class="line">$ cloud-config init-node --chain-name gitops-test-chain --domain node0 --account a746b04e30709203d3c8aadcca31bb024bbcf5df</span><br><span class="line">// 生成node0节点配置文件</span><br><span class="line">$ cloud-config update-node --chain-name gitops-test-chain --domain node0</span><br><span class="line">// 生成node0资源清单</span><br><span class="line">$ cloud-config update-yaml --chain-name gitops-test-chain --domain node0 --storage-class nfs-client</span><br><span class="line"></span><br><span class="line">// 提交节点初始化配置</span><br><span class="line">$ cd gitops-test-chain-node0</span><br><span class="line">$ git init</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;init node config&quot;</span><br><span class="line">$ git remote add origin https://project_xxx_bot:xxxxxxxx@git.XXX.com/xxx/gitops-test-chain-node0.git</span><br><span class="line">$ git push -u origin master</span><br></pre></td></tr></table></figure>

<h3 id="参与方1设置流水线"><a href="#参与方1设置流水线" class="headerlink" title="参与方1设置流水线"></a>参与方1设置流水线</h3><p>在<code>Jekins</code>中创建新的流水线，设置源码为链级配置的仓库: <code>https://gitee.com/cita-cloud/gitops-test-chain.git</code>的<code>master</code>分支，并设置检出到子目录<code>gitops-test-chain</code>。</p>
<p>设置<code>WebHook</code>的<code>token</code>并设置过滤条件，只在<code>master</code>分支有更新的时候才触发流水线。</p>
<p>执行脚本类似前面的初始化节点配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set +e</span><br><span class="line">rm -rf gitops-test-chain-node0</span><br><span class="line">git clone https://project_xxx_bot:xxxxxxxx@git.XXX.com/xxx/gitops-test-chain-node0.git</span><br><span class="line">docker run -i --rm -v `pwd`:`pwd` -w `pwd` citacloud/cloud-config:v6.4.0 cloud-config init-node --chain-name gitops-test-chain --domain node0 --account 6b9ac59d83e9d0744f6231d453e2b883b1819358</span><br><span class="line">docker run -i --rm -v `pwd`:`pwd` -w `pwd` citacloud/cloud-config:v6.4.0 cloud-config update-node --chain-name gitops-test-chain --domain node0</span><br><span class="line">docker run -i --rm -v `pwd`:`pwd` -w `pwd` citacloud/cloud-config:v6.4.0 cloud-config update-yaml --chain-name gitops-test-chain --domain node0 --storage-class nfs-client</span><br><span class="line">cd gitops-test-chain-node0</span><br><span class="line">git add .</span><br><span class="line">git commit -m &quot;update node0 config&quot;</span><br><span class="line">git push -u origin master</span><br></pre></td></tr></table></figure>

<p>在链级配置的仓库中设置<code>WebHook</code>:</p>
<p><img src="/blockchain-with-gitops/webhook.png"></p>
<h3 id="参与方2初始化节点并设置流水线"><a href="#参与方2初始化节点并设置流水线" class="headerlink" title="参与方2初始化节点并设置流水线"></a>参与方2初始化节点并设置流水线</h3><p>操作同参与方1，这里不再赘述。</p>
<h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><p>节点配置仓库到<code>k8s</code>集群的同步使用<code>ArgoCD</code>。</p>
<p>安装和使用方法参见<a href="https://argo-cd.readthedocs.io/en/stable/getting_started/">文档</a>。</p>
<p>创建节点应用，并设置自动同步配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">argocd app create gitops-test-chain-node0 --repo https://project_xxx_bot:xxxxxxxx@git.XXX.com/xxx/gitops-test-chain-node0.git --path yamls --dest-server https://xxx.xxx.xxx.xxx:6443 --dest-namespace default --sync-policy auto</span><br><span class="line"></span><br><span class="line">argocd app create gitops-test-chain-node1 --repo https://project_yyy_bot:yyyyyyyy@git.YYY.com/yyy/gitops-test-chain-node1.git --path yamls --dest-server https://yyy.yyy.yyy.yyy:6443 --dest-namespace default --sync-policy auto</span><br></pre></td></tr></table></figure>

<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="参与方3添加节点网络信息"><a href="#参与方3添加节点网络信息" class="headerlink" title="参与方3添加节点网络信息"></a>参与方3添加节点网络信息</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 拉取最新的链的配置，并创建新的分支</span><br><span class="line">$ git clone git@gitee.com:cita-cloud/gitops-test-chain.git</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git checkout -b set-node2</span><br><span class="line">$ cd ..</span><br><span class="line"></span><br><span class="line">// 添加节点0的网络信息</span><br><span class="line">$ cloud-config append-node --chain-name gitops-test-chain --node www.node2.com:60000:node2:k8s</span><br><span class="line"></span><br><span class="line">// 提交修改</span><br><span class="line">$ cd gitops-test-chain</span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;set node2&quot;</span><br><span class="line">$ git push --set-upstream origin set-node2</span><br></pre></td></tr></table></figure>

<p>创建<code>PR</code>:</p>
<p><img src="/blockchain-with-gitops/add-node2.png"></p>
<p>审核后合并。</p>
<p>此后<code>node0</code>和<code>node1</code>会经由<code>WebHook</code>得知链的配置发生变更，并通过<code>Jenkins</code>流水线自动更新节点配置文件，再通过<code>Argocd</code>，自动将新配置应用到集群中。</p>
<p>因为我们并没有真正的运行<code>node2</code>，所以<code>node0</code>和<code>node1</code>的<code>network</code>微服务应该会报连接错误，如下：</p>
<pre><code> 2022-04-25T12:59:31.881922Z  INFO network::peer: connecting.. peer=gitops-test-chain-node2 host=gitops-test-chain-node2-nodeport port=40000                                                                │
│ 2022-04-25T12:59:36.885651Z  INFO network::peer: connecting.. peer=gitops-test-chain-node2 host=gitops-test-chain-node2-nodeport port=40000
</code></pre>
]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title>在 Kubernetes 中，免费 HTTPS 证书（cert-manager）</title>
    <url>/cert-manager/</url>
    <content><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>我们使用 Certbot 工具向 Let’s Encrypt 免费申请并自动续期证书。</p>
<p>在 Kubernetes Cluster 中，我们使用 cert-manager 组件来实现。</p>
<p>该笔记将记录：在 Kubernetes Cluster 1.22 中，部署 cert-manager 1.7 组件，以及相关问题解决办法。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>集群版本：Kubernetes Cluster v1.22.3-aliyun.1<br>组件版本：cert-manager v1.7（Supported Kubernetes versions: 1.18-1.23）</p>
<h3 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h3><p>1）作为系列部署资源，cert-manager 运行在 Kubernetes Cluster 中，并利用 CRD 来配置 CA 并请求证书；<br>2）部署方式：我们使用官方文档中推荐的 cmctl 命令，不再使用原始的 YAML 清单文件；<br>3）在部署 cert-manager 组件之后，需要创建代表 CA 的 Issuer 或 ClusterIssuer 资源；<br>4）在集群中部署多个 cert-manager 实例会出现意外行为（以前 v1.3 文档提到过，该版本不清楚是否存在该限制）；</p>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">OS=$(go <span class="built_in">env</span> GOOS); ARCH=$(go <span class="built_in">env</span> GOARCH); curl -sSL -o cmctl.tar.gz https://github.com/cert-manager/cert-manager/releases/download/v1.7.2/cmctl-<span class="variable">$OS</span>-<span class="variable">$ARCH</span>.tar.gz</span><br><span class="line">tar xzf cmctl.tar.gz</span><br><span class="line">sudo <span class="built_in">mv</span> cmctl /usr/local/bin</span><br></pre></td></tr></table></figure>

<h3 id="第一步、部署组件"><a href="#第一步、部署组件" class="headerlink" title="第一步、部署组件"></a>第一步、部署组件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cmctl x install --dry-run</span><br></pre></td></tr></table></figure>

<h3 id="第二步、验证安装"><a href="#第二步、验证安装" class="headerlink" title="第二步、验证安装"></a>第二步、验证安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cmctl check api --wait=2m</span></span><br><span class="line">The cert-manager API is ready</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pods --namespace cert-manager</span></span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">cert-manager-75cf8df6b6-t2xns              1/1     Running   0          2m37s</span><br><span class="line">cert-manager-cainjector-857f5bd88c-5xzd7   1/1     Running   0          2m37s</span><br><span class="line">cert-manager-webhook-5cd99556d6-s6jf5      1/1     Running   0          2m37s</span><br></pre></td></tr></table></figure>

<h3 id="第三步、签发测试"><a href="#第三步、签发测试" class="headerlink" title="第三步、签发测试"></a>第三步、签发测试</h3><p>验证方法：通过创建自签名证书，并检查证书是否能够自动签发（参考 <a href="https://cert-manager.io/docs/installation/verify/">Verifying the Installation</a> 文档，以获取具体细节）</p>
<p>我们有使用手工方式来验证（我们通过文档中提到的社区工具来验证，但是失败）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># cat &gt; test-resources.yaml  &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">    name: cert-manager-test</span><br><span class="line">---</span><br><span class="line">apiVersion: cert-manager.io/v1</span><br><span class="line">kind: Issuer</span><br><span class="line">metadata:</span><br><span class="line">    name: test-selfsigned</span><br><span class="line">    namespace: cert-manager-test</span><br><span class="line">spec:</span><br><span class="line">    selfSigned: &#123;&#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: cert-manager.io/v1</span><br><span class="line">kind: Certificate</span><br><span class="line">metadata:</span><br><span class="line">    name: selfsigned-cert</span><br><span class="line">    namespace: cert-manager-test</span><br><span class="line">spec:</span><br><span class="line">    dnsNames:</span><br><span class="line">    - example.com</span><br><span class="line">    secretName: selfsigned-cert-tls</span><br><span class="line">    issuerRef:</span><br><span class="line">    name: test-selfsigned</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># kubectl apply -f test-resources.yaml</span><br><span class="line">...</span><br><span class="line">Status:</span><br><span class="line">    Conditions:</span><br><span class="line">    Last Transition Time:  2022-04-01T09:51:50Z</span><br><span class="line">    Message:               Certificate is up to date and has not expired</span><br><span class="line">    Observed Generation:   1</span><br><span class="line">    Reason:                Ready</span><br><span class="line">    Status:                True</span><br><span class="line">    Type:                  Ready</span><br><span class="line">    Not After:               2022-06-30T09:51:50Z</span><br><span class="line">    Not Before:              2022-04-01T09:51:50Z</span><br><span class="line">    Renewal Time:            2022-05-31T09:51:50Z</span><br><span class="line">    Revision:                1</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># kubectl delete -f test-resources.yaml</span><br><span class="line">namespace &quot;cert-manager-test&quot; deleted</span><br><span class="line">issuer.cert-manager.io &quot;test-selfsigned&quot; deleted</span><br><span class="line">certificate.cert-manager.io &quot;selfsigned-cert&quot; deleted</span><br></pre></td></tr></table></figure>

<p>至此，已完成 cert-manager 部署，接下来便是使用 cert-manager 来申请 Let’s Encrypt 证书（结合我们的需求）。</p>
<h2 id="通过-cert-manager-申请-Let’s-Encrypt-证书"><a href="#通过-cert-manager-申请-Let’s-Encrypt-证书" class="headerlink" title="通过 cert-manager 申请 Let’s Encrypt 证书"></a>通过 cert-manager 申请 Let’s Encrypt 证书</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><p>前面步骤演示如何部署 cert-manager 组件，并成功申请自签名证书，但这并非我们的实际应用场景。</p>
<p>我们希望通过 cert-manager 组件，在集群内完成 Let’s Encrypt 证书申请和管理：<br>1）我们使用 阿里云 DNS，并通过 DNS01 完成域名所有权认证（部分集群在内网，无法使用 HTTP01 认证）；</p>
<h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><p>需要阅读如下文档，以了解相关内容：<br><a href="https://cert-manager.io/docs/configuration/acme/">cert-manager&#x2F;Configuration&#x2F;ACME</a><br>—- <a href="https://cert-manager.io/docs/configuration/acme/dns01/">cert-manager&#x2F;Configuration&#x2F;ACME&#x2F;DNS01</a><br>——– <a href="https://cert-manager.io/docs/configuration/acme/dns01/webhook/">cert-manager&#x2F;Configuration&#x2F;ACME&#x2F;DNS01&#x2F;Webhook</a></p>
<p>我们这里使用 <a href="https://github.com/DEVmachine-fr/cert-manager-alidns-webhook">DEVmachine-fr&#x2F;cert-manager-alidns-webhook</a> 来完成证书申请。</p>
<p>安装 Webhook 部分：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 网络原因，所以我们提前下载 helm chart 文件</span><br><span class="line">wget https://github.com/DEVmachine-fr/cert-manager-alidns-webhook/releases/download/alidns-webhook-0.6.1/alidns-webhook-0.6.1.tgz</span><br><span class="line"></span><br><span class="line"># 查看相关变量</span><br><span class="line">helm show values ./alidns-webhook-0.6.1.tgz</span><br><span class="line"></span><br><span class="line"># 大多数变量不需要修改，除了 groupName 参数</span><br><span class="line">helm -n cert-manager install alidns-webhook ./alidns-webhook-0.6.1.tgz \</span><br><span class="line">    --set groupName=your-company.example.com</span><br></pre></td></tr></table></figure>

<p>创建 ClusterIssuer 资源：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 完成 DNS 质询需要访问阿里云接口来修改 DNS 记录，所以需要使用 KEY 与 TOKEN 来认证</span><br><span class="line">kubectl create secret generic                      \</span><br><span class="line">    alidns-secrets                                 \</span><br><span class="line">    --from-literal=&quot;access-token=your-token&quot;       \</span><br><span class="line">    --from-literal=&quot;secret-key=your-secret-key&quot;</span><br><span class="line"></span><br><span class="line"># 创建 ClusterIssuer 资源</span><br><span class="line">kubectl apply -f &lt;&lt;EOF</span><br><span class="line">apiVersion: cert-manager.io/v1</span><br><span class="line">kind: ClusterIssuer</span><br><span class="line">metadata:</span><br><span class="line">    name: letsencrypt</span><br><span class="line">spec:</span><br><span class="line">    acme:</span><br><span class="line">    # 修改，邮箱地址</span><br><span class="line">    email: contact@example.com</span><br><span class="line">    # 修改，生产地址：https://acme-v02.api.letsencrypt.org/directory</span><br><span class="line">    server: https://acme-staging-v02.api.letsencrypt.org/directory</span><br><span class="line">    privateKeySecretRef:</span><br><span class="line">        name: letsencrypt</span><br><span class="line">    solvers:</span><br><span class="line">    - dns01:</span><br><span class="line">        webhook:</span><br><span class="line">            # 修改，应用我们刚才创建的 Secret 资源</span><br><span class="line">            config:</span><br><span class="line">                accessTokenSecretRef:</span><br><span class="line">                key: access-token</span><br><span class="line">                name: alidns-secrets</span><br><span class="line">                regionId: cn-beijing</span><br><span class="line">                secretKeySecretRef:</span><br><span class="line">                key: secret-key</span><br><span class="line">                name: alidns-secrets</span><br><span class="line">            # 修改，需要填写在安装时指定的 groupName 信息</span><br><span class="line">            groupName: example.com</span><br><span class="line">            solverName: alidns-solver</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h1 id="在-Ingress-中，使用-HTTPS-证书："><a href="#在-Ingress-中，使用-HTTPS-证书：" class="headerlink" title="在 Ingress 中，使用 HTTPS 证书："></a>在 Ingress 中，使用 HTTPS 证书：</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># https://cert-manager.io/docs/usage/ingress/</span><br><span class="line">kubectl apply -f &lt;&lt;EOF</span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">    annotations:</span><br><span class="line">    # 修改，暗示要使用的 issuer 资源，由管理员提供</span><br><span class="line">    cert-manager.io/cluster-issuer: nameOfClusterIssuer</span><br><span class="line">    name: myIngress</span><br><span class="line">    namespace: myIngress</span><br><span class="line">spec:</span><br><span class="line">    rules:</span><br><span class="line">    ...</span><br><span class="line">    tls:</span><br><span class="line">    - hosts:</span><br><span class="line">    # 修改，需要签发证书的域名</span><br><span class="line">    - example.com</span><br><span class="line">    # 修改，保存证书的 Secret 资源（cert-manger 负责创建）</span><br><span class="line">    secretName: myingress-cert</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://github.com/DEVmachine-fr/cert-manager-alidns-webhook">GitHub - DEVmachine-fr&#x2F;cert-manager-alidns-webhook</a><br><a href="https://cert-manager.io/docs/installation/">Installation | cert-manager</a><br><a href="https://cert-manager.io/docs/installation/cmctl/">cmctl | cert-manager</a><br><a href="https://cert-manager.io/docs/usage/ingress/">Securing Ingress Resources | cert-manager</a></p>
]]></content>
      <categories>
        <category>云计算</category>
      </categories>
  </entry>
  <entry>
    <title>聊一聊控制器模式</title>
    <url>/chat-from-controller-pattern/</url>
    <content><![CDATA[<h2 id="开头"><a href="#开头" class="headerlink" title="开头"></a>开头</h2><p>容器的出现和k8s的普及已经改变了我们传统的运维方式，这些技术给我们带来了更多的资源利用率和更强的容错能力。</p>
<p>这其中控制器模式必然占据了举足轻重的作用。这种设计模式是开发和扩展Kubernetes的核心，在Kubernetes的代码仓库中能到处看到它的身影。</p>
<h2 id="声明式or命令式"><a href="#声明式or命令式" class="headerlink" title="声明式or命令式"></a>声明式or命令式</h2><p>当然，当提到控制器模式时候，你也一定会看到声明式API这种东西，这两种一定是同时搭配使用的。</p>
<p>再之，提到“声明式”，你也必然会在大量的文章中看到“命令式”与“声明式”的比较。这些文章一般会说：</p>
<blockquote>
<ol>
<li>与命令性API相比，声明性API的使用更加简洁，并且提供了更好的抽象性。</li>
<li>“命令式”强调的是“how”，你必须step-by-step告诉计算机如何完成一项工作（类似自己做菜）。“声明式”只需要告诉计算机你想要什么，声明你的”what”，计算机会为你完成具体的工作（类似于去饭店点菜吃饭）</li>
</ol>
</blockquote>
<p>还有的教程上会说：</p>
<blockquote>
<p>“声明式 API”允许有多个 API 写端，以 PATCH 的方式对 API 对象进行修改，而无需关心本地原始 YAML 文件的内容。</p>
</blockquote>
<p>在我看来，声明式能够提供所谓的更简洁的API，且有Patch的能力，<strong>这些并不是我们常用的命令式API所不能做到的</strong>。</p>
<p>如果你觉得API不够简洁抽象，那你应该去考虑你设计上是否存在问题，或者没按照规范来？</p>
<p>想要命令式API具有Patch的能力，那可以把资源的属性全部传过去，让你的接口具有类似compare and update的能力不就行了？</p>
<p>所以，不管声明式还是命令式，我觉得核心还是在背后处理API的方式上的不同，这种处理方式通常叫做”控制器模式”。</p>
<h2 id="云平台的困境"><a href="#云平台的困境" class="headerlink" title="云平台的困境"></a>云平台的困境</h2><p>在之前的工作中，我们基于IaaS平台来实现业务。与web服务不同的是，IaaS平台存在更多的耗时请求，且请求过程中出现报错、超时等错误更是家常便饭。</p>
<p>不管是自研IaaS还是二次开发，针对一个多阶段的耗时请求，为了保证发生异常时不存在垃圾资源，通常会实现与正向操作相反的方法，我们称之为rollback方法。</p>
<p>就拿创建虚拟机实例的例子来说，一个createVmInstance操作通常会有很多阶段(正向)：</p>
<p>[AllocateVolume -&gt; AllocateNic -&gt; CreateOnHypervisor…]</p>
<p>这里的每一步都会报错，那么为了报错之后保持环境干净，你必须提供相应的回滚操作(反向)：</p>
<p>[DeleteOnHypervisor -&gt; DeleteNic -&gt; DeleteVolume]</p>
<p>这里通常会设计成一个Stack，当执行一个正向操作时，将相应的反向操作压栈；当发生错误时对栈进行Pop，依次执行回滚操作。</p>
<p>有点像<strong>Saga的分布式事务</strong>。</p>
<p>但是这里会有个几个问题：</p>
<ol>
<li><p>我在执行回滚函数时又发生了报错该怎么办？这时候可能就需要定时GC相关的逻辑来处理垃圾资源。然后，用户通过重试请求createVmInstance的方式再次创建，循环往复。</p>
</li>
<li><p>假如vm创建成功，其下一个子资源被人误删(虽然很多时候会报device is busy删除失败)，那这个vm只能一直处于错误状态了。</p>
</li>
</ol>
<h2 id="控制器模式"><a href="#控制器模式" class="headerlink" title="控制器模式"></a>控制器模式</h2><p>控制器模式可以很好地解决上面解决上面这两个问题。</p>
<p>下面以k8s中的控制器来说明。</p>
<p>Kubernetes控制器会追踪一个或多个资源，并且将资源描述成如下结构：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Object <span class="keyword">struct</span> &#123;</span><br><span class="line">    metav1.TypeMeta</span><br><span class="line">    metav1.ObjectMeta</span><br><span class="line">    Spec ObjectSpec</span><br><span class="line">    Status ObjectStatus</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>除了一些必要的元数据外，还有两个字段: Spec(期望)和Status(当前状态)。控制器不断监控该资源，比较Spec的定义和当前的实际环境，在一个调谐循环中尝试将当前实际环境变换成期望的状态。伪代码如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">  desired := getDesiredState()</span><br><span class="line">  current := getCurrentState()</span><br><span class="line">  makeChanges(desired, current)</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<p>这也就是我们常说的最终一致性，当整个系统恢复正常时，总能给你一个你想要的状态的资源，哪怕你中间误删了一些子资源。</p>
<p>在Kubernetes中，主要通过Informer机制来实现，它作为客户端被使用在每一个Kubernetes的控制器逻辑中。<br>其中，为了保证事件不丢失，实现了list-watch机制， cache机制减少了对apiserver的请求压力，还有限速队列等保护机制等，这里不再展开叙述。</p>
<h2 id="现实中的例子"><a href="#现实中的例子" class="headerlink" title="现实中的例子"></a>现实中的例子</h2><p>控制器模式让我想起了我大学时的自动化专业，在自动控制领域中，闭环的负反馈控制系统如下所示：</p>
<p><img src="/chat-from-controller-pattern/auto_controller.png"></p>
<p>可以看到有类似的地方：通过不断地比较输入信号和反馈信号，再经过相应的算法（如经典的PID算法），以达到输出信号和输入信号趋于一致的状态。</p>
<p>这些控制器的例子在生活中比比皆是：空调温湿度调节，电机转速等。</p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>在分布式环境下，错误永远不能避免，如果你想让你的软件达到Always On的效果，可能就需要引入控制器模式。当然，基于Kubernetes之上运行的大多数软件，已经不需要再考虑这些问题了。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第十期-cloud-init快速开始</title>
    <url>/cloud-init-quickstart/</url>
    <content><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>以前，在操作系统的安装过程中：首先，我们需要准备操作系统镜像（通常是 ISO 镜像），并制作启动盘以从中启动。然后，在系统安装的过程中，用户与安装界面交互，填写必要的操作系统安装参数。最后，操作系统安装工具根据用户填写的参数完成系统安装与配置。</p>
<p>后来，出现无人值守安装：即预先将在以往安装过程中需要填写的操作参数写入配置文件。然后，向操作系统安装工具传递该配置文件的路径或读取方法。最后，操作系统安装工具在启动时，自动读取配置文件，以自动完成操作系统安装。</p>
<p>现在，我们发现：对于已安装完成的操作系统，能够通过克隆复制，直接得到可运行的操作系统。例如：对于安装到硬盘的操作系统，我们能够使用硬盘工具，直接克隆到新硬盘；对于安装到虚拟机的操作系统，我们能够直接复制虚拟机磁盘文件。所有没有必要再进行复杂、冗长的操作系统安装过程。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>cloud-init，正是这样一个工具，通过 cloud-init 装置，我们能够彻底省略安装过程，利用已完成安装的虚拟磁盘文件，更快速的完成操作系统部署过程。</p>
<p>cloud-init 是行业标准的跨平台云实例初始化的多分发方法，所有主要的公共云提供商、私有云基础设施的供应系统和裸机安装都支持它。多数云环境都是通过这种方式来完成操作系统的快速安装部署。</p>
<h3 id="原理简述"><a href="#原理简述" class="headerlink" title="原理简述"></a>原理简述</h3><p>为了使用 cloud-init 装置，我们需要准备 镜像 与 配置 两样东西：</p>
<ul>
<li>1）镜像：是已经安装完成的操作系统镜像文件（比如虚拟机磁盘 VMDK 文件），而不再需要操作系统安装镜像；</li>
<li>2）配置：我们再编写描述操作系统信息的配置文件，该配置文件包含 主机名、帐号密码、网络配置、磁盘配置 等等配置信息；</li>
<li>3）当镜像启动时，在镜像内置的 cloud-init 进程随之启动，通过读取该配置文件，在启动过程中直接完成操作系统的配置；</li>
</ul>
<h2 id="概念术语"><a href="#概念术语" class="headerlink" title="概念术语"></a>概念术语</h2><h3 id="Cloud-Image"><a href="#Cloud-Image" class="headerlink" title="Cloud Image"></a>Cloud Image</h3><p>我们前面提到的“镜像”，在 cloud-init 中，被称为“Cloud Image”：</p>
<ul>
<li>1）它是个已完成操作系统安装的磁盘文件；</li>
<li>2）每个虚拟机的磁盘都是 Cloud Image 的克隆；</li>
</ul>
<h3 id="Datasource"><a href="#Datasource" class="headerlink" title="Datasource"></a>Datasource</h3><p>我们前面提到的“配置”，在 cloud-init 中，被称为“Datasource”：</p>
<ul>
<li>1）为每个虚拟机提供各种配置，比如 Hostname、Network Configuration、Password 等等；</li>
<li>2）该配置由用户负责编写；</li>
</ul>
<p>Datasource 主要提供两个配置文件：user-data；meta-data；</p>
<p>获取 Datasource 的常用方法有两种：</p>
<ul>
<li>1）HTTP：通过 HTTP 获取配置文件地址，而地址已预先硬编码到 Cloud Image 中（很多云厂商使用这种方式）；</li>
<li>2）NoCloud：将配置文件打包到 ISO 镜像，并挂载到虚拟机中；</li>
</ul>
<h3 id="cloud-init"><a href="#cloud-init" class="headerlink" title="cloud-init"></a>cloud-init</h3><p>被集成到 Cloud Image 中，当镜像启动时，将运行 cloud-init 进程；</p>
<p>当运行 cloud-init 服务时，主要完成两项工作：</p>
<ul>
<li>1）探测并读取 Datasource 配置；</li>
<li>2）将这些配置应用到当前的虚拟机实例中；</li>
</ul>
<h2 id="安装使用"><a href="#安装使用" class="headerlink" title="安装使用"></a>安装使用</h2><h3 id="第一步、创建镜像"><a href="#第一步、创建镜像" class="headerlink" title="第一步、创建镜像"></a>第一步、创建镜像</h3><p>下载 Cloud Image 文件：<a href="https://cloud-images.ubuntu.com/">https://cloud-images.ubuntu.com/</a></p>
<pre><code># .img 文件为 QCOW2 格式
wget http://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img

# 创建磁盘文件 
qemu-img convert -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw
qemu-img resize focal-server-cloudimg-amd64.raw 100G

# 创建磁盘文件：此方式使用 QCOW2 的写时复制特性，hal9000.img 引用 focal-server-cloudimg-amd64.img 文件，空间占用小
qemu-img create -b focal-server-cloudimg-amd64.img -f qcow2 -F qcow2 hal9000.img 10G
</code></pre>
<h3 id="第二步、创建配置"><a href="#第二步、创建配置" class="headerlink" title="第二步、创建配置"></a>第二步、创建配置</h3><pre><code>apt-get install whois cloud-image-utils

mkpasswd -m sha512crypt 123456 -S &quot;12345342&quot;

cat &gt; user-data &lt;&lt;EOF
#cloud-config

hostname: focal-server
manage_etc_hosts: localhost

users:
  - name: root
    lock_passwd: false
    hashed_passwd: &#39;&lt;the output of mkpassword...&gt;&#39;
    ssh_authorized_keys:
      - ssh-rsa AAAAB3NzaC1y...

# SSH
ssh_pwauth: True
disable_root: false
EOF

cloud-localds user-data.iso user-data
</code></pre>
<h3 id="第三步、创建虚拟机"><a href="#第三步、创建虚拟机" class="headerlink" title="第三步、创建虚拟机"></a>第三步、创建虚拟机</h3><p>然后，在虚拟机中同时挂载 focal-server-cloudimg-amd64.raw 与 user-data.iso 文件；</p>
<pre><code>virt-install              \
    --vcpus=4             \
    --ram=8192            \
    --import              \
    --os-variant=ubuntu20.04      \
    --network network=cluster-network,model=virtio                \
    --graphics vnc,listen=0.0.0.0 \
    --noautoconsole               \
    --disk path=/srv/isos/user-data.iso,device=cdrom              \
    --disk path=/srv/image/focal-server-cloudimg-amd64.raw,format=qcow2   \
    --name=focal-server
</code></pre>
<p>当开机启动完成后，便可通过 root&#x2F;123456 进行登录；</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cloudinit.readthedocs.io/en/latest/">cloud-init Documentation — cloud-init 22.1 documentation</a></p>
<p><a href="https://askubuntu.com/questions/451673/default-username-password-for-ubuntu-cloud-image">12.04 - Default username&#x2F;password for Ubuntu Cloud image? - Ask Ubuntu</a></p>
<p><a href="https://cloudinit.readthedocs.io/en/latest/topics/network-config-format-v2.html#">Networking Config Version 2 — cloud-init 22.1 documentation</a></p>
<p><a href="https://serverfault.com/questions/920117/how-do-i-set-a-password-on-an-ubuntu-cloud-image">linux - How do I set a password on an Ubuntu cloud image? - Server Fault</a></p>
<p><a href="https://github.com/vmware/photon/issues/659">How to set root password with cloud-config? · Issue #659 · vmware&#x2F;photon · GitHub</a></p>
<p><a href="https://askubuntu.com/questions/451673/default-username-password-for-ubuntu-cloud-image">12.04 - Default username&#x2F;password for Ubuntu Cloud image? - Ask Ubuntu</a></p>
<p><a href="https://www.tcc-consulting.com.hk/110/cloud-technology/cloud-init-to-enable-cloud-image-root-login/">Cloud-init To Enable Cloud Image Root Login – TCC Consulting Limited</a></p>
<p><a href="https://mcwhirter.com.au/craige/blog/2015/Enable_Root_Login_Over_SSH_With_Cloud-Init_On_OpenStack/">mcwhirter.com.au&#x2F;craige&#x2F;blog&#x2F;2015&#x2F;Enable Root Login Over SSH With Cloud-Init on OpenStack</a></p>
<p><a href="https://sumit-ghosh.com/articles/create-vm-using-libvirt-cloud-images-cloud-init/">Creating a VM using Libvirt, Cloud Image and Cloud-Init | Sumit’s Dreams of Electric Sheeps</a></p>
]]></content>
      <categories>
        <category>云计算</category>
      </categories>
  </entry>
  <entry>
    <title>可移动数据中心的构想与实现</title>
    <url>/edge-dc/</url>
    <content><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>现在，在我们的工作环境中，以笔记本为办公平台，在其上完成例如 编码、远程、文档处理 等等工作。对于其他数据资源，则保存在远程文件托管服务（例如 NAS 或 云盘服务 等等）中，通过网络进行远程访问及数据备份。</p>
<p>但是，该方案的最大问题是我们无法保证网络总是可用或高速，在某些特殊工作环境中，或无法访问网络，或网络速度受限，或网络质量不稳定，导致我们无法访问远程文件托管服务中的资源。</p>
<p>所以，我们尝试将经常访问的__数据资源本地化__，即将常用访问的数据直接保存在笔记本电脑上，并在其上运行__备份服务__以将我们的数据备份到远程文件托管服务。鉴于此，即使网络资源成为瓶颈，也不会影响我们对数据资源的访问，而远程文件托管服务则作为 <strong>极低频访问资源的存储</strong> 及 <strong>笔记本数据备份的后端存储</strong> 而存在。</p>
<p>但是，随着办公平台的扩大，我们平时不得不以 Linux 系统为主要办公环境，但是有的时候会用到 Windows 系统，尤其是需要运行仅支持 Windows 平台的软件（例如 企业微信、钉钉、微信 等等）。我们尝试更换 Macbook，但并不能解决问题；两个笔记本，携带也不方便；通过 Wine 方案，仅能解决部分软件的运行问题，仍旧存在部分软件无法通过 Wine 来运行。</p>
<p>所以，我们尝试在虚拟机中运行操作系统，将我们的办公环境迁移到虚拟机。我们在宿主机中运行 Linux 操作系统，并在其中部署桌面虚拟化（例如 VirtualBox 等等），并在虚拟机中运行 Windows 操作系统。同时，借助虚拟机的 Guest Additions 组件，实现宿主机与虚拟机之间互操作（例如 文件共享、复制粘贴 等等）</p>
<p>但是，很多时候两个操作系统（Windows&#x2F;Linux）都要使用相同的服务。例如：或为了提供网络质量，两个操作系统都需要使用网络加速服务；或为了访问办公网络，两个操作系统都要接入企业 VPN 服务；</p>
<p>所以，我们开始思考，既然虚拟机的流量是通过 NAT 进行网络访问，那能不能在虚拟化中运行路由器操作系统，然后所有的虚拟机操作系统将数据包发往路由器操作系统，而路由器操作系统的其他接口负责将数据包发送到外部网络。这样的网络模型就更加贴近于现实环境的终端网络，事情也变得越来越疯狂。</p>
<p>关键词：可移动数据中心、边缘数据中心、便携式数据中心、虚拟数据中心</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>根据我们的想法进行描绘，整个系统原型类似如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-------------------------------------------------+</span><br><span class="line">|   OpenWrt   | Windows VM | Linux VM |    ...    |</span><br><span class="line">+-------------------------------------------------+</span><br><span class="line">|   LINUX + KVM + Storage                         |</span><br><span class="line">+-------------------------------------------------+</span><br><span class="line">|                     LAPTOP                      |</span><br><span class="line">+-------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>LAPTOP</strong>：最底层是笔记本物理硬件本身，所有的环境将运行在其上。这其中既有配电设施，又有冷却设施，运维管理中心则为本机。至于机柜和布线系统，鉴于整个系统的基于多种虚拟化技术实现，所以完全不需要机柜和布线系统。</p>
<p><strong>LINUX + KVM + Storage</strong>：向上则是虚拟化环境，运行 Linux 操作系统，并在其中部署 KVM 虚拟化，最后将物理存储加入到 KVM 虚拟化的存储池中，以向上层虚拟机提供存储服务。</p>
<p><strong>OpenWrt</strong>：作为虚拟机访问外部网络的网关，所有虚拟机流量将通过 OpenWrt 路由到外部网络。对于 KVM 环境，运行 <a href="https://openwrt.org/docs/guide-user/virtualization/qemu">OpenWrt in QEMU</a> 路由，网络加速也在 OpenWrt 中实现，通过全局流量检查来同时实现Linux 和 Windows 的网络加速，如此便无需在两个系统中安装客户端。</p>
<p><strong>Windows VM、Linux VM</strong>：作为实际我们办公的操作系统，且所有虚拟机网卡与 OpenWrt 网卡位于相同的二层网络，所有的虚拟机流量将发送到 OpenWrt，并由 OpenWrt 的另张网卡发送到外部网络。</p>
<p><strong>虚拟机的数据共享</strong>：鉴于多个虚拟机间的很多数据、文件、程序是需要共享的，我们通过用 NFS 或 CIFS 来实现。但是存储还是落在主机上，所以需要使用 KVM 隔离网络来允许虚拟机访问宿主机，这导致但个虚拟机至少有两块网卡。</p>
<p><strong>虚拟机的桌面访问</strong>：宿主机具备图形化桌面，我们使用 virt-manager 对 KVM 虚拟机进行管理及桌面访问，对于虚拟机间的复制粘贴，通过 SPICE Agent 能够解决。</p>
<p>如此，我们便能将数据中心装在包里，随身携带。硬件配置要求，取决于个人工作负载，我们平时运行 Linux 及相关的应用就要 16G 的内存，主要是应用程序开的多。</p>
<h2 id="首个实现"><a href="#首个实现" class="headerlink" title="首个实现"></a>首个实现</h2><p>我们已实现该技术方案的首个版本，但是多少有些出入（但差别并不大）：</p>
<p><img src="/edge-dc/pasted_image.png"></p>
<p>共 4 层（从上到下，L4、L3、L2、L1）</p>
<p>虚拟化选用 VirtalBox 的原因是：<br>1）鉴于是移动数据中心，所以涉及桌面环境事件响应（例如 休眠处理 等等），而桌面虚拟化软件处理的更好；<br>2）VritualBox 免费；</p>
<p>鉴于虚拟化（L2）及物理层（L1）暂无特殊配置，所以不再详细说明。该部分的后续内容将概述网络层（L3）与客户机层（L4）的实现，及相关问题处理。</p>
<h3 id="第一步、创建-OpenWrt-实例"><a href="#第一步、创建-OpenWrt-实例" class="headerlink" title="第一步、创建 OpenWrt 实例"></a>第一步、创建 OpenWrt 实例</h3><p>在 VirtualBox 中，部署 OpenWrt 服务：参考 <a href="https://openwrt.org/docs/guide-user/virtualization/virtualbox-vm%20">[OpenWrt Wiki] OpenWrt on VirtualBox HowTo</a> 文档，获取官方配置说明。</p>
<p>我们需要三张不同网络网络类型的网卡（尽量依序创建）：<br>1）NAT：负责网络访问，否则 OpoenWrt 将无法上网；（相当于 WAN 接口）<br>2）Internal Network：实现 OpenWrt、Linux 的二层互联；（相当于 LAN 接口）<br>3）Host-only Adapter：能与主机通信，用于从主机连接和管理 OpenWrt 服务；</p>
<h3 id="第二步、配置-OpenWrt-服务"><a href="#第二步、配置-OpenWrt-服务" class="headerlink" title="第二步、配置 OpenWrt 服务"></a>第二步、配置 OpenWrt 服务</h3><p>我们需要配置 OpenWrt 服务，使其成为路由设备：<br><a href="https://openwrt.org/docs/guide-user/network/openwrt_as_routerdevice%20">[OpenWrt Wiki] OpenWrt as router device</a></p>
<p>如果遇到问题，或许需要使用 TCPDump 抓包：<br><a href="https://openwrt.org/docs/guide-user/firewall/misc/tcpdump_wireshark%20">[OpenWrt Wiki] How to capture, filter and inspect packets using tcpdump or wireshark tools</a></p>
<h3 id="第三步、配置-Linux-Guest-实例"><a href="#第三步、配置-Linux-Guest-实例" class="headerlink" title="第三步、配置 Linux Guest 实例"></a>第三步、配置 Linux Guest 实例</h3><p>创建 Linux Guest 主机，并添加 Internal Network 类型网络，使其与 OpenWrt 二层互联；</p>
<p>然后，启动 Linux Guest 实例；</p>
<p>如果配置正确，Linux Guest 能够通过 OpenWrt 的 DHCP 获取 IP 地址；</p>
<h3 id="后续的改进工作"><a href="#后续的改进工作" class="headerlink" title="后续的改进工作"></a>后续的改进工作</h3><p>首个版本已能够运行，但是还有很多改进工作：</p>
<p>1）<strong>继续使用 VirtualBox 桌面虚拟化</strong>：KVM 非桌面虚拟化，对桌面场景处理不是很好。比如 当 KVM 休眠恢复后，时间跳跃导致 Guest CPU Usage 飙升；</p>
<p>2）<strong>替换 Host 系统为 Linux 发行版</strong>：Window 10 + VirtualBox 的问题是，网卡插拔后（间隔久一点），Guest Bridage Network 无法就再也无法访问网络；Linux + VirtualBox 经过测试暂无问题。桥接网络是为了 OpenWrt 与 另个数据中心运行动态路由协议，如果用 NAT Network 就无法运行动态路由协议。另外，笔记本的有线网卡&#x2F;无线网卡可以进行 主&#x2F;备&#x2F;负载。</p>
<p>3）<strong>引入存储（待定）</strong>：该改进的目的是使得整个模型更加贴近于终端环境，并通过 FCoE 或 iSCSI 的方式提供块存储。但并无必要性，虚拟机的磁盘扩容也是在文件系统上扩容的，没必要引入独立的存储服务。后来（04&#x2F;27&#x2F;2022）我们觉得还是要引入存储服务，为多个 Guest 提供共享的网络文件系统存储，实现文件共享，同时使得整个模型更加贴近于数据中心。</p>
<h3 id="改进工作的推进"><a href="#改进工作的推进" class="headerlink" title="改进工作的推进"></a>改进工作的推进</h3><p>我们确实对整个模型进行改进</p>
<p><img src="/edge-dc/pasted_image001.png"></p>
<p>1）<strong>替换 Host 系统为 Linux 发行版</strong>：Ubuntu 20.04 LTS + VirtualBox 6.1.32</p>
<p>2）<strong>引入存储</strong>：使用 OpenMediaVault 作为存储，目前主要是将其作为网络文件系统来使用：（1）在多个 Guest 间通过网络分享文件和数据；（2）将数据从 Linux Guest 脱离出来，数据备份工作转移到存储服务层面；</p>
<p>3）<strong>替换 OpenWrt 组件</strong>：我们引入 pfSense 防火墙，以利用其中的 FRR 模块与 L2TP 模块；</p>
<h2 id="低廉的分布式数据中心"><a href="#低廉的分布式数据中心" class="headerlink" title="低廉的分布式数据中心"></a>低廉的分布式数据中心</h2><p>为了能从办公室访问家里的网络（我们没有出口路由的控制权），而且是任意访问（我们希望直接访问 80 端口，但是受限于网络环境，80 端口默认被运营商屏蔽），所以我们通过部署 L2TP VPN 解决。通过十分服务器部署 LNS 服务，家里的路由器和笔记都作为 LAC 拨号到 LNS，形成二层网络，我们便能通过内网地址直接访问家里的服务。但缺点也很明显，双端需要配置静态路由，否则网络无法互通。虽然需要静态路由，但也不影响使用。而在思考改进该缺点的时候，我们得到创建分布式的低廉数据中心的灵感。</p>
<p>我们的目标很简单：利用当前公网架构，将分散在各地理位置的资源集结到一起，形成大型逻辑网络。于是便产生如下拓扑（相关的技术细节不再赘述，该图已披露出关键的技术要点）</p>
<p><img src="/edge-dc/pasted_image002.png"></p>
]]></content>
  </entry>
  <entry>
    <title>在Rust中实现gRPC重试</title>
    <url>/grpc-retry-in-rust/</url>
    <content><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p><code>CITA-Cloud</code>采用了微服务架构，微服务之间以及对应用暴露的接口都采用了<code>gRPC</code>。</p>
<p><code>gRPC</code>调用时可能会返回错误，需要对错误进行处理。经过讨论之后，我们采用的<a href="https://github.com/cita-cloud/rfcs/pull/7/files">方案</a>是将错误码分成两层。应用层面的错误用单独的<code>status_code</code>来表示；<code>gRPC</code>本身的错误会用响应中的<code>Status</code>来表示。</p>
<p>前段时间碰到了可能是网络抖动造成的客户端返回<code>UNAVAILABLE</code>的现象。因为这个是<code>gRPC</code>本身的错误，应用层没办法处理，只能靠客户端重试来解决。</p>
<h2 id="gRPC重试"><a href="#gRPC重试" class="headerlink" title="gRPC重试"></a>gRPC重试</h2><p>针对这个需求，可以使用<code>gRPC</code>本身就提供的拦截器功能。通过注入一个拦截器，检查每次调用的结果，如果返回错误（并不是每种错误都可以重试的，具体参见官方关于<a href="https://grpc.github.io/grpc/core/md_doc_statuscodes.html">错误码的描述</a>），则再次发起调用。</p>
<p>在<code>golang</code>这样的亲儿子上，甚至已经有<a href="https://github.com/grpc-ecosystem/go-grpc-middleware/tree/master/retry">现成的库</a>可以非常方便的做到这样的事情。</p>
<h2 id="gRPC重试-in-Rust"><a href="#gRPC重试-in-Rust" class="headerlink" title="gRPC重试 in Rust"></a>gRPC重试 in Rust</h2><p>因为<code>CITA-Cloud</code>主要使用<code>Rust</code>，结果搜索了一圈，震惊的发现竟然没有现成的库。在<code>Rust</code>这样造轮子情绪高涨的社区里，这是一个很不寻常的情况。</p>
<p>所幸一番搜索之后，发现了相应的<a href="https://github.com/hyperium/tonic/issues/733">原因</a>，还是跟<code>Rust</code>的所有权特性有关系。</p>
<p>因为要在失败后重试，就要复制一份调用的请求参数。这个在其他语言里面根本不是个事，但是在<code>Rust</code>里就麻烦了。</p>
<p><code>tonic</code>(一个纯<code>Rust</code>的<code>gRPC</code>实现)中一个接口的客户端函数原型为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pub async fn store(</span><br><span class="line">            &amp;mut self,</span><br><span class="line">            request: impl tonic::IntoRequest&lt;super::Content&gt;,</span><br><span class="line">        ) -&gt; Result&lt;tonic::Response&lt;super::super::common::StatusCode&gt;, tonic::Status&gt;</span><br></pre></td></tr></table></figure>

<p>请求的类型是<code>tonic::IntoRequest&lt;T&gt;</code>（其中的<code>T</code>为请求中的应用层数据结构），这个类型是没有实现<code>Clone</code>的。</p>
<p>至于为什么不实现，开发者的解释是要考虑到<code>gRPC</code>的<code>stream</code>模式，<code>stream</code>中的请求是没法<code>Clone</code>的。</p>
<p>那非<code>stream</code>模式可以实现吗？答案也是不行，因为<code>gRPC</code>是基于<code>Http2</code>的，<code>Http2</code>总是<code>stream</code>的，因此单次调用模式其实就是只包含一个请求的<code>stream</code>。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>请求的类型<code>tonic::IntoRequest&lt;T&gt;</code>无法<code>Clone</code>，但是里面的<code>T</code>通常都是可以<code>Clone</code>的。</p>
<p>因此在<code>Rust</code>中像<code>golang</code>一样通过拦截器来非常优雅的实现重试是做不到了，但是用复杂一点的方法还是可以实现的。</p>
<p>其实说白了就是在应用层，按照最直接的方式来实现重试。在应用层多封装一层函数，其参数是应用层的请求类型<code>T</code>。调用接口之后，判断结果，如果是可重试的错误，则将类型<code>T</code>复制一份，重新发起调用。</p>
<p>当然这样实现的问题是重复的模式化的代码会非常多，所以具体实现还是用了一些技巧尽量让重复的代码少一点。</p>
<p>方案参考了<a href="https://github.com/temporalio/sdk-core/tree/master/client/src">temporalio&#x2F;sdk-core</a>，具体实现参见<a href="https://github.com/cita-cloud/cita_cloud_proto/pull/4/files">代码</a>。</p>
<p>为了复用<code>retry</code>的逻辑，单独抽象出了<code>retry</code>模块。首先定义了<code>RetryClient</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pub struct RetryClient&lt;SG&gt; &#123;</span><br><span class="line">    client: SG,</span><br><span class="line">    retry_config: RetryConfig,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>client</code>是原始的<code>gRPC client</code>，<code>retry_config</code>是重试相关的选项，比如最多重试多少次等。</p>
<p>重试的逻辑在其成员方法<code>call_with_retry</code>中，里面主要用到了<code>FutureRetry</code>，即把整个调用封装成一个<code>Future</code>闭包，退避策略则使用了<code>ExponentialBackoff</code>。</p>
<p>当然最根本的还是前面提到的，要封装一层，使闭包的参数是可以<code>Clone</code>的。这部分都是一些模式化的代码，因此使用了一个宏来自动生成相关代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">macro_rules! retry_call &#123;</span><br><span class="line">    ($myself:ident, $call_name:ident) =&gt; &#123; retry_call!($myself, $call_name,) &#125;;</span><br><span class="line">    ($myself:ident, $call_name:ident, $($args:expr),*) =&gt; &#123;&#123;</span><br><span class="line">        let call_name_str = stringify!($call_name);</span><br><span class="line">        let fact = || &#123; async &#123; $myself.get_client_clone().$call_name($($args,)*).await.map(|ret| ret.into_inner()) &#125;&#125;;</span><br><span class="line">        $myself.call_with_retry(fact, call_name_str).await</span><br><span class="line">    &#125;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>为了让<code>RetryClient</code>能够用于不同的<code>Service</code>，这里会把每个<code>Service</code>的客户端函数定义成一个<code>Trait</code>。比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#[async_trait::async_trait]</span><br><span class="line">pub trait StorageClientTrait &#123;</span><br><span class="line">    async fn store(&amp;self, content: storage::Content) -&gt; Result&lt;common::StatusCode, tonic::Status&gt;;</span><br><span class="line"></span><br><span class="line">    async fn load(&amp;self, key: storage::ExtKey) -&gt; Result&lt;storage::Value, tonic::Status&gt;;</span><br><span class="line"></span><br><span class="line">    async fn delete(&amp;self, key: storage::ExtKey) -&gt; Result&lt;common::StatusCode, tonic::Status&gt;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意这里的函数原型是封装之后的。</p>
<p>然后为<code>RetryClient</code>相对应的特化类型实现这个<code>Trait</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#[async_trait::async_trait]</span><br><span class="line">impl StorageClientTrait for RetryClient&lt;StorageServiceClient&lt;InterceptedSvc&gt;&gt; &#123;</span><br><span class="line">    async fn store(&amp;self, content: storage::Content) -&gt; Result&lt;common::StatusCode, tonic::Status&gt; &#123;</span><br><span class="line">        retry_call!(self, store, content.clone())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    async fn load(&amp;self, key: storage::ExtKey) -&gt; Result&lt;storage::Value, tonic::Status&gt; &#123;</span><br><span class="line">        retry_call!(self, load, key.clone())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    async fn delete(&amp;self, key: storage::ExtKey) -&gt; Result&lt;common::StatusCode, tonic::Status&gt; &#123;</span><br><span class="line">        retry_call!(self, delete, key.clone())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>内容是完全使用前面的宏来实现的。第一个参数是<code>RetryClient</code>的<code>self</code>，第二个参数是<code>gRPC</code>接口的名称，后面是接口的参数。</p>
<p>这样就实现了一个尽量通用的<code>RetryClient</code>，然后以尽量少的重复代码来为多个<code>Service</code>都实现了重试的功能。</p>
<p>用法可以参见里面的测试代码。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">let mock_client = TestClient::new(code);</span><br><span class="line">let retry_client = RetryClient::new(mock_client, Default::default());</span><br><span class="line">let result = retry_client.test(1).await;</span><br></pre></td></tr></table></figure>
<p>首先按照原有的方法获取底层的<code>Client</code>；然后将其和<code>RetryConfig</code>一起放入<code>RetryClient</code>，得到带重试功能的客户端；用这个客户端调用前述<code>Trait</code>中封装的方法就会自带重试功能了。</p>
]]></content>
      <categories>
        <category>Rust</category>
      </categories>
  </entry>
  <entry>
    <title>在 Pod 中，访问 Kubernetes API 接口，并控制访问权限</title>
    <url>/k8s-service-account/</url>
    <content><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在实际的应用场景中，我们的应用程序运行在集群中（以 Pod 的形式存在），并且该应用程序将在集群中进行创建资源、修改资源、删除资源等等操作。</p>
<p>在 Pod 中，访问 Kubernetes API 的方法有很多，而通过 Client Libraries（程序类库）是官方推荐的做法，也是我们接下来将要学习的方法。</p>
<p>该笔记将记录：在 Pod 中，通过 Client Libraries 访问 Kubernetes API（管理 Kubernetes 集群）的方法，以及相关问题的解决办法。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>接下来我们将介绍例如，如果创建 ServiceAccount 对应用程序进行访问控制，只允许其查看 Pod 资源（即查看 Pod 列表和详细信息）</p>
<h3 id="第-1-步、创建-ServiceAccount-资源"><a href="#第-1-步、创建-ServiceAccount-资源" class="headerlink" title="第 1 步、创建 ServiceAccount 资源"></a>第 1 步、创建 ServiceAccount 资源</h3><p>而这其中最大的问题是，如何进行合理的授权，即对于既定的用户或应用程序，如何允许或拒绝特定的操作？—— 通过 ServiceAccount 实现。</p>
<pre><code># kubectl create serviceaccount myappsa
</code></pre>
<h3 id="第-2-步、引用-ServiceAccount-资源"><a href="#第-2-步、引用-ServiceAccount-资源" class="headerlink" title="第 2 步、引用 ServiceAccount 资源"></a>第 2 步、引用 ServiceAccount 资源</h3><p>定义一个 Pod，使用为 myappsa 的 ServiceAccount 资源：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f - &lt;&lt;EOF</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp</span><br><span class="line">spec:</span><br><span class="line">  serviceAccountName: myappsa</span><br><span class="line">  containers:</span><br><span class="line">  - name: main</span><br><span class="line">    image: bitnami/kubectl:latest</span><br><span class="line">    command:</span><br><span class="line">    - &quot;sleep&quot;</span><br><span class="line">    - &quot;infinity&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>ServiceAccount 是种身份，而在 Pod 中引用 ServiceAccount 则是将这种身份赋予 Pod 实例。而接下来的任务是给 ServiceAccount 这种身份赋予各种权限 —— 具体的做法便是将各种角色（Role）绑定（RoleBinding）到这个身份（ServiceAccount）上。</p>
<h3 id="第-3-步、创建-Role-资源"><a href="#第-3-步、创建-Role-资源" class="headerlink" title="第 3 步、创建 Role 资源"></a>第 3 步、创建 Role 资源</h3><p>定义名为 podreader 的 Role 资源，并定义其能够进行的访问操作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f - &lt;&lt;EOF</span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: podreader</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;]</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 或，直接使用命令创建</span><br><span class="line"># kubectl create role podreader --verb=get --verb=list --resource=pods -n default</span><br></pre></td></tr></table></figure>

<h3 id="第-4-步、将-Role-与-ServiceAccount-绑定"><a href="#第-4-步、将-Role-与-ServiceAccount-绑定" class="headerlink" title="第 4 步、将 Role 与 ServiceAccount 绑定"></a>第 4 步、将 Role 与 ServiceAccount 绑定</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl apply -f - &lt;&lt;EOF</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: podreaderbinding</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: podreader</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: myappsa</span><br><span class="line">    namespace: default</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 或，从从命令行直接创建：</span><br><span class="line"># kubectl create rolebinding podreaderbinding --role=default:podreader --serviceaccount=default:myappsa --namesepace default -n default</span><br></pre></td></tr></table></figure>

<h3 id="第-5-步、访问-Kubernetes-API-测试"><a href="#第-5-步、访问-Kubernetes-API-测试" class="headerlink" title="第 5 步、访问 Kubernetes API 测试"></a>第 5 步、访问 Kubernetes API 测试</h3><p>通过 Service Account 相关信息来访问资源：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 通过我们运行的 kubectl 容器访问</span><br><span class="line">kubectl get pods                                    # 这里是为了体现 kubectl 并未通过 kubeconfig 的信息来访问集群，</span><br><span class="line">                                                    # 而是通过 /var/run/secrets/kubernetes.io/serviceaccount 来访问集群</span><br><span class="line">                                                    # kubectl 手册介绍该命令是如何查找访问信息的；</span><br><span class="line"></span><br><span class="line"># 通过 curl API 访问</span><br><span class="line">APISERVER=https://kubernetes.default.svc</span><br><span class="line">SERVICEACCOUNT=/var/run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">NAMESPACE=$(cat $&#123;SERVICEACCOUNT&#125;/namespace)</span><br><span class="line">TOKEN=$(cat $&#123;SERVICEACCOUNT&#125;/token)</span><br><span class="line">CACERT=$&#123;SERVICEACCOUNT&#125;/ca.crt</span><br><span class="line">curl --cacert $&#123;CACERT&#125; --header &quot;Authorization: Bearer $&#123;TOKEN&#125;&quot; -X GET $&#123;APISERVER&#125;/api/v1/namespaces/default/pods</span><br></pre></td></tr></table></figure>

<h3 id="第-6-步、通过客户端类库访问集群"><a href="#第-6-步、通过客户端类库访问集群" class="headerlink" title="第 6 步、通过客户端类库访问集群"></a>第 6 步、通过客户端类库访问集群</h3><p>以 Java 客户端为例：</p>
<p>如果需要在集群内部访问集群：<br>1）按照如上示例，定义 ServiceAccount 资源，<br>2）并参照 <a href="https://github.com/kubernetes-client/java/blob/master/examples/examples-release-15/src/main/java/io/kubernetes/client/examples/InClusterClientExample.java">InClusterClientExample.java</a> 示例代码</p>
<p>如果需要在集群外部访问集群：<br>1）按照如上示例，在远端集群定义 ServiceAccount 资源，<br>2）并参照 <a href="https://github.com/kubernetes-client/java/blob/master/examples/examples-release-15/src/main/java/io/kubernetes/client/examples/KubeConfigFileClientExample.java%20">java&#x2F;KubeConfigFileClientExample.java</a> 示例代码</p>
<h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><h3 id="访问集群级别的资源"><a href="#访问集群级别的资源" class="headerlink" title="访问集群级别的资源"></a>访问集群级别的资源</h3><p>上面是命名空间内的访问控制设置，因为上述使用的是 Role 和 RoleBinding 资源，而对于集群范围的访问控制应该使用 ClusterRole 和ClusterRoleBinding 命令。</p>
<p>替换 kind 为 ClusterRole 及 ClusterRoleBinding 即可，其他部分与 Role、RoleBinding 类似，这里不再赘述。</p>
<h3 id="通过-ServiceAccount-生成-Kubeconfig-的方法"><a href="#通过-ServiceAccount-生成-Kubeconfig-的方法" class="headerlink" title="通过 ServiceAccount 生成 Kubeconfig 的方法"></a>通过 ServiceAccount 生成 Kubeconfig 的方法</h3><p>如果需要使用 kubeconfig 文件，可通过 ServiceAccount 资源来创建，参考 <a href="https://stackoverflow.com/questions/47770676/how-to-create-a-kubectl-config-file-for-serviceaccount%20">How to create a kubectl config file for serviceaccount</a> 讨论。</p>
<p>简而言之，kubeconfig 的 user 部分为 token，而非 client-certificate-data 与 client-key-data 参数。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/%20">Accessing the Kubernetes API from a Pod | Kubernetes</a><br><a href="https://stackoverflow.com/questions/59353819/how-to-bind-roles-with-service-accounts-kubernetes%20">dashboard - How to bind roles with service accounts - Kubernetes - Stack Overflow</a><br><a href="https://www.baeldung.com/kubernetes-java-client%20">A Quick Intro to the Kubernetes Java Client | Baeldung</a></p>
]]></content>
  </entry>
  <entry>
    <title>Kubernetes中的负载均衡原理————以iptables模式为例</title>
    <url>/k8s-service-iptables/</url>
    <content><![CDATA[<h2 id="什么是负载均衡"><a href="#什么是负载均衡" class="headerlink" title="什么是负载均衡"></a>什么是负载均衡</h2><p>负载均衡是高可用架构中的一个关键组件，他可以将请求平摊到后端服务上。</p>
<p>在单副本应用时代，你的服务负载可能并不高，这个时候负载均衡并没有多大用处。当你的应用火爆，用户请求数量多时，你需要将你的服务从单副本转变为多副本，那这个时候，是很有必要引入负载均衡的。</p>
<p>特别是当前微服务盛行的时代，一个应用多副本的高可用形态随处可见，负载均衡已经是个不可或缺的组件了。</p>
<h2 id="Kubernetes中的Service介绍"><a href="#Kubernetes中的Service介绍" class="headerlink" title="Kubernetes中的Service介绍"></a>Kubernetes中的Service介绍</h2><p>我们知道，Kubernetes中的Pod由于各种原因随时有可能被销毁和新建，且一般应用均以多副本的形式存在。</p>
<p>如果你想要访问一组Pod（或称之为微服务）时，必须有一种抽象资源，能够跟踪到其下所有的Pod，这个抽象便是Service。</p>
<p>Service主要有如下作用：</p>
<ul>
<li>服务发现：动态地将具有相同selector标志的后端Pod绑定起来</li>
<li>负载均衡：通过iptables或ipvs的负载均衡算法实现</li>
</ul>
<p>这里我们主要来讲下Service的负载均衡。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>从 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">官网</a> 可知，Service有三种代理模式：</p>
<ul>
<li>userspace代理模式</li>
<li>iptables代理模式</li>
<li>IPVS代理模式</li>
</ul>
<p>以iptables模式为例：</p>
<p><img src="/k8s-service-iptables/iptables-svc.png"></p>
<p>可以从图上看到，有一个重要的组件————kube-proxy，它以DaemonSet的形式存在，通过访问apiserver并watch相应资源(Service对象和Endpoint对象)来动态生成各自节点上的iptables规则。</p>
<p>当用户想要访问Pod服务时，iptables会通过NAT(网络地址转化)等方式<strong>随机</strong>请求到任意一个Pod。</p>
<p>在iptables模式下，kube-proxy通过在目标Node节点上的iptables配置中的NAT表的PREROUTIN和POSTROUTING链中创建一系列的自定义链(这些自定义链主要是”KUBE-SERVICE”链， “KUBE-POSTROUTING”链，每个服务对应的”KUBE-SVC-XXXXXX”链和”KUBE-SEP-XXXX”链)，然后通过这些自定义链对流经到该Node的数据包做DNAT和SNAT操作从而实现路由，负载均衡和地址转化。</p>
<h2 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h2><p>iptables是Linux平台下的包过滤防火墙，相关的四表五链知识可以去网上学习了解。</p>
<p>当设置了iptables规则后，每个数据包都要通过iptables的过滤，不同流向的数据包会通过不同的链：</p>
<ul>
<li>到本机某进程的报文：PREROUTING –&gt; INPUT</li>
<li>由本机转发的报文：PREROUTING –&gt; FORWARD –&gt; POSTROUTING</li>
<li>由本机的某进程发出报文：OUTPUT –&gt; POSTROUTING</li>
</ul>
<p>每个链上会有一些规则去过滤数据包进行操作，这些规则在大体上又可以分为4类，分别存在4张table中：</p>
<ul>
<li>filter表：负责过滤功能，防火墙；内核模块：iptables_filter</li>
<li>nat表：network address translation，网络地址转换功能；内核模块：iptable_nat</li>
<li>mangle表：拆解报文，做出修改，并重新封装 的功能；内核模块：iptable_mangle</li>
<li>raw表：关闭nat表上启用的连接追踪机制；内核模块：iptable_raw</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>下面我们来做个实验，具体看下<code>kube-proxy</code>生成的<code>iptables</code>规则是怎样请求到其中一个<code>Pod</code>里的</p>
<h3 id="1-准备"><a href="#1-准备" class="headerlink" title="1. 准备"></a>1. 准备</h3><p>在集群中应用如下创建一个名为nginx-service的Service和副本数为3的nginx Deployment：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建Deployment</span></span><br><span class="line">kubectl create deployment nginx --image=nginx --replicas=3</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建Service</span></span><br><span class="line">kubectl expose deployment nginx --port=80 --target-port=80</span><br></pre></td></tr></table></figure>
<p>我们可以看到3个Pod的ip分别为:</p>
<ul>
<li>10.244.1.14</li>
<li>10.244.1.16</li>
<li>10.244.1.17</li>
</ul>
<p>给Service分配的ip为: 10.97.54.248</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pod -owide</span></span><br><span class="line"></span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox                  1/1     Running   0          24s   10.244.1.20   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-6799fc88d8-8jnxd   1/1     Running   0          39h   10.244.1.14   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-6799fc88d8-g4d5x   1/1     Running   0          39h   10.244.1.16   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-6799fc88d8-q45nc   1/1     Running   0          44m   10.244.1.17   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get svc</span></span><br><span class="line"></span><br><span class="line">NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   175d</span><br><span class="line">nginx-svc    ClusterIP   10.97.54.248   &lt;none&gt;        80/TCP    3m1s</span><br></pre></td></tr></table></figure>

<p>我们可以看到nginx Service其下已有一组Endpoint暴露出来，对应的便是3个Pod的ip地址</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe svc nginx-svc</span></span><br><span class="line"></span><br><span class="line">Name:              nginx-svc</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            app=nginx</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          app=nginx</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP Family Policy:  SingleStack</span><br><span class="line">IP Families:       IPv4</span><br><span class="line">IP:                10.97.54.248</span><br><span class="line">IPs:               10.97.54.248</span><br><span class="line">Port:              &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         10.244.1.14:80,10.244.1.16:80,10.244.1.17:80</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h3 id="2-ClusterIP"><a href="#2-ClusterIP" class="headerlink" title="2. ClusterIP"></a>2. ClusterIP</h3><p>此外，我还建了一个busybox的镜像作为请求的发起方，他的地址是：10.244.1.20</p>
<p>所以当前的请求路径是：10.244.1.20(busybox) –&gt; 10.97.54.248:80(nginx-svc)</p>
<p>根据前文知识（由本机的某进程发出报文），我们先来看主机上的OUTPUT链上的nat表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL OUTPUT</span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT 878 packets, 52888 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">  37M 2597M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span><br><span class="line">  11M  673M DOCKER     all  --  *      *       0.0.0.0/0           !127.0.0.0/8          ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure>
<p>可以看到流量指向了KUBE-SERVICES的链</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-SERVICES</span><br><span class="line">Chain KUBE-SERVICES (2 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-SVC-HL5LMXD5JFHQZ6LN  tcp  --  *      *       0.0.0.0/0            10.97.54.248         /* default/nginx-svc cluster IP */ tcp dpt:80</span><br><span class="line">    0     0 KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  *      *       0.0.0.0/0            10.96.0.1            /* default/kubernetes:https cluster IP */ tcp dpt:443</span><br><span class="line">    0     0 KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns cluster IP */ udp dpt:53</span><br><span class="line">    0     0 KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53</span><br><span class="line">    0     0 KUBE-SVC-JD5MR3NA4I4DYORP  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153</span><br><span class="line"> 1756  106K KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure>
<p>由于目标是Service的ip(10.97.54.248)，所以这边又匹配到了KUBE-SVC-HL5LMXD5JFHQZ6LN这条链。</p>
<p>也可以从后面的注释中看到下面需要走哪条链</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-SVC-HL5LMXD5JFHQZ6LN</span><br><span class="line">Chain KUBE-SVC-HL5LMXD5JFHQZ6LN (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.244.0.0/16        10.97.54.248         /* default/nginx-svc cluster IP */ tcp dpt:80</span><br><span class="line">    0     0 KUBE-SEP-U46YXJIMXXUGWXXH  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc */ statistic mode random probability 0.33333333349</span><br><span class="line">    0     0 KUBE-SEP-DUL3TOEKR4Q7XNNH  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc */ statistic mode random probability 0.50000000000</span><br><span class="line">    0     0 KUBE-SEP-OJQRYVIILJUTFXOB  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc */</span><br></pre></td></tr></table></figure>
<p>到这里，若源端不是10.244.0.0&#x2F;16的话，会被打上标记；由于我们的busybox是该网段的，这条规则略过。</p>
<p>然后会随机匹配KUBE-SEP-U46YXJIMXXUGWXXH，KUBE-SEP-DUL3TOEKR4Q7XNNH，KUBE-SEP-OJQRYVIILJUTFXOB这三条链的其中一条。</p>
<p>其意思是：会有1&#x2F;3的概率命中KUBE-SEP-U46YXJIMXXUGWXXH这条链，如果没命中的话，会有2&#x2F;3 * 1&#x2F;2 &#x3D; 1&#x2F;3 的概率命中第二条链KUBE-SEP-DUL3TOEKR4Q7XNNH，最后还有1&#x2F;3的概率命中最后一条链KUBE-SEP-OJQRYVIILJUTFXOB。</p>
<p>可以看出，这边是在做负载均衡。</p>
<p>我们选择其中一条链KUBE-SEP-U46YXJIMXXUGWXXH继续走下去</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-SEP-U46YXJIMXXUGWXXH</span><br><span class="line">Chain KUBE-SEP-U46YXJIMXXUGWXXH (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  all  --  *      *       10.244.1.14          0.0.0.0/0            /* default/nginx-svc */</span><br><span class="line">    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc */ tcp to:10.244.1.14:80</span><br></pre></td></tr></table></figure>
<p>KUBE-MARK-MASQ自己Pod内访问，打上标记，可以先不看。</p>
<p>看DNAT那条链，可以看到这里做了目标地址转化，最终我们的请求从：<br>10.244.1.20(busybox) –&gt; 10.97.54.248:80(nginx-svc)<br>变成了<br>10.244.1.20(busybox) –&gt; 10.244.1.14:80(nginx-6799fc88d8-8jnxd)</p>
<p>OUTPUT链走完之后还会经过POSTROUTING链：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL POSTROUTING</span><br><span class="line">Chain POSTROUTING (policy ACCEPT 5321 packets, 321K bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">  41M 2939M KUBE-POSTROUTING  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */</span><br><span class="line">    0     0 MASQUERADE  all  --  *      !docker0  172.17.0.0/16        0.0.0.0/0</span><br><span class="line">6994K  525M RETURN     all  --  *      *       10.244.0.0/16        10.244.0.0/16</span><br><span class="line"> 758K   67M MASQUERADE  all  --  *      *       10.244.0.0/16       !224.0.0.0/4          random-fully</span><br><span class="line">    0     0 RETURN     all  --  *      *      !10.244.0.0/16        10.244.0.0/24</span><br><span class="line">    0     0 MASQUERADE  all  --  *      *      !10.244.0.0/16        10.244.0.0/16        random-fully</span><br><span class="line">root@master:~# iptables -t nat -nvL KUBE-POSTROUTING</span><br><span class="line">Chain KUBE-POSTROUTING (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line"> 5396  325K RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0            mark match ! 0x4000/0x4000</span><br><span class="line">    0     0 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK xor 0x4000</span><br><span class="line">    0     0 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ random-fully</span><br></pre></td></tr></table></figure>
<p>KUBE-POSTROUTING会对数据包进行判断，如果发现它有0x4000&#x2F;0x4000标记，就会跳到MASQUERADE规则，由于我们并没有被打上标记，直接RETURN。</p>
<h3 id="3-NodePort"><a href="#3-NodePort" class="headerlink" title="3. NodePort"></a>3. NodePort</h3><p>NodePort是集群外访问集群内服务的一种方式，从iptables规则来看，NodePort是ClusterIP的超集，额外比ClusterIP多了一些规则。</p>
<p>现在我把原来的ClusterIP删了之后创建了一个名为nginx-svc-nodeport的NodePort类型的service。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">zjq@master:~$ kubectl get svc</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kubernetes           ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP        175d</span><br><span class="line">nginx-svc-nodeport   NodePort    10.109.235.134   &lt;none&gt;        80:31082/TCP   10s</span><br></pre></td></tr></table></figure>
<p>该Service对集群外暴露的端口是31082，这个端口是由每个节点上的kube-proxy打开的，可以用如下命令查看：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# netstat -anp | grep 31082</span><br><span class="line">tcp        0      0 0.0.0.0:31082           0.0.0.0:*               LISTEN      1231199/kube-proxy</span><br><span class="line"></span><br><span class="line">root@node1:~# netstat -anp | grep 31082</span><br><span class="line">tcp        0      0 0.0.0.0:31082           0.0.0.0:*               LISTEN      1986768/kube-proxy</span><br></pre></td></tr></table></figure>
<p>这样，你便能通过任意节点+port的方式访问到微服务了。</p>
<p>现在我们来看下NodePort类型的iptables。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL PREROUTING</span><br><span class="line">Chain PREROUTING (policy ACCEPT 20 packets, 980 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">6621K  791M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span><br><span class="line">2918K  426M DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-SERVICES</span><br><span class="line">Chain KUBE-SERVICES (2 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-SVC-XKWTKZBTDCMU3FHC  tcp  --  *      *       0.0.0.0/0            10.109.235.134       /* default/nginx-svc-nodeport cluster IP */ tcp dpt:80</span><br><span class="line">    0     0 KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  *      *       0.0.0.0/0            10.96.0.1            /* default/kubernetes:https cluster IP */ tcp dpt:443</span><br><span class="line">    0     0 KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns cluster IP */ udp dpt:53</span><br><span class="line">    0     0 KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53</span><br><span class="line">    0     0 KUBE-SVC-JD5MR3NA4I4DYORP  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153</span><br><span class="line"> 1978  119K KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure>
<p>从上述看到，会先跳转掉KUBE-NODEPORTS这条链，来看下KUBE-NODEPORTS这条链之后的路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-NODEPORTS</span><br><span class="line">Chain KUBE-NODEPORTS (1 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-SVC-XKWTKZBTDCMU3FHC  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc-nodeport */ tcp dpt:31082</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@master:~# iptables -t nat -nvL KUBE-SVC-XKWTKZBTDCMU3FHC</span><br><span class="line">Chain KUBE-SVC-XKWTKZBTDCMU3FHC (2 references)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.244.0.0/16        10.109.235.134       /* default/nginx-svc-nodeport cluster IP */ tcp dpt:80</span><br><span class="line">    0     0 KUBE-MARK-MASQ  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc-nodeport */ tcp dpt:31082</span><br><span class="line">    0     0 KUBE-SEP-VLHANZGCXJXNRTPY  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc-nodeport */ statistic mode random probability 0.33333333349</span><br><span class="line">    0     0 KUBE-SEP-L66MBC5WQIY6TV6O  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc-nodeport */ statistic mode random probability 0.50000000000</span><br><span class="line">    0     0 KUBE-SEP-EAEOYSPXP66WOJLO  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc-nodeport */</span><br></pre></td></tr></table></figure>
<p>先会匹配到第二个KUBE-MARK-MASQ链，用于打上标记(之后有用)。</p>
<p>再往下，通过随机负载均衡和ClusterIP的逻辑一致。</p>
<p>这里需要注意的一点是，这时候执行KUBE-POSTROUTING链时，由于匹配到之前做的标记0x4000，会做一个SNAT操作。</p>
<p>为什么要做SNAT转化呢？这边假设一个场景，如下图：</p>
<p><img src="/k8s-service-iptables/snat.png"></p>
<p>当一个外部的client通过node2的地址访问一个Service的时候，node2上的负载均衡规则，就可能把这个IP包转发给一个在node1上的Pod。这里没有任何问题。</p>
<p>而当node1上的这个Pod处理完请求之后，它就会按照这个IP包的源地址发出回复。</p>
<p>可是，如果没有做SNAT操作的话，这时候，被转发来的IP包的源地址就是client的IP地址。所以此时，Pod就会直接将回复发给client。对于client来说，它的请求明明发给了node2，收到的回复却来自node1，这个client很可能会报错。</p>
<h3 id="4-LoadBalance"><a href="#4-LoadBalance" class="headerlink" title="4. LoadBalance"></a>4. LoadBalance</h3><p>LoadBalance是NodePort的超集，这边不再分析。</p>
<h2 id="原理分析"><a href="#原理分析" class="headerlink" title="原理分析"></a>原理分析</h2><p>通过分析kube-proxy的实现能够更好地理解Service的实现。</p>
<p>新建proxyServer对象newProxyServer方法，会根据不同的模式来初始化proxier对象。</p>
<p>如果你的节点未开启ipvs，则自动降级为iptables模式。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newProxyServer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">config *proxyconfigapi.KubeProxyConfiguration,</span></span></span><br><span class="line"><span class="params"><span class="function">cleanupAndExit <span class="type">bool</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">master <span class="type">string</span>)</span></span> (*ProxyServer, <span class="type">error</span>) &#123;</span><br><span class="line">	...</span><br><span class="line">	<span class="keyword">if</span> proxyMode == proxyModeIPTables &#123;</span><br><span class="line">        klog.V(<span class="number">0</span>).InfoS(<span class="string">&quot;Using iptables Proxier&quot;</span>)</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> dualStack &#123;</span><br><span class="line">            ...</span><br><span class="line">            <span class="comment">// TODO this has side effects that should only happen when Run() is invoked.</span></span><br><span class="line">            proxier, err = iptables.NewDualStackProxier(</span><br><span class="line">            ipt,</span><br><span class="line">            utilsysctl.New(),</span><br><span class="line">            execer,</span><br><span class="line">            config.IPTables.SyncPeriod.Duration,</span><br><span class="line">            config.IPTables.MinSyncPeriod.Duration,</span><br><span class="line">            config.IPTables.MasqueradeAll,</span><br><span class="line">            <span class="type">int</span>(*config.IPTables.MasqueradeBit),</span><br><span class="line">            localDetectors,</span><br><span class="line">            hostname,</span><br><span class="line">            nodeIPTuple(config.BindAddress),</span><br><span class="line">            recorder,</span><br><span class="line">            healthzServer,</span><br><span class="line">            config.NodePortAddresses,</span><br><span class="line">            )</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			...</span><br><span class="line">            <span class="comment">// TODO this has side effects that should only happen when Run() is invoked.</span></span><br><span class="line">            proxier, err = iptables.NewProxier(</span><br><span class="line">                iptInterface,</span><br><span class="line">                utilsysctl.New(),</span><br><span class="line">                execer,</span><br><span class="line">                config.IPTables.SyncPeriod.Duration,</span><br><span class="line">                config.IPTables.MinSyncPeriod.Duration,</span><br><span class="line">                config.IPTables.MasqueradeAll,</span><br><span class="line">                <span class="type">int</span>(*config.IPTables.MasqueradeBit),</span><br><span class="line">                localDetector,</span><br><span class="line">                hostname,</span><br><span class="line">                nodeIP,</span><br><span class="line">                recorder,</span><br><span class="line">                healthzServer,</span><br><span class="line">                config.NodePortAddresses,</span><br><span class="line">            )</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;unable to create proxier: %v&quot;</span>, err)</span><br><span class="line">        &#125;</span><br><span class="line">        proxymetrics.RegisterMetrics()</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> proxyMode == proxyModeIPVS &#123;</span><br><span class="line">		...</span><br><span class="line">		klog.V(<span class="number">0</span>).InfoS(<span class="string">&quot;Using ipvs Proxier&quot;</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		...</span><br><span class="line">        klog.V(<span class="number">0</span>).InfoS(<span class="string">&quot;Using userspace Proxier&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ProxyServer结构体最主要的是Run()方法：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *ProxyServer)</span></span> Run() <span class="type">error</span> &#123;</span><br><span class="line">	...</span><br><span class="line">    <span class="comment">// 暴露/healthz接口</span></span><br><span class="line">    <span class="comment">// Start up a healthz server if requested</span></span><br><span class="line">    serveHealthz(s.HealthzServer, errCh)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 暴露指标信息</span></span><br><span class="line">    <span class="comment">// Start up a metrics server if requested</span></span><br><span class="line">    serveMetrics(s.MetricsBindAddress, s.ProxyMode, s.EnableProfiling, errCh)</span><br><span class="line">	</span><br><span class="line">	...</span><br><span class="line">    <span class="comment">// 新建informerFactory</span></span><br><span class="line">    <span class="comment">// Make informers that filter out objects that want a non-default service proxy.</span></span><br><span class="line">    informerFactory := informers.NewSharedInformerFactoryWithOptions(s.Client, s.ConfigSyncPeriod,</span><br><span class="line">        informers.WithTweakListOptions(<span class="function"><span class="keyword">func</span><span class="params">(options *metav1.ListOptions)</span></span> &#123;</span><br><span class="line">        options.LabelSelector = labelSelector.String()</span><br><span class="line">        &#125;))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// kube-proxy主要watch了service和endpoint(或endpointSlices)资源的变动，</span></span><br><span class="line">    <span class="comment">// 当它们有变动时，对应节点上的iptables规则也会相相应地变动</span></span><br><span class="line">    <span class="comment">// Create configs (i.e. Watches for Services and Endpoints or EndpointSlices)</span></span><br><span class="line">    <span class="comment">// Note: RegisterHandler() calls need to happen before creation of Sources because sources</span></span><br><span class="line">    <span class="comment">// only notify on changes, and the initial update (on process start) may be lost if no handlers</span></span><br><span class="line">    <span class="comment">// are registered yet.</span></span><br><span class="line">    serviceConfig := config.NewServiceConfig(informerFactory.Core().V1().Services(), s.ConfigSyncPeriod)</span><br><span class="line">    serviceConfig.RegisterEventHandler(s.Proxier)</span><br><span class="line">    <span class="keyword">go</span> serviceConfig.Run(wait.NeverStop)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> endpointsHandler, ok := s.Proxier.(config.EndpointsHandler); ok &amp;&amp; !s.UseEndpointSlices &#123;</span><br><span class="line">        endpointsConfig := config.NewEndpointsConfig(informerFactory.Core().V1().Endpoints(), s.ConfigSyncPeriod)</span><br><span class="line">        endpointsConfig.RegisterEventHandler(endpointsHandler)</span><br><span class="line">        <span class="keyword">go</span> endpointsConfig.Run(wait.NeverStop)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        endpointSliceConfig := config.NewEndpointSliceConfig(informerFactory.Discovery().V1().EndpointSlices(), s.ConfigSyncPeriod)</span><br><span class="line">        endpointSliceConfig.RegisterEventHandler(s.Proxier)</span><br><span class="line">        <span class="keyword">go</span> endpointSliceConfig.Run(wait.NeverStop)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 启动informer</span></span><br><span class="line">    <span class="comment">// This has to start after the calls to NewServiceConfig and NewEndpointsConfig because those</span></span><br><span class="line">    <span class="comment">// functions must configure their shared informer event handlers first.</span></span><br><span class="line">    informerFactory.Start(wait.NeverStop)</span><br><span class="line">	</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// Birth Cry after the birth is successful</span></span><br><span class="line">    s.birthCry()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 进入定时循环</span></span><br><span class="line">    <span class="keyword">go</span> s.Proxier.SyncLoop()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &lt;-errCh</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里我们来看下serviceConfig中的Run()方法，这里面根据注册的eventHandlers动作，均会执行OnServiceSynced()方法，</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Run waits for cache synced and invokes handlers after syncing.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *ServiceConfig)</span></span> Run(stopCh &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;) &#123;</span><br><span class="line">    klog.InfoS(<span class="string">&quot;Starting service config controller&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> !cache.WaitForNamedCacheSync(<span class="string">&quot;service config&quot;</span>, stopCh, c.listerSynced) &#123;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i := <span class="keyword">range</span> c.eventHandlers &#123;</span><br><span class="line">        klog.V(<span class="number">3</span>).InfoS(<span class="string">&quot;Calling handler.OnServiceSynced()&quot;</span>)</span><br><span class="line">        <span class="comment">// 注册的事件动作执行相应的OnServiceSynced()</span></span><br><span class="line">        c.eventHandlers[i].OnServiceSynced()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在OnServiceSynced中，我们可以看到核心方法syncProxyRules()</p>
<p>在十分冗长的syncProxyRules方法中(大约800行)，里面就会应用iptables rule到当前的节点。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// OnServiceSynced is called once all the initial event handlers were</span></span><br><span class="line"><span class="comment">// called and the state is fully propagated to local cache.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(proxier *Proxier)</span></span> OnServiceSynced() &#123;</span><br><span class="line">    proxier.mu.Lock()</span><br><span class="line">    proxier.servicesSynced = <span class="literal">true</span></span><br><span class="line">    proxier.setInitialized(proxier.endpointSlicesSynced)</span><br><span class="line">    proxier.mu.Unlock()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Sync unconditionally - this is called once per lifetime.</span></span><br><span class="line">    <span class="comment">// 应用iptables rule到节点上</span></span><br><span class="line">    proxier.syncProxyRules()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文简单介绍了负载均衡的作用和Kubernetes中负载均衡的原理，并通过一些实验例子来展示请求是如何被转发到Pod中的，最后通过分析核心源码的方式来了解具体实现。</p>
<p>但是，iptables也存在许多问题：</p>
<ol>
<li><p>iptables规则多了之后性能下降。按照 <a href="https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/#why-ipvs-for-kubernetes">官方说法</a>:</p>
<blockquote>
<p>尽管 Kubernetes 在版本1.6中已经支持5000个节点，但是使用 iptables 的 kube-proxy 实际上是将集群扩展到5000个节点的一个瓶颈。一个例子是在一个包含5000个节点的集群中使用 NodePort Service，如果我们有2000个服务，每个服务有10个 pods，这将导致每个工作节点上至少有20000个 iptable 记录，这会使内核非常繁忙。</p>
</blockquote>
</li>
<li><p>iptables使用的负载均衡算法简单，不支持复杂场景。相反，ipvs能够支持更多的负载均衡算法，且性能更好</p>
</li>
</ol>
<p>另外，基于eBPF技术实现的CNI插件cilium可以完全替换kube-proxy，感兴趣的同学可以试一下。</p>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title>LVM 快照与恢复（备份与恢复的快速方法）</title>
    <url>/lvm-snapshot/</url>
    <content><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在进行某些验证性操作时，我们需要创建测试数据，并在验证操作的过程中修改数据。但是如果验证操作失败，那么我们又需要重新创建测试数据。为了避免重新创建数据，我们常见的做法是备份测试数据，以在验证失败时能够从备份数据中快速进行恢复。</p>
<p>还有种场景是服务升级的时候：为了能够在升级失败时回滚，需要对服务数据进行备份，否则数据被破坏之后，服务回滚后也无法运行。但是由于服务数据较多，导致备份周期长，服务停机时间长。而且在升级过程中，并非所有的数据都需要备份，因为并非所有的数据都会被破坏。</p>
<p>该笔记将记录：在 LVM 中，使用 Snapshot 快照的方法（对数据进行快速的备份与恢复），以及常见问题的解决办法。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>当创建快照后，如果不小心删除任何文件，也不必担心，因为快照具有我们已删除的原始文件。</p>
<p>注意事项：<br>1）快照不能用于持久的备份策略 —— 备份是某些数据文件的主副本，而快照是块级别，所以不能使用快照作为备份选项；<br>2）不要更改快照卷，保持原样，而快照用于快速恢复。</p>
<h3 id="环境概述"><a href="#环境概述" class="headerlink" title="环境概述"></a>环境概述</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pvcreate /dev/sdb                                                               # 10G</span><br><span class="line">vgcreate vgdt /dev/sdb</span><br><span class="line">lvcreate -n source --size 3G vgdt</span><br><span class="line"></span><br><span class="line">mkfs.ext4 /dev/vgdt/source</span><br><span class="line">mount /dev/vgdt/source /mnt/</span><br><span class="line">echo 123456 &gt; /mnt/foo.txt</span><br><span class="line">md5sum /mnt/foo.txt                                                             # f447b20a7fcbf53a5d5be013ea0b15af</span><br></pre></td></tr></table></figure>

<h3 id="第一步、创建快照"><a href="#第一步、创建快照" class="headerlink" title="第一步、创建快照"></a>第一步、创建快照</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># lvcreate --size  1G --snapshot --name backup4source /dev/vgdt/source</span><br><span class="line">  Logical volume &quot;backup4source&quot; created.</span><br><span class="line"></span><br><span class="line"># lvextend --size +1G /dev/vgdt/backup4source                                   # 再额外增加 1G 空间</span><br><span class="line">  Size of logical volume vgdt/backup4source changed from 1.00 GiB (256 extents) to 2.00 GiB (512 extents).</span><br><span class="line">  Logical volume vgdt/backup4source successfully resized.</span><br><span class="line"></span><br><span class="line"># lvs /dev/vgdt/backup4source                                                   # 查看快照信息</span><br><span class="line">  LV            VG   Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  backup4source vgdt swi-a-s--- 2.00g      source 0.01 </span><br><span class="line"></span><br><span class="line"># lvdisplay /dev/vgdt/backup4source </span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/vgdt/backup4source</span><br><span class="line">  ...</span><br><span class="line">  LV snapshot status     active destination for source                          # 该快照所属的 LV</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<h3 id="第二步、数据修改"><a href="#第二步、数据修改" class="headerlink" title="第二步、数据修改"></a>第二步、数据修改</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># lvs</span><br><span class="line">  LV            VG        Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  ubuntu-lv     ubuntu-vg -wi-ao---- &lt;9.00g                                                    </span><br><span class="line">  backup4source vgdt      swi-a-s---  2.00g      source 0.01                                   </span><br><span class="line">  source        vgdt      owi-aos---  3.00g</span><br><span class="line"></span><br><span class="line"># dd if=/dev/zero of=/mnt/foo.txt bs=1M count=1024 conv=fdatasync </span><br><span class="line">1024+0 records in</span><br><span class="line">1024+0 records out</span><br><span class="line">1073741824 bytes (1.1 GB, 1.0 GiB) copied, 9.26228 s, 116 MB/s</span><br><span class="line"></span><br><span class="line"># lvs</span><br><span class="line">  LV            VG        Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  ubuntu-lv     ubuntu-vg -wi-ao---- &lt;9.00g                                                    </span><br><span class="line">  backup4source vgdt      swi-a-s---  2.00g      source 50.21                   # 原始数据被写入快照分区，以占用 50.21%</span><br><span class="line">  source        vgdt      owi-aos---  3.00g </span><br></pre></td></tr></table></figure>

<p>关于 Snapshot 大小：<br>1）如果数据变更的总量超过 Snapshot 大小，则会产生 Input&#x2F;output error 错误，进而导致 Snapshot 不可用（解释扩容也无法恢复）；<br>2）如果要避免该问题，可以创建相同大小的 Snapshot，或者自动扩容 Snapshot 分区（这里不再展开详细说明）；</p>
<h3 id="第三步、恢复快照"><a href="#第三步、恢复快照" class="headerlink" title="第三步、恢复快照"></a>第三步、恢复快照</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># umount /mnt</span><br><span class="line"></span><br><span class="line"># lvconvert --merge /dev/vgdt/backup4source</span><br><span class="line">  Merging of volume vgdt/backup4source started.</span><br><span class="line">  vgdt/source: Merged: 50.29%</span><br><span class="line">...</span><br><span class="line">  vgdt/source: Merged: 100.00%</span><br><span class="line"></span><br><span class="line"># mount /dev/vgdt/source /mnt/</span><br><span class="line"></span><br><span class="line"># md5sum /mnt/foo.txt</span><br><span class="line">f447b20a7fcbf53a5d5be013ea0b15af  /mnt/foo.txt</span><br></pre></td></tr></table></figure>

<p>补充说明：<br>1）当 merge 后，Snapshot 会被自动删除；</p>
<h2 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h2><h3 id="删除快照"><a href="#删除快照" class="headerlink" title="删除快照"></a>删除快照</h3><p>如果没有必要保留快照，则可以删除：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># lvremove /dev/vgdt/backup4source</span><br></pre></td></tr></table></figure>

<h3 id="自动扩容"><a href="#自动扩容" class="headerlink" title="自动扩容"></a>自动扩容</h3><p>该特性是为了让 Snapshot 自动扩容，而不需要分配足够的空间，且当空间不足时不需要人工介入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># vim /etc/lvm/lvm.conf</span><br><span class="line">...</span><br><span class="line">snapshot_autoextend_threshold = 70                                              # 当用量超过 70% 时，</span><br><span class="line">snapshot_autoextend_percent = 20                                                # 自动扩容 20%</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.tecmint.com/take-snapshot-of-logical-volume-and-restore-in-lvm/%20">How to Take ‘Snapshot of Logical Volume and Restore’ in LVM - Part III</a><br><a href="https://kerneltalks.com/disk-management/how-to-guide-lvm-snapshot/%20">How-to guide: LVM snapshot - Kernel Talks</a></p>
]]></content>
      <categories>
        <category>云计算</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第十一期-联盟链中的Rollup方案</title>
    <url>/rollup-in-consortium-blockchain/</url>
    <content><![CDATA[<h3 id="区块链扩容方案"><a href="#区块链扩容方案" class="headerlink" title="区块链扩容方案"></a>区块链扩容方案</h3><p>区块链自诞生一来，一直受性能问题困扰。</p>
<p>因为区块链跟传统的分布式系统（比如分布式数据库）有一个很大的不同，就是每个节点都是副本，要同步所有的数据并执行同样的处理，而传统的分布式数据库一般是三副本。这就导致分布式数据库增加节点可以提升性能，而区块链增加节点，不但不能提升性能，反而会降低性能（通信量变大）。</p>
<p>横向扩展不行，那还有纵向扩展，即提升节点的硬件配置。</p>
<p>可惜这一条路也被堵死了，因为在区块链里，去中心化是绝对的政治正确，提升节点的硬件要求，会把一部分人挡在外面，导致链更中心化了。</p>
<p>大区块方案其实就是间接的纵向扩展。因为区块扩大了，能打包的交易多了，但是出块间隔不变。加（工作）量不加价，对打工人的要求当然就更高了。</p>
<p>所谓开源节流，既然开源这边横竖都不行，自然只能节流了。怎么节流？把一部分工作推出去，担子轻省了，自然跑得就快了。</p>
<p>状态通道就是最早的这一类方案，类似的还有闪电网络，雷电网络等等，原理都差不多。</p>
<p>因为早期区块链（就是比特币）只有转账操作，这些方案也基本上只支持转账操作，所以也叫支付通道。</p>
<p>打个比方，比如5个同事去饭店聚餐，总共消费200元，AA一下每个人40。如果每个人都直接给老板支付40，加上找零什么的，老板一下就忙不过来了。有没有更省事的办法呢？老板说你们5个人先把200凑出来，没有零钱，相互找零都内部搞定，完了再一起给我。</p>
<p>这里老板就相当于区块链，5个人内部相互支付就是支付通道。</p>
<p>状态通道的缺点：</p>
<ol>
<li>人不好凑。几个人是同事还好，几个陌生人就不能这么搞了。</li>
<li>相比而言资金还是有风险的。你给了同事50，同事要找你10，但是现在没零钱，说回头给你，结果回头就忘记了。但是老板是不敢这么做的，吃饭多收钱店都开不下去了。</li>
</ol>
<p>接下来是侧链方案，有点类似于影分身，一个人工作压力山大，多召唤几个分身一起分担。这个方案跟状态通道不一样，这个是直接把整个链都搞了多套，说白了就是多链方案。</p>
<p>这类方案的问题是，工作倒是分担了，扯皮也跟着多了起来。山头林立，到底听谁的？</p>
<p>分片，<code>Plasma</code>都属于这一类的变种，为了弥补其缺点各种打补丁，搞得方案巨复杂，这里就不详述了。</p>
<p>终于，本文的主角<code>Rollup</code>出现了。</p>
<p>跟前面的横向扩展，纵向扩展类似。多链可以认为是横向扩展，而<code>Rollup</code>则是纵向扩展。所以<code>Rollup</code>被称为<code>layer 2</code>（两层）方案。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">其实一般侧链，Plasma等也会被划入layer 2方案。但是我觉得它们的区别还是挺明显的，所以这里我并没有按照惯用的说法，请大家注意。</span><br></pre></td></tr></table></figure>

<p>前面大家折腾了很久的多链方案之后，终于发现这些漏洞是按下葫芦浮起瓢，怎么都堵不住。专业的说法是，数据可用性和计算正确性无法同时保证。</p>
<p>因为<code>Plasma</code>想把数据和计算都平分到多条链里面，而<code>Rollup</code>则是分工一下，<code>layer 1</code>负责数据可用性，<code>layer 2</code>只负责计算。</p>
<h3 id="Rollup"><a href="#Rollup" class="headerlink" title="Rollup"></a>Rollup</h3><p>该方案对链的性能提升来自几个方面：</p>
<ol>
<li>聚合器收集用户交易，然后批量打包发送到<code>layer 1</code>上，可以节省<code>gas</code>。在同样的<code>gas limit</code>下，区块可以打包更多的交易。<ul>
<li>对交易进行压缩。去除非必要字段，甚至直接数据压缩。</li>
<li>以<code>calldata</code>的方式将交易发送到链上，这种存储形式的<code>gas</code>消耗更低。</li>
</ul>
</li>
<li>链外计算，<code>layer 1</code>的负担减轻了。</li>
<li><code>Arbitrum</code>项目还提出了序列器方案，由中心化系统对交易进行排序，进一步提升系统性能。</li>
</ol>
<p>当然这里面也是有安全问题的，因此还设置了复杂的挑战机制，通过举报有奖的方式来解决可能出现的欺诈问题。</p>
<h3 id="联盟链中Rollup"><a href="#联盟链中Rollup" class="headerlink" title="联盟链中Rollup"></a>联盟链中Rollup</h3><p>然而，对于联盟链来说，<code>Rollup</code>提升性能方面的作用并不重要。</p>
<p>一方面，联盟链的性能压力不大；另外一方面，联盟链里没有<code>gas</code>，方案里那些节省<code>gas</code>的奇技淫巧根本没用。</p>
<p>那么我们为什么要花这么多精力去研究它呢？</p>
<p>因为<code>Rollup</code>方案有一个被忽视的，其实非常重要的作用，就是它提供了区块链进一步解耦的思路：</p>
<ol>
<li>用户直接与聚合器进行交互，而不是链。用户接口和相应的数据结构（交易结构）与链解耦。</li>
<li>合约引擎与链解耦。<ul>
<li><code>Layer 1</code>链只提供存证功能，保证原始交易的数据有效性，和对计算结果的存证。</li>
<li>合约部分将跟传统应用开发类似，开发者可以用更灵活的方式开发智能合约。</li>
</ul>
</li>
<li>序列器提供了去中心化与中心化结合的思路。用户可以根据信任程度走不同的路径，甚至在信任的前提下，数据有效性可以由单一中心化系统保证，更好的与传统系统结合。</li>
</ol>
<p>然后我们就可以让整个联盟链系统更加贴近应用场景：</p>
<ol>
<li>用户接口可以更贴近应用，设计为更适合应用的方式。比如：<ul>
<li>数据结构可以针对特定应用优化。</li>
<li>验证方式不一定使用数字签名，可以用更传统的用户名&#x2F;密码等方式。</li>
</ul>
</li>
<li>智能合约开发形式更接近传统应用。<ul>
<li>使用通用编程语言。</li>
<li>智能合约是一个单独的系统，设计更加灵活。</li>
</ul>
</li>
<li>实现去中心化和中心化相结合的方式，更加贴近现实场景。<ul>
<li>用户信任中心化系统时，可以走中心化系统，作为<code>fast path</code>。</li>
<li>用户不信任中心化系统的时候，走去中心化系统作为<code>slow path</code>。</li>
</ul>
</li>
</ol>
<p>当然这里面还有很多技术挑战，比如：</p>
<ol>
<li>用通用语言编写智能合约，怎么保证其执行的确定性？</li>
<li><code>layer 2</code>要证明计算的正确性，就要给出相应的密码学证据，是否有比以太坊基于默克尔树的方案更轻量的方案？</li>
<li>用户在不同的路径间切换时，如何处理回退等情况，如何让用户体验更好，甚至无感？</li>
</ol>
<p>对具体方案有兴趣的小伙伴欢迎关注<a href="https://github.com/cita-cloud">CITA-Cloud</a>中相应的<a href="https://github.com/cita-cloud/rfcs/pull/10/files">RFC</a>，也欢迎大家一起来探讨相关的话题。</p>
<h3 id="区块链的抽象"><a href="#区块链的抽象" class="headerlink" title="区块链的抽象"></a>区块链的抽象</h3><p>引入<code>Rollup</code>的思路之后，联盟链从抽象角度会变得更像可信计算和存证的结合。</p>
<p><img src="/rollup-in-consortium-blockchain/rollup.png"></p>
<p>这里的<code>caller</code>和<code>add_server</code>就是很传统的可信计算的关系。</p>
<p>可信计算已经发展出了非常多的先进技术，比如零知识证明之类。但是欺诈者足够无耻的话，你总是拿他没办法，而区块链可以以仲裁者的角色，填补上这个漏洞。</p>
]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title>在 Kubernetes 中，部署云原生 NFS 存储</title>
    <url>/rook-nfs/</url>
    <content><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>我们常用的存储软件（比如 NFS、Ceph、EdgeFS、YugabyteDB 等等）并不具备（或仅具备部分）高可用、自愈、自动扩展等等特性。</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>Rook，是开源的云原生存储编排器，提供平台，框架，支持“多种本机存储解决方案”与“云原生环境”集成。Rook 基于底层云原生平台对这些存储软件进行强化。通过使用“__底层的云原生容器管理、调度、协调平台提供的__”基础设施，来为存储服务添加 自我管理、自我缩放、自愈的 等等特性。它通过自动部署、引导、配置、部署、缩放、升级、迁移、灾难恢复，监控，资源管理来实现。</p>
<p>该笔记将记录在 Kubernetes 中如何部署 Rook 服务，底层使用 NFS 存储，以及常见问题解决办法。</p>
<h2 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h2><ol>
<li>Kubernetes v1.16 or higher</li>
<li>The desired volume to export needs to be attached to the NFS * server pod via a PVC</li>
<li>NFS client packages must be installed on all nodes where * Kubernetes might run pods with NFS mounted.</li>
</ol>
<h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><ol>
<li>Rook NFS v1.7（03&#x2F;14&#x2F;2022），建议阅读 <a href="https://rook.io/docs/nfs/v1.7/%20">NFS Docs&#x2F;v1.7</a> 文档以了解更多细节，这里我们仅记录适用于我们环境的部署过程。</li>
<li>Kubernetes HA Cluster 1.18.20, worker k8s-storage as dedicated storage node</li>
</ol>
<h2 id="关于存储"><a href="#关于存储" class="headerlink" title="关于存储"></a>关于存储</h2><ol>
<li>简单的拓扑结构为 Normal Pod ⇒ Storage Class ⇒ NFS Server ⇒ PVC ⇒ PV (hostPath) 所以我们以 hostPath 方式来提供最终的存储；</li>
<li>通过专用的存储节点，即 Kubernetes Worker 但是不会向该节点调度 Pod 实例（通过 Taint 及 Namespace defaultTolerations 来实现）；</li>
</ol>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Taint node，以专用于存储</span></span><br><span class="line">kubectl taint nodes k8s-storage dedicated=storage:NoSchedule</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启 PodNodeSelector,PodTolerationRestriction 插件（不再细述）</span></span><br><span class="line">kube-apiserver ... --enable-admission-plugins=NodeRestriction,PodNodeSelector,PodTolerationRestriction ...</span><br></pre></td></tr></table></figure>

<h2 id="第一步、部署-NFS-Operator-组件"><a href="#第一步、部署-NFS-Operator-组件" class="headerlink" title="第一步、部署 NFS Operator 组件"></a>第一步、部署 NFS Operator 组件</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --single-branch --branch v1.7.3 https://github.com/rook/nfs.git</span><br><span class="line"><span class="built_in">cd</span> nfs/cluster/examples/kubernetes/nfs</span><br><span class="line">kubectl create -f crds.yaml</span><br><span class="line">kubectl create -f operator.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pods -n rook-nfs-system </span></span><br><span class="line">NAME                                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">rook-nfs-operator-794b5c98bd-rc8lv   1/1     Running   0          8m31s</span><br></pre></td></tr></table></figure>

<p>补充说明：</p>
<ul>
<li>Operator 是否调度到 k8s-storage（专用存储节点）并不重要；</li>
</ul>
<h2 id="第二步、创建-NFS-Server-服务"><a href="#第二步、创建-NFS-Server-服务" class="headerlink" title="第二步、创建 NFS Server 服务"></a>第二步、创建 NFS Server 服务</h2><p>rbac.yaml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span>  <span class="string">rook-nfs</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">scheduler.alpha.kubernetes.io/node-selector:</span> <span class="string">kubernetes.io/hostname=k8s-storage</span></span><br><span class="line">    <span class="attr">scheduler.alpha.kubernetes.io/defaultTolerations:</span> <span class="string">&#x27;[&#123;&quot;operator&quot;: &quot;Exists&quot;, &quot;effect&quot;: &quot;NoSchedule&quot;, &quot;key&quot;: &quot;dedicated&quot;&#125;]&#x27;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-nfs-server</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-nfs</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-nfs-provisioner-runner</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumes&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;delete&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumeclaims&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;update&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;storageclasses&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;events&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;services&quot;</span>, <span class="string">&quot;endpoints&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;policy&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;podsecuritypolicies&quot;</span>]</span><br><span class="line">    <span class="attr">resourceNames:</span> [<span class="string">&quot;rook-nfs-policy&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;use&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;endpoints&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">nfs.rook.io</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;*&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-nfs-provisioner-runner</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">rook-nfs-server</span></span><br><span class="line">     <span class="comment"># replace with namespace where provisioner is deployed</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">rook-nfs</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-nfs-provisioner-runner</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure>

<p>nfs-server.yaml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-nfs-pv</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-nfs</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">claimRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">rook-nfs-pvc</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">rook-nfs</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">500Gi</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">hostPath:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">&quot;/srv/k8s-storage-nfs-rook-pv&quot;</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">k8s-storage</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># A default storageclass must be present</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-nfs-pvc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-nfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">volumeName:</span> <span class="string">&quot;rook-nfs-pv&quot;</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">500Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">nfs.rook.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NFSServer</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-nfs</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-nfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">exports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">share-01</span></span><br><span class="line">    <span class="attr">server:</span></span><br><span class="line">      <span class="attr">accessMode:</span> <span class="string">ReadWrite</span></span><br><span class="line">      <span class="attr">squash:</span> <span class="string">&quot;none&quot;</span></span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">rook-nfs-pvc</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">rook:</span> <span class="string">nfs</span></span><br></pre></td></tr></table></figure>

<p>查看结果：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kubectl -n rook-nfs get nfsservers.nfs.rook.io</span><br><span class="line">NAME       AGE   STATE</span><br><span class="line">rook-nfs   2m    Running</span><br><span class="line"></span><br><span class="line"># kubectl -n rook-nfs get pod -l app=rook-nfs -o wide</span><br><span class="line">NAME         READY   STATUS    RESTARTS   AGE    IP               NODE      NOMINATED NODE   READINESS GATES</span><br><span class="line">rook-nfs-0   2/2     Running   0          6m8s   192.168.59.130   k8s-w03   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h2 id="第三步、使用-NFS-存储"><a href="#第三步、使用-NFS-存储" class="headerlink" title="第三步、使用 NFS 存储"></a>第三步、使用 NFS 存储</h2><p>storage-class.yaml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">rook-nfs</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-nfs-share-01</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">exportName:</span> <span class="string">share-01</span></span><br><span class="line">  <span class="attr">nfsServerName:</span> <span class="string">rook-nfs</span></span><br><span class="line">  <span class="attr">nfsServerNamespace:</span> <span class="string">rook-nfs</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">nfs.rook.io/rook-nfs-provisioner</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line"><span class="attr">volumeBindingMode:</span> <span class="string">Immediate</span></span><br></pre></td></tr></table></figure>

<p>testing.yaml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-nfs-pv-claim</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">&quot;rook-nfs-share-01&quot;</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Mi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># cat nfs/cluster/examples/kubernetes/nfs/busybox-rc.yaml </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nfs-demo</span></span><br><span class="line">    <span class="attr">role:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-busybox</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nfs-demo</span></span><br><span class="line">      <span class="attr">role:</span> <span class="string">busybox</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nfs-demo</span></span><br><span class="line">        <span class="attr">role:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">          <span class="attr">command:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;while true; do date &gt; /mnt/index.html; hostname &gt;&gt; /mnt/index.html; sleep $(($RANDOM % 5 + 5)); done&quot;</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">busybox</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="comment"># name must match the volume name below</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">rook-nfs-vol</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">&quot;/mnt&quot;</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">rook-nfs-vol</span></span><br><span class="line">          <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">            <span class="attr">claimName:</span> <span class="string">rook-nfs-pv-claim</span></span><br></pre></td></tr></table></figure>

<h1 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h1><p>Pod 通过 Service 进行 NFS 挂载：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl -n rook-nfs get service rook-nfs </span></span><br><span class="line">NAME       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)            AGE</span><br><span class="line">rook-nfs   ClusterIP   10.111.156.207   &lt;none&gt;        2049/TCP,111/TCP   135m</span><br></pre></td></tr></table></figure>

<h1 id="调试追踪"><a href="#调试追踪" class="headerlink" title="调试追踪"></a>调试追踪</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl -n rook-nfs-system logs -l app=rook-nfs-operator</span><br><span class="line"></span><br><span class="line"># NFS Server</span><br><span class="line">kubectl -n rook-nfs logs rook-nfs-0 nfs-server                                </span><br><span class="line"></span><br><span class="line"># Storage Class</span><br><span class="line">kubectl -n rook-nfs logs rook-nfs-0 nfs-provisioner                           </span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li><a href="https://zhimin-wen.medium.com/default-toleration-at-namespace-level-f66dd3da4451">Default Toleration at Namespace Level | by Zhimin Wen | Medium</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/94722">DaemonSet not respecting Namespace defaultTolerations · Issue #94722 · kubernetes&#x2F;kubernetes</a></li>
<li><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Taints and Tolerations | Kubernetes</a></li>
<li><a href="https://stackoverflow.com/questions/69449258/k3s-node-restriction-for-namespace">plugins - k3s node restriction for namespace - Stack Overflow</a></li>
<li><a href="https://rook.io/docs/nfs/v1.7/quickstart.html">Rook NFS&#x2F;v1.7.3&#x2F;Network Filesystem (NFS) Quickstart</a></li>
</ul>
]]></content>
      <categories>
        <category>云计算</category>
      </categories>
  </entry>
  <entry>
    <title>在对象存储上运行CITA-Cloud</title>
    <url>/run-CITA-Cloud-on-object-storage/</url>
    <content><![CDATA[<h2 id="对象存储与生态"><a href="#对象存储与生态" class="headerlink" title="对象存储与生态"></a>对象存储与生态</h2><p>说起对象存储，不得不提Amazon的AWS S3(Simple Storage Service).</p>
<p>进入到21世纪，数据急剧增长，当时已经是电商巨头的Amazon需要一种海量的、可扩展的、支持非结构化数据且对开发友好的网络存储。2006年，S3云服务应运而生。</p>
<p>从那之后，各个云厂商也随之跟进，发布了自己的对象存储服务。</p>
<p>S3也不仅仅代表了一种云服务，也成为了对象存储业界的一种协议。</p>
<p>当前，很多云产品也都构建在S3之上，围绕对象存储基础设施，构建了庞大的生态。除了云商巨头，像Snowflake、Databricks这些明星data infra公司，更是离不开对象存储。</p>
<p>国内近年来也有基于对象存储创业的公司，如兼容S3协议的JuiceFS，新式云仓Databend。</p>
<p>从最几年的趋势来看，S3或者说对象存储俨然已经成为云上数据湖的基础。</p>
<h2 id="使用JuiceFS让CITA-Cloud跑在对象存储上"><a href="#使用JuiceFS让CITA-Cloud跑在对象存储上" class="headerlink" title="使用JuiceFS让CITA-Cloud跑在对象存储上"></a>使用JuiceFS让CITA-Cloud跑在对象存储上</h2><p>对象存储普遍用于存放图片、音视频等静态文件。</p>
<p>那么，既然作为存储的一种，CITA-Cloud能否直接运行在对象存储之上，而不占用本地空间呢？答案是肯定的，下面我们演示下怎么使用JuiceFS让CITA-Cloud跑在对象存储上。</p>
<h3 id="JuiceFS简介与使用"><a href="#JuiceFS简介与使用" class="headerlink" title="JuiceFS简介与使用"></a>JuiceFS简介与使用</h3><p>JuiceFS是为云环境设计，兼容 POSIX、HDFS 和 S3 协议的分布式文件系统，具体介绍可以查看<a href="https://juicefs.com/zh-cn/">官网</a></p>
<p>在我看来，JuiceFS能让对象存储上的一个Bucket变成一块大的云盘，挂载在本地，让应用能像读取本地文件一样来操作对象存储上的对象。</p>
<p>JuiceFS主要分为三部分：JuiceFS客户端、数据存储(公有云对象存储&#x2F;MinIO等)、元数据引擎(Redis&#x2F;SQLite等)。</p>
<p>这里我们使用Redis作为元数据引擎。JuiceFS提供了针对Kubernetes环境的CSI，并提供了Helm的安装方式。</p>
<p>JuiceFS安装时需要一个存储来保存对应文件系统的元数据，这里我们使用Redis；后端对象存储使用MinIO。</p>
<p>Helm安装时的配置如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">storageClasses:</span><br><span class="line">- # -- `StorageClass` Name. It is important.</span><br><span class="line">  name: juicefs-sc</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">-- Default is `<span class="literal">true</span>` will create a new `StorageClass`. It will create `Secret` and `StorageClass` used by CSI Driver.</span></span><br><span class="line">  enabled: true</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">-- Either `Delete` or `Retain`. Refer to [this document](https://juicefs.com/docs/csi/guide/resource-optimization<span class="comment">#reclaim-policy) for more information.</span></span></span><br><span class="line">  reclaimPolicy: Delete</span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">-- Additional annotations <span class="keyword">for</span> this `StorageClass`, e.g. make it default.</span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">annotations:</span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">  storageclass.kubernetes.io/is-default-class: <span class="string">&quot;true&quot;</span></span></span><br><span class="line">  backend:</span><br><span class="line">    # -- The JuiceFS file system name</span><br><span class="line">    name: &quot;juicefs&quot;</span><br><span class="line">    # -- Connection URL for metadata engine (e.g. Redis), for community edition use only. Refer to [this document](https://juicefs.com/docs/community/databases_for_metadata) for more information.</span><br><span class="line">    metaurl: &quot;redis://:123456@redis-service.default:6379/1&quot;</span><br><span class="line">    # -- Object storage type, such as `s3`, `gs`, `oss`, for community edition use only. Refer to [this document](https://juicefs.com/docs/community/how_to_setup_object_storage) for the full supported list.</span><br><span class="line">    storage: &quot;minio&quot;</span><br><span class="line">    # -- Bucket URL, for community edition use only. Refer to [this document](https://juicefs.com/docs/community/how_to_setup_object_storage) to learn how to setup different object storage.</span><br><span class="line">    bucket: &quot;http://minio.zhujq:9000/juicefs&quot;</span><br><span class="line">    # -- JuiceFS managed token, for cloud service use only. Refer to [this document](https://juicefs.com/docs/cloud/acl) for more details.</span><br><span class="line">    token: &quot;&quot;</span><br><span class="line">    # -- Access key for object storage</span><br><span class="line">    accessKey: &quot;minio&quot;</span><br><span class="line">    # -- Secret key for object storage</span><br><span class="line">    secretKey: &quot;minio123&quot;</span><br></pre></td></tr></table></figure>

<p>安装完成之后，可以看到多了一个名为<code>juicefs-sc</code>的StorageClass，在我的集群中如下所示：<br><img src="/run-CITA-Cloud-on-object-storage/juice_sc.jpg"></p>
<h3 id="创建一条链"><a href="#创建一条链" class="headerlink" title="创建一条链"></a>创建一条链</h3><p>有了相应的StorageClass，我们可以用Cloud-Config来创建一条链，创建时指定StorageClass参数为juicefs-sc。</p>
<p>可以看到一条4个节点的链已经运行起来了，并且能正常出块：<br><img src="/run-CITA-Cloud-on-object-storage/chain_pod.jpg"><br>同时创建了对应的PVC:<br><img src="/run-CITA-Cloud-on-object-storage/juicefs_pvc.jpg"></p>
<p>在MinIO界面上，我们能看到对应的名为juice的bucket被创建，链节点的数据文件被分割为多个chunk进行存储：</p>
<p><img src="/run-CITA-Cloud-on-object-storage/minio.jpg"></p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>对象存储作为一种相对廉价的海量存储，在强调降本增效的今天来看，越来越能体现它的重要性。</p>
<p>当然，上面的例子只是一个实验，想要在生产环境使用，需要做完善的测试和评估，相信将来会有更多能与对象存储一起结合的场景。</p>
]]></content>
      <categories>
        <category>Object Storage</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第一期</title>
    <url>/tech-salon-1/</url>
    <content><![CDATA[<p>U2：</p>
<p>我分享一篇文章吧</p>
<p>U2：</p>
<p><a href="http://mp.weixin.qq.com/s?__biz=MzA4MzE1MzQ3MA==&mid=2450143644&idx=1&sn=e26b237b0f4aaa2b35bfc3e6ecffe906&chksm=88045ecdbf73d7dbafae2a2cf634b85294314c72607f3ef8c34a37e63bb5a315621dd76fc45d&mpshare=1&scene=1&srcid=1025yttN4S3P8UEFnx8RVyec&sharer_sharetime=1635670635719&sharer_shareid=c4e24cfa2e469b6626d2dcfbf10c1f34#rd">打浦路（Taproot）比你想的宽｜预言家周报#143</a></p>
<p>U2：</p>
<p>这个是介绍最新的比特币的一个软分叉taproot</p>
<p>U2：</p>
<p>这里主要的几个特性，一个是默克尔化抽象语法树（Merklized Abstract Syntax Trees, MAST），这个其实我理解还是默克尔树的一种用法</p>
<p>U2：</p>
<p>比特币为什么搞这个呢？</p>
<p>U2：</p>
<p>其实还是跟原来的P2SH的设计有关</p>
<p>U2：</p>
<p>我们看下原来比特币P2SH的设计</p>
<p>U2：</p>
<p><img src="/tech-salon-1/lockscript.png"></p>
<p>U2：</p>
<p>这里其实在一开始未解锁交易的时候，这笔资金有一定的隐私性，但是一旦解锁，就必须暴露全部的脚本，隐私性不好</p>
<p>U2：</p>
<p>所以现在其实是将一个合约不同的条件分支组成一个默克尔树</p>
<p>U2：</p>
<p><img src="/tech-salon-1/merkletree.png"></p>
<p>U2：</p>
<p>这里我也在思考联盟链或者eth是否也能实现类型的效果呢？</p>
<p>U2：</p>
<p>这个问题大家可以思考下</p>
<p>U2：</p>
<p>另外就是Schnorr 签名，这个签名一个比较重要的属性就是可以把多个私钥的签名，可以聚合成一个签名，看起来仿佛是一把私钥签出的。</p>
<p>这样的话就为n-n 这种签名提供一种隐私性</p>
<p>U2：</p>
<p>对于其他人来看，其实并不知道是一个人签的还是多个人签的</p>
<p>U2：</p>
<p>对于m-n的签名呢？和前面的的Mast结合，其实也能打到一样的效果。比如一个2-3的多签名，可以理解为多个不同分支的2-2</p>
<p>U2：</p>
<p>Gregory Maxwell 写道 ：</p>
<p>在讨论默克尔化脚本时，一个大家常常提起的问题是，我们能否实现一种精巧的合约，使其与最常见、最无聊的支付没有分别。不然的话，使用这些时髦技术的输出的匿名集，也就是另一个小众集合而已，在实践中没有多大的意义。</p>
<p>U2：</p>
<p>Maxwell其实提到了一种抽象而且通用的做法</p>
<p>U2：</p>
<p>就是如何做到隐私，而这个隐私又是一种普通的交易</p>
<p>U2：</p>
<p>原来其实个人钱包和合约钱包是有差别的</p>
<p>U2：</p>
<p>但是可以通过MAST和Schnorr签名，再把个人和合约用户都统一到一个脚本模式下</p>
<p>U2：</p>
<p>P2TR</p>
<p>U2：</p>
<p>然后经过这样的处理，其实对于外界来看，是无法分辨到底是个人，还是合约，做到大隐隐于市</p>
<p>U2：</p>
<p>其实历史上的每一次比特币的升级都是非常慎重，修改也是非常小的，但是也是非常精密的</p>
<p>U2：</p>
<p>最后留给大家一个问题，如何在eth合约上实现类似的特性呢？</p>
<p>————— 2021-11-06 —————<br>rink：</p>
<p>感觉以太坊上不好做，因为以太坊智能合约是任意的计算程序，而比特币的lock script是一个判定程序，只返回成功与否。我在秘猿的时候其实研究过这个问题。我当时就想用正常的程序描述业务操作，然后有个工具能自动提取出其中的判定程序。当然有个最简单的方法，就是assert，但是这还是需要在链上重复所有的计算。我设想的是像零知识证明一样，验证和计算是不对等的，验证的工作量要比计算少。这个好像是没有找到通用的解决方案。</p>
]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第十二期 - OAuth2</title>
    <url>/tech-salon-12-oauth2/</url>
    <content><![CDATA[<h3 id="1-什么是OAuth2"><a href="#1-什么是OAuth2" class="headerlink" title="1.什么是OAuth2"></a>1.什么是OAuth2</h3><blockquote>
<p>　　OAuth（开放授权）是一个开放标准，允许用户授权第三方移动应用访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方移动应用或分享他们数据的所有内容，OAuth2.0是OAuth协议的延续版本，但不向后兼容OAuth 1.0即完全废止了OAuth1.0。</p>
</blockquote>
<h3 id="2-应用场景"><a href="#2-应用场景" class="headerlink" title="2.应用场景"></a>2.应用场景</h3><blockquote>
<ul>
<li>第三方应用授权登录：在APP或者网页接入一些第三方应用时，时长会需要用户登录另一个合作平台，比如QQ，微博，微信的授权登录。</li>
<li>原生app授权：app登录请求后台接口，为了安全认证，所有请求都带token信息，如果登录验证、请求后台数据。</li>
<li>前后端分离应用：前后端分离框架，前端请求后台数据，需要进行oauth2安全认证，比如使用vue、react后者h5开发的app。</li>
</ul>
</blockquote>
<h3 id="3-应用场景"><a href="#3-应用场景" class="headerlink" title="3.应用场景"></a>3.应用场景</h3><blockquote>
<ul>
<li>Third-party application：第三方应用程序，本文中又称”客户端”（client），比如打开知乎，使用第三方登录，选择qq登录，这时候知乎就是客户端。</li>
<li>HTTP service：HTTP服务提供商，本文中简称”服务提供商”，即上例的qq。</li>
<li>Resource Owner：资源所有者，本文中又称”用户”（user）,即登录用户。</li>
<li>User Agent：浏览器 或 App。</li>
<li>Authorization server：认证服务器，即服务提供商专门用来处理认证的服务器。</li>
<li>Resource server：资源服务器，即服务提供商存放用户生成的资源的服务器。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。</li>
</ul>
</blockquote>
<h3 id="4-运行流程"><a href="#4-运行流程" class="headerlink" title="4.运行流程"></a>4.运行流程</h3><blockquote>
<p>（A）用户打开客户端以后，客户端要求用户给予授权。</p>
<p>（B）用户同意给予客户端授权。</p>
<p>（C）客户端使用上一步获得的授权，向认证服务器申请令牌。</p>
<p>（D）认证服务器对客户端进行认证以后，确认无误，同意发放令牌。</p>
<p>（E）客户端使用令牌，向资源服务器申请获取资源。</p>
<p>（F）资源服务器确认令牌无误，同意向客户端开放资源。</p>
</blockquote>
<p><img src="/tech-salon-12-oauth2/1.png"></p>
<h3 id="5-授权模式"><a href="#5-授权模式" class="headerlink" title="5.授权模式"></a>5.授权模式</h3><blockquote>
<ul>
<li>授权码模式（authorization code）</li>
<li>简化模式（implicit）</li>
<li>密码模式（resource owner password credentials）</li>
<li>客户端模式（client credentials）</li>
</ul>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">授权码模式（authorization code）是功能最完整、流程最严密的授权模式。</span><br><span class="line"></span><br><span class="line">（1）用户访问客户端，后者将前者导向认证服务器，假设用户给予授权，认证服务器将用户导向客户端事先指定的&quot;重定向URI&quot;（redirection URI），同时附上一个授权码。</span><br><span class="line">（2）客户端收到授权码，附上早先的&quot;重定向URI&quot;，向认证服务器申请令牌：</span><br><span class="line"> GET /oauth/token?response_type=code&amp;client_id=test&amp;redirect_uri=重定向页面链接。请求成功返回code授权码，一般有效时间是10分钟。</span><br><span class="line">（3）认证服务器核对了授权码和重定向URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token）。</span><br><span class="line"> POST /oauth/token?response_type=authorization_code&amp;code=SplxlOBeZQQYbYS6WxSbIA&amp;redirect_uri=重定向页面链接。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">简化模式（implicit grant type）不通过第三方应用程序的服务器，直接在浏览器中向认证服务器申请令牌，跳过了&quot;授权码&quot;这个步骤，因此得名。所有步骤在浏览器中完成，令牌对访问者是可见的，且客户端不需要认证。</span><br><span class="line"></span><br><span class="line">（A）客户端将用户导向认证服务器。</span><br><span class="line">（B）用户决定是否给于客户端授权。</span><br><span class="line">（C）假设用户给予授权，认证服务器将用户导向客户端指定的&quot;重定向URI&quot;，并在URI的Hash部分包含了访问令牌。</span><br><span class="line">（D）浏览器向资源服务器发出请求，其中不包括上一步收到的Hash值。</span><br><span class="line">（E）资源服务器返回一个网页，其中包含的代码可以获取Hash值中的令牌。</span><br><span class="line">（F）浏览器执行上一步获得的脚本，提取出令牌。</span><br><span class="line">（G）浏览器将令牌发给客户端。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">密码模式（Resource Owner Password Credentials Grant）中，用户向客户端提供自己的用户名和密码。客户端使用这些信息，向&quot;服务商提供商&quot;索要授权。在这种模式中，用户必须把自己的密码给客户端，但是客户端不得储存密码。这通常用在用户对客户端高度信任的情况下。一般不支持refresh token。</span><br><span class="line"></span><br><span class="line">（A）用户向客户端提供用户名和密码。</span><br><span class="line">（B）客户端将用户名和密码发给认证服务器，向后者请求令牌。</span><br><span class="line">（C）认证服务器确认无误后，向客户端提供访问令牌。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">客户端模式（Client Credentials Grant）</span><br><span class="line"></span><br><span class="line">指客户端以自己的名义，而不是以用户的名义，向&quot;服务提供商&quot;进行认证。严格地说，客户端模式并不属于OAuth框架所要解决的问题。在这种模式中，用户直接向客户端注册，客户端以自己的名义要求&quot;服务提供商&quot;提供服务，其实不存在授权问题。</span><br><span class="line"></span><br><span class="line">（A）客户端向认证服务器进行身份认证，并要求一个访问令牌。</span><br><span class="line">（B）认证服务器确认无误后，向客户端提供访问令牌。</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第二期</title>
    <url>/tech-salon-2/</url>
    <content><![CDATA[<p>王快乐：</p>
<p>好！我来分享一下 高可用的网络架构 吧</p>
<p>王快乐：</p>
<p>这个高可用的网络架构更偏向于基础设施，与我最近的工作内容相关。</p>
<p>王快乐：</p>
<p>希望能给应用层网络高可用带来启发吧。</p>
<p>王快乐：</p>
<p>所谓基础设施的高可用网络架构，是为了解决提升带宽、增加可靠性，先给大家分享一下基础设施高可用网络架构的拓扑图。</p>
<p><img src="/tech-salon-2/network.png"></p>
<p>王快乐：</p>
<p>这个架构，一会儿再解释。我先分享一些简单网络的知识，做个铺垫。</p>
<p>王快乐：</p>
<p>华为有个 eNSP 模拟器，它提供一些网络设备，可以让我们随意组合，来进行网络实验。</p>
<p>这个是我昨晚做实验的一个拓扑，用来测试 ACL 了。提升网络安全的。通过 ACL 来控制，哪些部门 的 哪些设备 能访问 哪些网络。</p>
<p>这是 eNSP 模拟器，挺有意思的。我总不能买一堆设备回来做实验。除了华为，华三和思科也提供自己的模拟器，意图都是相同的。</p>
<p><img src="/tech-salon-2/ensp.png"></p>
<p>王快乐：</p>
<p>我们常见的网络设备，一般有交换机、路由器、防火墙、无线设备（AP、AC），还有 WAF 这些设备就比较少见。</p>
<p>基础设施的高可用网络架构里面，常规配置是：交换机 比较多，相当多；路由器一般就两台；防火墙也两台；其他设备一边根据需求、成本各方面进行综合考虑是否采用</p>
<p>主要的还是交换机。</p>
<p>王快乐：</p>
<p>交换机一般分为两类，盒式交换机，框式交换机。</p>
<p>盒式交换机就是我们平时见到的那种，一个方形的盒子。</p>
<p>框式交换机式数据中心用的，挺大的，各部分都是模块化的。</p>
<p>王快乐：</p>
<p>这个是盒式交换机</p>
<p><img src="/tech-salon-2/switch.png"></p>
<p>王快乐：</p>
<p>这个是框式交换机，各个部分都是模块化的，坏了直接换。</p>
<p>说到这，有句话真的是，技术进步的厉害，思想进步的慢。与盒式交换机比，框式还是模块化思想，将各个部分模块化、易插拔，坏了直接换模块。</p>
<p><img src="/tech-salon-2/big-switch.png"></p>
<p>王快乐：</p>
<p>盒式交换机，小型园区网、企业网会使用。框式交换机，一般数据中心会使用。</p>
<p>王快乐：</p>
<p>路由器、防火墙、无线设备（AP、AC），这些就不介绍了，这些不是这次分享的重点。</p>
<p>王快乐：</p>
<p>然后，我们再看一下交换机角色。这里有用到了分层的思想。</p>
<p>交换机的角色，分为三类：接入层、汇聚层、核心层。</p>
<p>从拓扑图里看，最下面是接入层，最上面是核心层。</p>
<p>接入层交换机：就是我们平台连接 PC 的交换机，功能很简单。听名字也是，只能接入设备，没有其他功能了。</p>
<p>汇聚层交换机：就是将链路进行汇聚，负责网络层的二层数据转发，只涉及 MAC 地址。也就是同个网络数据的转发。可以直接理解成内网数据转发。</p>
<p>核心层交换机：就有点路由功能了，三层转发，会涉及一些 IP 网络。交换机也有路由功能的，只是做的比较简单。说是分层思想，但也不可能 100% 都分开，也很小的耦合和关联度。</p>
<p><img src="/tech-salon-2/network.png"></p>
<p>yzl：</p>
<p>ip网络是ip协议层的网络吗</p>
<p>王快乐：</p>
<p>我们再来看一下交换机和交换机、交换机和终端设备的连接方式。</p>
<p>交换机和终端设备：我们平时连的比较简单，直接 PC 连上就行了。服务器不是这个样子，公司内网的服务器有四个网口，需要连接两台交换机，提供冗余和带宽的。</p>
<p>交换机和交换机：那绝对是个“环路连接法”。从图上也能看出来，一台交换机，连了两台设备，两台设备又互联，就是个环路。</p>
<p>抽出来就是下图的一个简单拓扑。模拟器比较简单，Server 没有那么多网口。</p>
<p><img src="/tech-salon-2/topo.png"></p>
<p>王快乐：</p>
<p>环路的问题不用担心，有生成树协议，STP RSTP MSTP。</p>
<p>通过这些协议，交换机进行角色选举，切换端口状态。端口状态有的：AP DP BP RP EP 五种状态，这个无需深究。</p>
<p>端口角色确认之后，某些端口是阻塞的，无法转发数据，这样就自己破除环路了。</p>
<p>自动将环路剪裁成树形结构，所以叫生成树协议。</p>
<p>王快乐：</p>
<p>有人可能会问……其实没人问……为啥非要接个环路，再引入一个协议，不接成环路不就好了？</p>
<p>为了冗余，假如一条链路断了，我们还可以走另外一条。</p>
<p><img src="/tech-salon-2/rongyu.png"></p>
<p>Shawn：</p>
<p>条条大路通罗马</p>
<p>王快乐：</p>
<p>是的，就是这个样子。除了冗余外，还能增加内网带宽。</p>
<p>王快乐：</p>
<p>现在回到最开始的架构图</p>
<p>iStack，Intelligent Stack 是华为的叫法，思科也有类似的技术。虚拟交换机，将两个交换机合并成一个逻辑的交换机，在逻辑上是一台设备。iStack 是用盒式交换机的。</p>
<p>CSS CLuster Switch System，也是华为的叫法，也是将多台设备逻辑成一台设备。CSS 需要使用框式交换机实现。</p>
<p>Eth-Trunk：是链路聚合。两台交换机之间，连了很多线。但是，这些线是逻辑上的一根线，提成带宽，增加冗余。</p>
<p>STP：是生成树协议，防止线连多了以后，出现环路。</p>
<p>整个网络，通过设备的方式（双设备，成本高），并辅以软件实现（Eth-Trunk，成本低）来提高冗余、增强可靠性。</p>
<p><img src="/tech-salon-2/network.png"></p>
<p>王快乐：</p>
<p>网络是个基础设施，里面涉及了很多东西，但是这个不是这个分享想表达的。今天的分享想表达的是 技术千变万化，但是背后的思想是固定，我们可以把这些思想借用过来，并进行进一步的提升这些思想，而不仅局限于技术的提升。</p>
<p>王快乐：</p>
<p>好了，今天就分享这么多吧，谢谢大家聆听。[呲牙]</p>
<p>洁洁：</p>
<p>@王快乐 讲讲家用路由和企业路由的区别。</p>
<p>志伟：</p>
<p>最主要的区别就是贵[破涕为笑]</p>
<p>Rain：</p>
<p>实现可靠性的级别不同，企业级要求7x24工作强度不能异常，家用的可接受每日重启</p>
<p>王快乐：</p>
<p>绝大多数东西都是需求和业务驱动的，然后技术只是做个实现。</p>
<p>所以，两者的差别就是功能多少的差别。</p>
<p>家用路由器的需求就是上网就行了：<br>1）PPPoE 做个拨号，NAT 做个地址转换；<br>2）DHCP 自动分配地址，解决用户不会配置、配置麻烦的问题。<br>3）再就是无线功能<br>4）前三点足够了，其他的功能都是面向其他有特殊需求的用户的。<br>5）然后功能多了，再提高一下性能。</p>
<p>家用路由器以解决家庭上网需求为主。</p>
<p>企业路由器就不一样，那老复杂了：<br>1）首先就没有无线功能，路由器的核心是路由三层数据包。家用路由器里并不强调这点。<br>2）焦点还是企业需求：7x24、可用性、QoS、网络隔离、访问安全、外部接入。</p>
]]></content>
      <categories>
        <category>网络</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第三期-量子通信入门</title>
    <url>/tech-salon-3-quantum-communication/</url>
    <content><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>量子通信是什么意思？ 到底有什么厉害之处？</p>
<p>量子计算机又是什么？为什么量子计算机是破解密码的终极武器？</p>
<p>几个月前，因为一次偶然的机会我接触了一本量子力学、量子通信的入门书籍，写得非常通俗易懂、幽默风趣。今天给大家分享的内容就来自这本书啦。</p>
<h2 id="一、光的波粒二象性"><a href="#一、光的波粒二象性" class="headerlink" title="一、光的波粒二象性"></a>一、光的波粒二象性</h2><p>说到主题之前，先谈一谈光的波粒二象性。</p>
<p>光是一种波还是一种粒子？一派是以惠更斯、麦克斯韦、赫兹等为代表的波派，一派是以爱因斯坦为代表的”粒子派“ ，两派斗争了非常久。光是一种波，这个非常好理解，因为光的干涉、衍射等现象很常见，大家都容易理解光是一种类似水波、电磁波一样的<code>波</code>。那么，爱因斯坦为啥会认为光是一种“粒子”呢？这得多亏“光电效应”这个实验，让爱因斯坦想到了答案。 （这个推理简单易懂，不展开说明，可以看书或百度）</p>
<p>那么到底哪一派获胜了呢？为了揭开谜团，科学家们做了很多实验（ N个版本的<strong>双缝干涉实验</strong>） ，想看看光到底是波还是粒子。</p>
<p>实验有N个版本，大家在物理课上学到的版本是最简单的版本，因为光具有<code>波</code>特性，经过2条缝之后，由于波的干涉，在最后的显示板上形成了”斑马线“样的干涉条纹。</p>
<p>科学家们再对实验进行升级，实验中的光变成一个一个光子打出。假设光是一种粒子，那么1个光子只能从一条缝穿过， 这样最终出现的就应该是2条杠，而不是<code>斑马线</code>状的干涉条纹。那么实际结果如何呢？</p>
<p>实验结果是：有时是斑马线，有时候是2道杠。这个结果，和你的“观察姿势”有关。假如你的实验装置能精确获取光子的路径信息，那么最终是2条杠。否则，实验结果就是斑马线。很反直觉对吗？像我们这种从小只学经典物理的普通人（甚至是爱因斯坦大神）觉得很难想象对吗？</p>
<p>所以量子力学的世界里，我们的思维方式就得改一改了。下面先给出波尔（堪称量子力学代言人）总结的量子世界3大基本原则：</p>
<ul>
<li>1）态叠加原理</li>
<li>2）测不准原理：叠加态不可能精确测量 （粒子的位置与动量不可同时被确定，位置的不确定性越小，则动量的不确定性越大，反之亦然。）</li>
<li>3）观察者原理（只要光子的路径信息”可以被精确测量“的环境下，它就会自动坍缩，不能同时穿过2条缝了）</li>
</ul>
<p>讲了这么多还没有讲到量子通信，不过很快就要讲到了！</p>
<h2 id="二、量子纠缠和量子通信"><a href="#二、量子纠缠和量子通信" class="headerlink" title="二、量子纠缠和量子通信"></a>二、量子纠缠和量子通信</h2><p>上面主要说的是一个一个的量子，下面要隆重出场的是<code>一对量子</code>（或者叫<code>孪生粒子</code> ，或者叫<code>纠缠态的一对量子</code>），<strong>量子纠缠</strong>才是加密通信的终极武器。</p>
<p>这里说的”一对量子“ 是怎么造出来的？ –  可以用粒子对撞机，两束粒子相撞，一个母粒子分裂成两个更小的粒子A和B。A和B动量大小相等，方向相反，A和B的角动量（类似自旋）也得相互抵消。如果说自旋态有上、下的定义区分（想象为顺时针、逆时针）。那么，根据量子理论，在不被观测的情况下，粒子处于多种可能性的叠加态。即：不是向上，也不是向下，而是两者并存！观测之后，两个粒子（A、B）之间才有上下之分，保持阴阳平衡，就算A和B相隔十万八千里，这种<code>纠缠关系</code>也会存在。这个就是<strong>量子纠缠</strong>。</p>
<p>这一对量子，就好比一对魔法硬币，不管千山万水，只要一枚硬币翻到正面朝上，另一枚一定会瞬间变成反面朝上。（<code>超距作用</code>）</p>
<p>这能为我们干点啥呢？如果把上面的量子A和B放在通信的两端。请看下面的3个步骤：</p>
<ul>
<li>1）量子A （想象成硬币）抛出 <code>反正正反</code>，量子B 收到相反值，很容易就知道A发出的是<code>反正正反</code>—　这个就是对称密钥</li>
<li><ol start="2">
<li>A端的人用微信给B端的人补发一串同长度的纠错码 <code>错对对错</code>  — 这个就是密文</li>
</ol>
</li>
<li>3）B端的人得到了有用的信息 <code>正反反正</code> （可能翻译过来就是：明天偷袭珍珠港）— 这个就是明文</li>
</ul>
<p>这个过程就是大名鼎鼎的<strong>量子通信</strong>了。</p>
<p>以前你可能以为量子通信一定超快，现在知道了，它不是快，微信传输的延时几秒，它就要延时几秒。它的威力在于加密！而且无条件安全的加密！</p>
<p>什么是绝对安全的加密，需要具备3大条件（香农证明过的）：</p>
<ul>
<li>1）随机加密</li>
<li>2）明文密文等长</li>
<li>3）一次一密</li>
</ul>
<p>这3个条件量子通信都满足了（不信你再看看）。</p>
<p>“无条件安全的加密”，听起来很牛是不是，别急，量子通信还有更牛的——它具有反窃听属性：</p>
<p>它能发现窃听者，而且，窃听者不知道被我发现了！</p>
<p>所以量子通信不怕窃听，也不怕破解，它只怕干扰。（比如，用强激光照射接收器）</p>
<p>好了，量子通信的就是这么简单又厉害。</p>
<p>再来看看我们熟知的加密方法：</p>
<ul>
<li>1）传统的对称加密。它怕啥？怕密码本泄露！</li>
<li>2）非对称加密（如RSA）。 它怕啥？ 怕算力。它是能被破解的。（超级计算机曾经用几个月的时间破解过，如果是量子计算机呢？）</li>
</ul>
<p>哦，刚才说到量子计算机。量子计算机为啥快？</p>
<p>量子计算机加速的根源在于量子叠加态的存在，在经典计算中，N位比特的CPU在同一时刻只能存储一种状态，但在量子计算中，N位量子比特的CPU在同一时刻可以同时保存2^N 个状态。所以，量子计算机的恐怖性能完全来源于并行，而非某种绝妙的算法。</p>
<p>我的分享就是这些。</p>
<p>如果想要了解更多，比如锲而不舍的科学家们是如何将双缝干涉实验改进演变到几近变态版本的，可以查阅原文：《猫、爱因斯坦和密码学 - 我也能看懂的量子通信》 作者：神们自己</p>
]]></content>
      <categories>
        <category>量子通信</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第三期</title>
    <url>/tech-salon-3/</url>
    <content><![CDATA[<p>U2：</p>
<p>今天分享下openzeppelin的Upgradeable Smart Contract，就是可升级智能合约</p>
<p>U2：</p>
<p>我们先看下如何用他这个提供的插件来写智能合约</p>
<p>U2：</p>
<p>然后再来介绍下里面的原理和实现逻辑</p>
<p>U2：</p>
<p>首先我看一个正常的合约</p>
<p>U2：</p>
<p>比如一个合约里有他的初始化构造方法，我们要把这个构造方法，替换成initialize方法</p>
<p>U2：</p>
<p>我们先不考虑为什么需要这么做，我们先看下他的使用，后面我们再去看他的原理</p>
<p>U2：</p>
<p><img src="/tech-salon-3/contract.png"></p>
<p>比如这个MYcontract合约，我们就把他的构造方法换成initialize</p>
<p>U2：</p>
<p>然后能因为构造方法其实只能被调用一次</p>
<p>U2：</p>
<p><img src="/tech-salon-3/contract1.png"></p>
<p>为了防止出现被错误调用，所以这里加了一些条件限制</p>
<p>U2：</p>
<p>我记得很早以前某篇文章里介绍智能这种编程方式也叫，面向条件的编程。</p>
<p>U2：</p>
<p>大家感兴趣可以去查查这个面向条件编程，因为无论是普通合约逻辑的编写，还是在安全方面做的一些保护，比如防重入攻击，都有点这个条件编程的思想。</p>
<p>U2：</p>
<p>我们接着介绍</p>
<p>U2：</p>
<p>其实每次这样去写这个initialize的方法就很麻烦</p>
<p>U2：</p>
<p><img src="/tech-salon-3/contract2.png"></p>
<p>所以这个openz就帮我们实现了一些逻辑，我们只要继承这个就好了</p>
<p>U2：</p>
<p>那咱们平时写合约的时候，实际上还有一些合约继承的东西</p>
<p>U2：</p>
<p>这种情况能，就要稍微处理下</p>
<p><img src="/tech-salon-3/contract3.png"></p>
<p>U2：</p>
<p>然后如果咱们用了erc20这种合约咋办呢？</p>
<p>U2：</p>
<p>这个其实openz也提供了他标准的可升级的erc20的实现，我们把原来继承erc20，直接替换掉就好了</p>
<p><img src="/tech-salon-3/interface.png"></p>
<p>U2：</p>
<p>然后能还要注意一点，就是在变量声明的时候，不要做初始化，Avoiding Initial Values in Field Declarations</p>
<p>U2：</p>
<p>比如这个</p>
<p><img src="/tech-salon-3/contract4.png"><br>U2：</p>
<p>这种其实就相当于，我们写了一个构造方法，然后在构造方法里初始化这个storage</p>
<p>U2：</p>
<p><img src="/tech-salon-3/contract5.png"><br>就需要把他改成上面这样的</p>
<p>U2：</p>
<p>还有一种是常量</p>
<p><img src="/tech-salon-3/contract6.png"><br>U2：</p>
<p>这种不一样的，他实际上是编码在合约ｃｏｄｅ里的</p>
<p>U2：</p>
<p>我们在部署合约的时候，evm实际上在完成构造方法后，构造方法这段代码实际上就没有保存在合约地址下的code的，因为反正只是在部署的时候用。</p>
<p>U2：</p>
<p>但是初始化的常量实际上会跟随这个合约code，保存在合约地址下的代码里。</p>
<p>U2：</p>
<p>然后这个初始化里构造的变量呢，也是以storage的形式存下来的。</p>
<p>U2：</p>
<p>然后能，在初始化合约的时候，可能在初始化方法中会创建一个新的合约实例</p>
<p>U2：</p>
<p><img src="/tech-salon-3/contract7.png"></p>
<p>比如这种</p>
<p>U2：</p>
<p>这能里面的这个erc20合约实际上是不支持升级的</p>
<p>U2：</p>
<p>如果想要这个合约支持升级怎么处理呢？</p>
<p>U2：</p>
<p>实际就是先部署里面这个合约，然后在初始化的时候把这个里面的合约地址再传进去就好了</p>
<p><img src="/tech-salon-3/contract8.png"></p>
<p>U2：</p>
<p>然后能，咱们在升级合约的时候也要注意几点</p>
<p>U2：</p>
<p>第一不要更改原有的storage</p>
<p>U2：</p>
<p>比如原来是这样</p>
<p>U2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">contract MyContract &#123;</span><br><span class="line">    uint256 private x;</span><br><span class="line">    string private y;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>U2：</p>
<p>我们不能把这个变量类型改掉</p>
<p>U2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">contract MyContract &#123;</span><br><span class="line">    string private x;</span><br><span class="line">    string private y;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>U2：</p>
<p>也不能换顺序</p>
<p>U2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">contract MyContract &#123;</span><br><span class="line">    string private y;</span><br><span class="line">    uint256 private x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>U2：</p>
<p>也不能在前面插入一个变量</p>
<p>U2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">contract MyContract &#123;</span><br><span class="line">    bytes private a;</span><br><span class="line">    uint256 private x;</span><br><span class="line">    string private y;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>U2：</p>
<p>能做的是，可以给这个变量换个名字，或者在后面追加一个变量</p>
<p>U2：</p>
<p>比如这个</p>
<p>U2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">contract MyContract &#123;</span><br><span class="line">    uint256 private x;</span><br><span class="line">    string private y;</span><br><span class="line">    bytes private z;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>U2：</p>
<p>就是加在最后</p>
<p>U2：</p>
<p>当然还有其他的错误使用情况，大家可以看他们的官方文档</p>
<p>U2：</p>
<p>然后能这个原理是怎么实现的呢</p>
<p>U2：</p>
<p>其实很简单</p>
<p>U2：</p>
<p>他这个就是用了一个代理合约</p>
<p>U2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">User ---- tx ---&gt; Proxy ----------&gt; Implementation_v0</span><br><span class="line">|</span><br><span class="line">------------&gt; Implementation_v1</span><br><span class="line">|</span><br><span class="line">------------&gt; Implementation_v2</span><br></pre></td></tr></table></figure>

<p>U2：</p>
<p>这个我们用户调用的一直都是这个代理合约</p>
<p>U2：</p>
<p>然后具体的实现合约可以换掉，然后把地址绑定到这个代理合约之类</p>
<p>U2：</p>
<p>我们看下这个代理合约的大致的逻辑思路</p>
<p>U2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">assembly &#123;</span><br><span class="line">    let ptr := mload(0x40)</span><br><span class="line"></span><br><span class="line">    // (1) copy incoming call data</span><br><span class="line">    calldatacopy(ptr, 0, calldatasize)</span><br><span class="line"></span><br><span class="line">    // (2) forward call to logic contract</span><br><span class="line">    let result := delegatecall(gas, _impl, ptr, calldatasize, 0, 0)</span><br><span class="line">    let size := returndatasize</span><br><span class="line"></span><br><span class="line">    // (3) retrieve return data</span><br><span class="line">    returndatacopy(ptr, 0, size)</span><br><span class="line"></span><br><span class="line">    // (4) forward return data back to caller</span><br><span class="line">    switch result</span><br><span class="line">    case 0 &#123; revert(ptr, size) &#125;</span><br><span class="line">    default &#123; return(ptr, size) &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>U2：</p>
<p>这个是assembly写的</p>
<p>U2：</p>
<p>这个第一步就是把calldata拷贝出来，就是交易里的data，第二步骤就是用这个delegatecall，然后第三就是获取return data，第四就是再吧这个returndata 返回</p>
<p>U2：</p>
<p>我们主要看这个delegatecall</p>
<p>U2：</p>
<p>solidity里有几种call，大家可以对比下这几个的区别</p>
<p>U2：</p>
<p>我们主要看这个delegatecall</p>
<p>U2：</p>
<p>这个能，其实就是把目标合约的code在当前合约的环境下执行，使用当前合约的storage</p>
<p>U2：</p>
<p>就是说逻辑合约的代码其实是被执行了，但是逻辑合约的storage其实是没有用的</p>
<p>U2：</p>
<p>他用的是在代理合约的storage</p>
<p>U2：</p>
<p>所以能，其实合约在初始化的时候，如果用solidity的构造方法，这个storage就留在逻辑合约里了，所以呢前面的把构造合约该成init就是这个道理，通过代理合约来调用iinit</p>
<p>U2：</p>
<p>然后这些storage就留在这个代理合约里了</p>
<p>U2：</p>
<p>然后后面就可以持续升级</p>
<p>U2：</p>
<p>刚才讲到在代理合约里保存逻辑合约的stroage</p>
<p>U2：</p>
<p>这里就有一个问题，就是原来代理合约因为也要保存逻辑合约的地址</p>
<p>U2：</p>
<p>比如在代理合约里声明了一个stroage</p>
<p>U2：</p>
<p>然后因为在solidity实现的时候，他会把这个storage给一个postion，然后实际上是按照他声明的位置来确定最终在合约下面的抽象模型里的kv里存的位置的</p>
<p>U2：</p>
<p>比如我们声明了三个变量</p>
<p>U2：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">address a;</span><br><span class="line">address b;</span><br><span class="line">address c;</span><br></pre></td></tr></table></figure>

<p>U2：</p>
<p>这三个实际上存在哪里是跟他的position有关的，就是变量的顺序</p>
<p>U2：</p>
<p>最终保存的时候我记得好像是按照合约地址＋position来作为这个storage的key的，然后再用mpt树做处理，具体这的细节，大家可以以后去研究。</p>
<p>U2：</p>
<p>然后这里可能就有一个问题</p>
<p><img src="/tech-salon-3/table.png"></p>
<p>U2：</p>
<p>就是proxy这里的storage，比如第一个和逻辑合约里的第一个storage，因为都是第一个，都存到proxy合约里</p>
<p>U2：</p>
<p>所以这里就冲突了</p>
<p>U2：</p>
<p>怎么办呢？</p>
<p>U2：</p>
<p><img src="/tech-salon-3/table1.png"></p>
<p>我们给他一个随即的slot就可以了</p>
<p>U2：</p>
<p>这里可以参考<a href="https://eips.ethereum.org/EIPS/eip-1967">https://eips.ethereum.org/EIPS/eip-1967</a></p>
<p>U2：</p>
<p>原理很简单，实际就是算一个随机的数，然后用solidity的assembly语言里的一个设置stroage的命令，设置这个值就好了</p>
<p>U2：</p>
<p>这样就解决了代理合约和实现合约里的stroage冲突的问题</p>
<p>U2：</p>
<p>然后在实现合约里，因为stroage实际无论怎么升级都是公用的代理合约里的</p>
<p>U2：</p>
<p>所以这里就不能修改原来的stroage，要不然就冲突了</p>
<p><img src="/tech-salon-3/table2.png"><br>U2：</p>
<p>然后在使用上，openz和truffle也好，还有和一些其他的合约开发工具都是做了集成，使用起来都还是比较方便的</p>
<p>U2：</p>
<p>我的介绍就到这里</p>
<p>U2：</p>
<p>谢谢大家</p>
]]></content>
      <categories>
        <category>智能合约</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第四期-跨链</title>
    <url>/tech-salon-4-cross-chain/</url>
    <content><![CDATA[<p>Rain：</p>
<p>关于跨链，V神在2016发布过一篇报告《Chain Interoperability》，里面总结了3种跨链方案的实践：</p>
<ul>
<li>Notaries（公证员）</li>
<li>Relays（中继技术）</li>
<li>Hash-locking（哈希锁定）</li>
</ul>
<p>Rain：</p>
<p>今天我就分享一下 Hash-locking（哈希锁定）的原理</p>
<p>Rain：</p>
<p>哈希锁定技术的核心原理是使用带有哈希锁定机制的合约进行资产锁定实现质押效果，为不同资产之间的交易提供了信任基础。</p>
<p>Rain：</p>
<p>使用哈希锁定机制的合约称为哈希时间锁合约（Hash Time Locked Contract，简称HTLC)</p>
<p>Rain：</p>
<p>HTLC 的核心是时间锁和哈希锁</p>
<p>Rain：</p>
<p>时间锁是指，交易双方约定在某个时间内提交才有效，超时则提案方案失效（无论是提出方或接受方）</p>
<p>Rain：</p>
<p>如果交易因为各种原因未能成功，时间锁能够让交易参与各方拿回自己资金，避免因欺诈或交易失败造成的损失</p>
<p>Rain：</p>
<p>哈希锁是指，对一个哈希值 h，如果提供原像 s 使得 hash(s) &#x3D; h，则提案有效，否则失效</p>
<p>Rain：</p>
<p>HTLC 主要由两部分逻辑组成：哈希验证和过期验证</p>
<p>Rain：</p>
<p>前面是概念定义，接下来以2个持有不同区块链资产的用户使用HTLC进行资产交换来示例说明</p>
<p>Rain：</p>
<p>假设 A用户 和 B用户 有资产交换的需求，A用户 想用 m 个 x链数字资产 和 B用户 换 n 个 y链数字资产</p>
<p>Rain：</p>
<p>大家可以把 x链数字资产 替换为 BTC，把 y链数字资产 替换为 ETH 来做实例理解</p>
<p>Rain：</p>
<p>首先需要在两条链上部署哈希时间锁定合约，然后执行如下步骤进行跨链资产交换</p>
<p>Rain：</p>
<ol>
<li>A用户 随机构建一个字符串 s，并计算出其哈希 h &#x3D; hash(s)；</li>
</ol>
<p>Rain：</p>
<ol start="2">
<li>A用户 将 h 发送给 B用户 的HTLC合约；</li>
</ol>
<p>Rain：</p>
<ol start="3">
<li>A用户 锁定自己的 m 个 x链数字资产，并设置一个较长的锁定时间 t1, 并设置了获取该 x链数字资产 的一个条件：如果 B用户 能够提供 h 的原始值 s 就可以得到该 x链数字资产；</li>
</ol>
<p>Rain：</p>
<ol start="4">
<li>B用户 观察到 A用户 HTLC合约中锁定了一个 x链数字资产, 然后 B用户 锁定自己的 n 个 y链数字资产 资产，并设置一个相对较短的锁定时间 t2, t2 &lt; t1, B用户 也设置了获取条件：如果 A用户 能提供 h 的原始值 s 就可以获取 n 个 y链数字资产；</li>
</ol>
<p>Rain：</p>
<ol start="5">
<li>A用户 将自己最初生成的字符串 s 发送到 B用户 的HTLC合约里取得了 n 个 y链数字资产；如果到时间点 t2 后仍未解锁，则退回 n 个 y链数字资产给 B用户；</li>
</ol>
<p>Rain：</p>
<ol start="6">
<li>B用户 观察到步骤 5 中 A用户 的 s 值，将其发送给 A用户 的HTLC合约成功获取 m 个 x链数字资产；如果到时间点 t1 后仍未解锁，则退回 m 个 x链数字资产给 A用户；</li>
</ol>
<p>Rain：</p>
<p>经过以上步骤，就完成了 A用户 用 m 个 x链数字资产 和 B用户 交换 n 个 y链数字资产</p>
<p>Rain：</p>
<p>以上的跨链事务流程图如下</p>
<p><img src="/tech-salon-4-cross-chain/workflow.png"></p>
<p>Rain：</p>
<p>我们可以代入参数来理解：</p>
<p>A用户 &#x3D; Alice<br>B用户 &#x3D; Bob<br>x链数字资产 &#x3D; BTC<br>y链数字资产 &#x3D; ETH<br>m &#x3D; 1<br>n &#x3D; 20</p>
<p>Rain：</p>
<p>场景是：Alice 用 1 个 BTC 和 Bob 交换 20 个 ETH</p>
<p>Rain：</p>
<p>那事务流程图就是</p>
<p><img src="/tech-salon-4-cross-chain/workflow1.png"><br>Rain：</p>
<p>HTLC 的应用有以下的限制</p>
<p>Rain：</p>
<ol>
<li>协议兼容性较低</li>
</ol>
<p>Rain：</p>
<p>HTLC 实施需要满足一些必要条件：</p>
<p>Rain：</p>
<p>一是用户资产所在区块链需要基于相同哈希算法（比如都使用比较常用的 SHA-256 哈希算法）；</p>
<p>Rain：</p>
<p>二是区块链需要兼容 HTLC 和其他可编程功能（如BTC的Bitcoin Script或ETH的智能合约）；</p>
<p>Rain：</p>
<p>三是交易双方需要在同一区块链上有交易账户；</p>
<p>Rain：</p>
<p>四是对于不包含资产托管账户（例如 Fabric）的区块链需要借助智能合约来构建账户概念。</p>
<p>Rain：</p>
<p>限制2. 时间锁机制造成退款时间过长</p>
<p>Rain：</p>
<p>时间锁有效降低了交易对手风险，但如果有中间节点因故无法进行交易，则必须等时间锁设定时间结束才能退款</p>
<p>Rain：</p>
<p>哈希锁定技术不是一种普适的跨链通讯机制，它解决的是价值交换问题，而不是信息传递问题，因此应用领域比较狭小</p>
<p>Rain：</p>
<p>以上就是我今天的分享</p>
<p>Rain：</p>
<p>抛出个问题：哈希锁定技术能否用于做跨链资产转移？</p>
<p>U2：</p>
<p>htlc应该是支持资产的原子交换</p>
<p>U2：</p>
<p>像侧链或者中继链这种relay模式的支持转移</p>
<p>U2：</p>
<p>相当于从a到b</p>
<p>U2：</p>
<p>另外研究区块链会发现，和传统的分布式事务算法里不同的是，区块链例一般都是用密码学加博弈论来保证的</p>
<p>U2：</p>
<p>像htlc本质上是一种序贯博弈</p>
<p>U2：</p>
<p>另外htlc也是依赖链本身的安全性，比如如果发生51%攻击依然会有资产丢失的风险</p>
<p>U2：</p>
<p>我记得某人写过一篇文章分析这个htlc博弈的过程，应该有一份是有一点点便宜可以占的，另外一方相对处于一个博弈的被动选择。比如就是说其中一方可以根据汇率选择是否进行交易，后后手只能根据前面的人选择被动选择，否则就有损失。</p>
]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第四期-分布式身份</title>
    <url>/tech-salon-4-did/</url>
    <content><![CDATA[<p>洁洁：</p>
<p>我来分享个简单点的</p>
<p>洁洁：</p>
<p>就是did</p>
<p>洁洁：</p>
<p>分布式数字身份的英文缩写</p>
<p>洁洁：</p>
<p>拥有分布式身份不止是人，包括组织，甚至未来也包括物品。这些人或者组织、物品不简单依靠于原先中心化权威机构，无法被拿走或者删除，而且是终身携带的身份。</p>
<p>洁洁：</p>
<p>国际电子技术委员会将“身份”定义为“一组与实体关联的属性”。数字身份通常由身份标识符及与之关联的属性声明来表示，分布式数字身份包括：分布式数字身份标识符和数字身份凭证（声明集合）两部分。</p>
<p>洁洁：</p>
<p>分布式数字身份标识符（DID）是由字符串组成的标识符，用来代表一个数字身份，不需要中央注册机构就可以实现全球唯一性。通常，一个实体可以拥有多个身份，每个身份被分配唯一的DID值，以及与之关联的非对称密钥。不同的身份之间没有关联信息，从而有效地避免了所有者身份信息的归集。</p>
<p>洁洁：</p>
<p>所以每个人都可以拥有多个did</p>
<p>洁洁：</p>
<p>比如这样，不同身份不同did</p>
<p><img src="/tech-salon-4-did/did.png"></p>
<p>洁洁：</p>
<p>“声明（claims）”是指与身份关联的属性信息，这个术语起源于基于声明的数字身份，一种断言（assert）数字身份的方式，独立于任何需要依赖它的特定系统。声明信息通常包括：诸如姓名，电子邮件地址、年龄、职业等。</p>
<p>声明可以是一个身份所有者（如个人或组织）自己发出的，也可以是由其他声明发行人发出的，当声明由发行人签出时被称为可验证声明。用户将声明提交给相关的应用，应用程序对其进行检查，应用服务商可以像信任发行人般信任其签署的可验证声明。多项声明的集合称为凭证（credentials）。</p>
<p>洁洁：</p>
<p>这种身份和传统的帐号有啥区别？</p>
<p>洁洁：</p>
<p>这种身份最大的好处是可以证明某个东西是我发的</p>
<p>洁洁：</p>
<p>传统身份可以被盗用，可以根据系统漏洞被仿冒…甚至dba直接往数据库插句话，你都没法否认</p>
<p>洁洁：</p>
<p>did只要守住自己的私钥，没有对内容签名。全网都可以轻松验假</p>
<p>洁洁：</p>
<p>did是怎么做到的呢？</p>
<p>洁洁：</p>
<p>其实很简单。 did生成的时候会生成一个document们的放在区块链上。这个document当中,就有这个身份的公钥。</p>
<p>洁洁：</p>
<p>所以每次发言的时候，把发言内容用私钥签个名一起发出去。</p>
<p>洁洁：</p>
<p>别人就可以轻松认定，这句话是你发出的。</p>
<p>洁洁：</p>
<p>其他人没有私钥是没办法轻松完成这个签名动作。哪怕是dba他也做不到。</p>
<p>洁洁：</p>
<p>所以在区块链的网络当中，私钥是不比当初常规系统中密码更加重要的东西。</p>
<p>洁洁：</p>
<p>现在基于这种特性，最成熟的应用是可验证声明，简称vc</p>
<p>洁洁：</p>
<p>可验证声明(Verifiable Credential)提供了一种规范来描述实体所具有的某些属性，实现基于证据的信任。DID持有者，可以通过可验证声明，向其他实体(个人、组织、具体事物等)证明自己的某些属性是可信的。同时，结合数字签名和零知识证明等密码学技术，可以使得声明更加安全可信，并进一步保障用户隐私不被侵犯。</p>
<p>洁洁：</p>
<p>这是一张大致示意图</p>
<p><img src="/tech-salon-4-did/example.png"></p>
<p>洁洁：</p>
<p>像现在的绿码应用，其实是很容易钻空子</p>
<p>洁洁：</p>
<p>用上vc就可以轻松解决这些问题</p>
<p>洁洁：</p>
<p>目前w3c对这种身份做了定义</p>
<p>洁洁：</p>
<p>分散标识符(DID)是一种新型的标识符，它是全局惟一的、可解析的、高可用性的，并且可以通过密码验证。DID通常与加密材料(如公钥和服务端点)相关联，用于建立安全的通信通道。DID对于任何有利于自我管理、加密可验证标识符(如个人标识符、组织标识符和物联网场景中的标识符)的应用程序都非常有用。例如，目前W3C可验证凭据的商业部署大量使用分散的标识符来标识人员、组织和事物，并实现许多安全和隐私保护保证。</p>
<p><img src="/tech-salon-4-did/did-sepc.png"></p>
<p>洁洁：</p>
<p>最后给大家看看did标识的结构是什么样的</p>
<p><img src="/tech-salon-4-did/scheme.png"></p>
]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第五期</title>
    <url>/tech-salon-5/</url>
    <content><![CDATA[<p>大勇：</p>
<p>今天要分享的是：云平台中的HPA（horizontal pod autoscalers）水平自动伸缩。</p>
<p>大勇：</p>
<p>虽然通常我们可以部署Deployment时设置pod 的数量。 但是这样设置不够灵活，在不同的运行环境，会导致资源的浪费，而随着吞吐量的上升，又需要不同的设置。</p>
<p>大勇：</p>
<p>HPA 即通过检测pod CPU的负载，解决deployment里某pod负载太重，动态伸缩pod的数量来负载均衡。</p>
<p>大勇：</p>
<p><img src="/tech-salon-5/image.jpeg"></p>
<p>大勇：</p>
<p>HPA可以检测pod cpu的使用情况，通知deployment增加或者减少所管理pod副本的数量。</p>
<p>大勇：</p>
<p>1、假如已经部署好了一个工作负载，deployment&#x2F;kms. 现在需要配置HPA。设置kms的pod数量最小为2，最大为10 执行命令：<code>kubectl autoscale deployment kms --min=2 --max=10</code></p>
<p>大勇：</p>
<p>当我们手动强行设置pod数为1时，kms的pod数量会马上删除一个，然后又重新创建一个新的pod，始终维持到最小数量2</p>
<p>大勇：</p>
<p>从步骤1来看，只是 为POD 限定了一个可设置的范围，也没多神奇的地方。</p>
<p>大勇：</p>
<p>步骤2 将进一步根据POD 负载指标，来设置自动扩容策略</p>
<p>大勇：</p>
<p>假如POD 的CUP 利用率超过80% 自动扩容</p>
<p>大勇：</p>
<p>则这样设置kms 的HPA <code>kubectl autoscale deployment kms --min=2 --max=10 --cpu-percent=80</code></p>
<p>大勇：</p>
<p>这里设置了cpu-percent&#x3D;80，表示pod的cpu利用率超过了百分之80会自动扩容，最小规格2个pod，每超过80%，再自动扩容，直到 pod最大数10.</p>
<p>大勇：</p>
<p>自动扩容，也不是无限扩容，也会参考当前集群总体资源来设置一个合理的区间阈值</p>
<p>大勇：</p>
<p>HPA 虽然能通过各自策略，实现POD的配置自动伸缩； 主要是仰仗SVC的强大的负载均衡</p>
<p>大勇：</p>
<p>k8s中svc有三种类型，分别为ClusterIP、NodePort、LoadBalancer；但是当SVC的类型是ClusterIP时，明显的POD端启动多个或者减少时，对服务的持续访问影响最小；而Deployment的部署方式，通常用在无状态部署，相当于访问每个POD 提供的功能效果是一样的。 即Deployment 部署的应用，最适合HPA自动扩容的方式。</p>
<p>大勇：</p>
<p>而当：SVC 是NodePort 对集群节点端口有依赖；SVC 是LoadBalancer 时对IP资源有要求; 或者通过SatefulSet(有状态) 方式部署的应用，每个POD 都有编号，pod的主机名会映射到DNS，相当于每个POD的都是不一样的。 则这些场景的云应用，采用HPA 自动扩容的方式又不太适用。</p>
<p>大勇：</p>
<p>好了，我今天的分享就到这了，主要是对POD 的自动化扩容提供了一种方法HPA，以及对它使用的场景进行了简单分析。 就我们当前的部署来看，大部分都是deployment 方式，还是能使用上的。</p>
]]></content>
      <categories>
        <category>云计算</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第六期-BLS</title>
    <url>/tech-salon-6-bls/</url>
    <content><![CDATA[<p>忆忆：</p>
<p>那我今天就分享BLS签名吧</p>
<p>忆忆：</p>
<p>BLS 签名不需要随机数，区块中的所有签名都可以组合成单个签名，m-of-n 类型的多重签名比较简单，不需要签名者之间进行多轮通信。</p>
<p>忆忆：</p>
<p>BLS签名需要用到两个结构：哈希到曲线以及曲线配对。哈希到曲线：一般签名将消息哈希视作数字，在这里将消息哈希到椭圆曲线（比较直接的方法是将哈希结果视作x坐标，选择曲线上对应的y较小的点）； 曲线配对：一个将两个不同点映射成一个数字的特殊函数，假设该函数为e（ · ，· ），两个任意的点为P，Q，则e（ ， ）满足如下性质：e（a·P ，b·Q ）&#x3D; e（ab·P ， Q ）&#x3D; e（P ， ab·Q ）&#x3D;e（P ， Q）^ （ab）</p>
<p>忆忆：</p>
<p>签名的具体方案：假设私钥为pk，公钥为P&#x3D;pk️G，签名的消息为m。首先将消息哈希到曲线H（m），然后得到签名S&#x3D;pk️H（m）；验证签名：用公钥检查e（P， H（m））&#x3D;e（G，S）为真即可</p>
<p>忆忆：</p>
<p>聚合签名：假设有（公钥Pi，已签名的消息mi），聚合签名为所有签名的简单和S&#x3D;Sum （Si），只需验证e（G，S）&#x3D;e（P1，H（m1））·…·e（Pn，H（mn））即可； n-of-n多重签名与Schnorr签名类似，也可以在简单加和时添加系数； m-of-n签名：聚合公钥P&#x3D;a1·P1 … an·Pn，ai&#x3D;hash（Pi，{P1，P2，…，Pn}），成员密钥MKi&#x3D;Sum （ai·pki）️H（P，i），成员签名Si&#x3D;pki️H（P，m） MKi，连个签名与联合公钥（S’，P’）只需将成员签名和公钥简单相加即可，验证e（G，S’）&#x3D; e（P’，H(P,m)）· e(P, Sum (H(P,i)))</p>
<p>忆忆：</p>
<p>BLS的弊端在于配对效率低下，函数e（P，Q）的验证难度较大，验证时间长。 我的分享就到这里</p>
]]></content>
      <categories>
        <category>密码学</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第六期-流量代理</title>
    <url>/tech-salon-6-flow/</url>
    <content><![CDATA[<p>大勇：</p>
<p>今天分享与云端的流量入口、流量代理有关。</p>
<p>大勇：</p>
<p>也看过一些云平台的部署架构、拓扑结构图之类的，发现很多，对流量代理、入口的界定不是很清晰。 有的把ingress 当成流量代理、有的认为是K8S service、有的认为是kube-proxy 等。 我这里再梳理下，其实以上都不是流量代理。</p>
<p>大勇：</p>
<p>ingress 是K8S 的一种原生资源(有别于CRUD的自定义资源)，是描述具体的路由规则的。</p>
<p>大勇：</p>
<p>Ingress Controller 是才是流量的入口，是名副其实的流量代理，也可以称为边缘服务(集群内与集群外流量网关)，纵向流量(有叫南北流量的说法)， 一般是Nginx 、traefik和 Haproxy（较少使用）。</p>
<p>大勇：</p>
<p>Ingerss 描述了一个或者多个 域名的路由规则，以 ingress 资源的形式存在。<br>简单说： Ingress 描述路由规则。</p>
<p>大勇：</p>
<p>可以认为ingress 其实就是一个配置文件，而已，在你真正访问pod应用的时候 是没起任何作用的</p>
<p>大勇：</p>
<p>但是这个配置文件又不能删除， 这是因为Ingress Controller 实时实现规则。Ingress Controller 会监听 api server上的 &#x2F;ingresses 资源 并实时生效。</p>
<p>大勇：</p>
<p>是Ingress Controller 是实时监听 api server 上ingresses ，如果没有了，会马上删除 自身的 反向代理配置 ，才让ingress 好像干掉了 就访问不了了。</p>
<p>大勇：</p>
<p>Ingress Controller 流量代理 自身的部署方式 以及服务类型 会决定了，整个K8S 的流量瓶颈，是每个节点的流量决定，还是整个集群所有的节点一起决定的。 这个先放后面，接下来讲，service与 kube-proxy 是否是流量代理的分析</p>
<p>大勇：</p>
<p>ingerss 与service的关系：</p>
<p>大勇：</p>
<p>ingress 在前，service在后， 前者不是流量代理， 顺利成章的会认为是后者，其实也不然。</p>
<p>大勇：</p>
<p>看K8S 对service的概念介绍：Service 概念<br>Kubernetes Service定义了这样一种抽象： Service是一种可以访问 Pod逻辑分组的策略， Service通常是通过 Label Selector访问 Pod组。<br>Service能够提供负载均衡的能力。</p>
<p>大勇：</p>
<p>service 有以下种类型：</p>
<p>大勇：</p>
<p>ClusterIp、NodePort、LoadBalancer(升级版nodeport)</p>
<p>大勇：</p>
<p>Service 也只仅仅是一种策略， 也不是应用每次被访问时，需要经过的组件，也不负载流量的转发。</p>
<p>大勇：</p>
<p>这里可能会有疑问，不负载流量转发，怎么又能实现这么多类型的服务，又怎么实现负载均衡的。</p>
<p>大勇：</p>
<p>这时候就该神奇的kube-proxy K8S组件闪亮登场了</p>
<p>大勇：</p>
<p>kube-proxy是Kubernetes的核心组件，部署在每个Node节点上，它是实现Kubernetes Service的通信与负载均衡机制的重要组件; kube-proxy负责为Pod创建代理服务，从apiserver获取所有server信息，并根据server信息创建代理服务，实现server到Pod的请求路由和转发，从而实现K8s层级的虚拟转发网络。</p>
<p>在k8s中，提供相同服务的一组pod可以抽象成一个service，通过service提供的统一入口对外提供服务，每个service都有一个虚拟IP地址（VIP）和端口号供客户端访问。kube-proxy存在于各个node节点上，主要用于Service功能的实现，具体来说，就是实现集群内的客户端pod访问service，或者是集群外的主机通过NodePort等方式访问service。在当前版本的k8s中，kube-proxy默认使用的是iptables模式，通过各个node节点上的iptables规则来实现service的负载均衡，但是随着service数量的增大，iptables模式由于线性查找匹配、全量更新等特点，其性能会显著下降。</p>
<p>大勇：</p>
<p>kube-proxy当前实现了三种代理模式：userspace, iptables, ipvs。其中userspace mode是v1.0及之前版本的默认模式，从v1.1版本中开始增加了iptables mode，在v1.2版本中正式替代userspace模式成为默认模式。也就是说kubernetes在v1.2版本之前是默认模式, v1.2版本之后默认模式是iptables。</p>
<p>大勇：</p>
<p>linux 分User space(用户空间)和 Kernel space(内核空间)。 在K8S 1.0 版本之前，kube-proxy 确实是流量代理</p>
<p>大勇：</p>
<p>每次的访问都是 从User space 到Kernel space 再到 User space 里的 kube-proxy</p>
<p>大勇：</p>
<p>显然这种方式会 因为K8S 底层组件的流量瓶颈，会影响整个上层应用生态</p>
<p>大勇：</p>
<p>使用userspace 的资源，比使用kernelspace的资源要昂贵的多， 这里引申一下， 能用TCP的场景，要比用HTTP的 节省资源。其他先不分析， 就工作层级来讲 TCP是工作在 kernelspace ，http是userspace 是应用层协议。</p>
<p>大勇：</p>
<p>第二个阶段，iptables mode, 该模式完全利用内核iptables来实现service的代理和LB, 这是K8s在v1.2及之后版本默认模式</p>
<p>大勇：</p>
<p>kube-proxy 在第二阶段 使用iptables mode 时，已经部署流量代理了，这些都交给 节点机器的iptables 路由表 来流量转换了， 而kube-proxy 只需要从api server 拿到service的策略配置，来实时维护好iptables 即可</p>
<p>大勇：</p>
<p>第二阶段，随着服务越来越多，维护Node上的iptables rules将会非常庞大，性能还会再打折扣</p>
<p>大勇：</p>
<p> 故迎来了 第三阶段ipvs mode.   当下 一般的云都是这种模式</p>
<p>大勇：</p>
<p>在kubernetes 1.8以上的版本中，对于kube-proxy组件增加了除iptables模式和用户模式之外还支持ipvs模式。kube-proxy ipvs 是基于 NAT 实现的，通过ipvs的NAT模式，对访问k8s service的请求进行虚IP到POD IP的转发。</p>
<p>大勇：</p>
<p>kubedns&#x2F;coredns 这个就是K8S的 dns服务器而已，不是流量代理，只提供了一个服务名、主机名等的DNS解析</p>
<p>大勇：</p>
<p>kube-proxy 经过以上阶段，也不是流量代理了，不会是应用访问的流量影响者</p>
<p>大勇：</p>
<p>综合以上分析，现在来看，流量控制代理瓶颈，不会出现在 K8S的service</p>
<p>大勇：</p>
<p>kube-proxy</p>
<p>大勇：</p>
<p>只有可能出现在 ingress-controller</p>
<p>大勇：</p>
<p>实际k8s 官方也提供了 ingress-controller的实现，只是也不太好用，</p>
<p>大勇：</p>
<p>现在基本都用 Nginx  、traefik、Haproxy</p>
<p>大勇：</p>
<p>来说下Nginx 吧， 早期版本部署是通过 DaemonSet 的方式</p>
<p>大勇：</p>
<p>这种方式就是每个节点启用一个 nginx代理</p>
<p>大勇：</p>
<p>但是会把当前节点机器的 80、443 端口占用掉</p>
<p>大勇：</p>
<p>因为Nginx 自身的 service 的暴露类型是 采用的 hostport 方式 类似与nodeprot 但是没负载均衡，就是只能用POD所在的节点机器IP 访问到</p>
<p>大勇：</p>
<p>好在是 DaemonSet方式，反正每个机器都有，用每个节点的IP 都能访问到nginx服务，然后反向代理到 应用的服务</p>
<p>大勇：</p>
<p>后面流量网关也进行了部署升级，先看下华为云的把：</p>
<p>大勇：</p>
<p>可以看到 不用DaemonSet，不用每台都启了， 启动几个pod就行， 自身的 service 也通过LoadBalancer 类型暴露了</p>
<p>大勇：</p>
<p>LoadBalancer 类型是需要负载均衡器来支持了，这种类型，如果采用类似BGP模式，理论上相当于 K8S集群每个节点的都可以成为流量入口。 相当于ingress-controller 作为流量入口，流量网关，它理论上可以做到，K8S的 所有节点都能充当入口。 以达到流量的最大化。</p>
<p>大勇：</p>
<p>今天的分析就到这了，梳理了一个论点，即：ingress-controller 才是流量代理，ingress、service、kube-proxy都不是，理论上可以做到流量的最大化，K8S 从机制上来看，没有流量瓶颈的桎梏。 要有也是 节点机器、其它方面的。</p>
]]></content>
      <categories>
        <category>云计算</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第七期</title>
    <url>/tech-salon-7/</url>
    <content><![CDATA[<p>涛涛：</p>
<p>今天给大家分享的是一个分布式链路追踪工具SkyWalking</p>
<p>涛涛：</p>
<p>介绍这个工具是根据之前space项目优化tps 导致程序中需要频繁记录日志排查影响tps,所以了解了下链路是追踪工具(未投入使用)</p>
<p>涛涛：</p>
<p>提到分布式链路追踪工具大家第一点想到的是zipkin ,对比zipkin,SkyWalking 的优势接下来会逐一介绍</p>
<p>涛涛：</p>
<p>skywalking支持dubbo，SpringCloud，SpringBoot集成，代码无侵入，通信方式采用GRPC，性能较好，实现方式是java探针，支持告警，支持JVM监控，支持全局调用统计等等，功能较完善</p>
<p>涛涛：</p>
<p>skywalking采用字节码增强的技术实现代码无侵入，zipKin代码侵入性比较高<br>skywalking功能比较丰富，报表统计，UI界面更加人性化<br>所以针对目前服务未使用过链路追踪工具,建议使用SkyWalking</p>
<p>涛涛：</p>
<p><img src="/tech-salon-7/arch.png"></p>
<p>上边的是SkyWalking的一个架构图</p>
<p>涛涛：</p>
<p>上面的Agent：负责收集日志数据，并且传递给中间的OAP服务器<br>中间的OAP：负责接收 Agent 发送的 Tracing 和Metric的数据信息，然后进行分析(Analysis Core) ，存储到外部存储器( Storage )，最终提供查询( Query )功能。<br>左面的UI：负责提供web控制台，查看链路，查看各种指标，性能等等。<br>右面Storage：负责数据的存储，支持多种存储类型（根据系统使用量决定存储类型）。</p>
<p>涛涛：</p>
<p>skywalking在性能剖析方面真的是非常强大，提供到基于堆栈的分析结果，能够让运维人员一眼定位到问题。</p>
<p>涛涛：</p>
<p>我们在代码中故意休眠了2秒，看看如何在skywalking中定位这个问题。</p>
<p>涛涛：</p>
<p>在性能剖析模块-&gt;新建任务-&gt;选择服务、填写端点、监控时间，操作如下图：</p>
<p>涛涛：</p>
<p>上图中选择了最大采样数为5，则直接访问5次：<a href="http://localhost:8888/order/list%EF%BC%8C%E7%84%B6%E5%90%8E%E9%80%89%E6%8B%A9%E8%BF%99%E4%B8%AA%E4%BB%BB%E5%8A%A1%E5%B0%86%E4%BC%9A%E5%87%BA%E7%8E%B0%E7%9B%91%E6%8E%A7%E5%88%B0%E7%9A%84%E6%95%B0%E6%8D%AE">http://localhost:8888/order/list，然后选择这个任务将会出现监控到的数据</a></p>
<p>涛涛：</p>
<p>可以看到{GET}&#x2F;order&#x2F;list这个接口上耗费了2秒以上，因此选择这个接口点击分析，可以看到详细的堆栈信息</p>
<p>涛涛：</p>
<p>直接可以定位到代码中睡眠2秒钟</p>
<p>涛涛：</p>
<p>skywalking 监控带有默认的规则 同时还适配了一些钩子（webhooks）。其实就是相当于一个回调，一旦触发了上述规则告警，skywalking则会调用配置的webhook，这样开发者就可以定制一些处理方法，比如发送邮件、微信、钉钉通知运维人员处理。</p>
<p>涛涛：</p>
<p>这就是国产软件的优势</p>
<p>涛涛：</p>
<p>对于代码入侵 skywalking 只是提供了agent 只要是普通的微服务即可，不需要引入什么依赖</p>
<p>涛涛：</p>
<p>只需要启动的时候增加启动命令即可</p>
<p>涛涛：</p>
<p>最后给大家提一个小建议 在我们选型中间件或者设计中间件的时候 尽量减少代码入侵行,可以大幅度的减少二次开发, 在满足其他中间件功能的同时 考虑新组件的兼容性、易用性</p>
]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第八期-AOP</title>
    <url>/tech-salon-8-aop/</url>
    <content><![CDATA[<p>.now：</p>
<p>hello大家好，这两天考虑了很久要分享什么，最后决定就从最近身边发现的可优化的小方案入手吧。今天给大家分享的是基于Spring AOP实现某些重要信息的加解密跟脱敏。</p>
<p>.now：</p>
<p>AOP面向切面编程，通过预编译方式和运行期间动态代理实现程序功能统一维护，这些原理网上一搜一大把，这次就不多搬那些原理过来了，主要就讲讲应用于加解密脱敏方面。</p>
<p>.now：</p>
<p>因为日常业务开发中，经常有碰到一些比较重要的用户信息，比如手机号，身份证号，邮箱，我们就直接大咧咧地明文保存到数据库里，万一数据库被黑，这些信息就全暴露了，用户就可能遭受到短信轰炸，身份证号被冒用等恶劣行为，数据安全很重要，所以就有必要进行加密后保存，查询时解密返回，页面脱敏显示比如手机号只展示前三后四，如：18812349876 置换为 188****9876。</p>
<p>.now：</p>
<p>之前看到有些项目返回时是手动调用工具类替换后再返回，如下这样：</p>
<p><img src="/tech-salon-8-aop/1.jpeg"></p>
<p>.now：</p>
<p>如果多个业务需要做加解密脱敏处理，每次新增都自己手动调用加密方法，每次查询都自己手动进行解密或者脱敏处理，就会多出很多重复代码，代码就显得很笨重。特别是后续万一修改了公共方法，就有可能要对应所有调用它的地方。</p>
<p>.now：</p>
<p>本次介绍的就是基于Spring AOP 注解的方式，实现加解密脱敏，对service层的业务代码无侵入。</p>
<p>.now：</p>
<p>涉及的代码如下，DecryptField，DesensitizedField，EncryptField三个注解用于标记需要处理的字段，NeedDecrypt，NeedDesensitized，NeedEncrypt三个注解用于标记切入点，DecryptAspect，DesensitizedAspect，EncryptAspect三个切面配置类，具体实现真正的加解密脱敏。</p>
<p><img src="/tech-salon-8-aop/2.jpeg"></p>
<p>.now：</p>
<p>这里先提一下Spring AOP 中的通知类型，Spring AOP 中有5种通知类型分别如下：</p>
<p><img src="/tech-salon-8-aop/3.jpeg"></p>
<p>.now：</p>
<p>这次介绍的主要就用到了Before和AfterReturning，在请求过来时，新增方法调用前进行加密配置，在查询返回后进行解密脱敏</p>
<p>.now：</p>
<p>接下来具体使用看看，首先看一下加密的切面类EncryptAspect，切面里定义了切入点是指定的注解@NeedEncrypt，在before方法里进行具体的加密处理，先找到被注解@EncryptField标记的字段，然后对该字段的值进行加密处理。里面具体的加密方式就可以自己自定义了，看自己需求。</p>
<p><img src="/tech-salon-8-aop/4.jpeg"></p>
<p><img src="/tech-salon-8-aop/5.jpeg"></p>
<p>.now：</p>
<p>我们只需要在方法添加【@NeedEncrypt】注解</p>
<p>.now：</p>
<p>在需要加密字段添加注解【@EncryptField】</p>
<p>.now：</p>
<p>调用接口，查看数据库，就可以看到对应的字段加密成功</p>
<p>.now：</p>
<p><img src="/tech-salon-8-aop/6.jpeg"></p>
<p><img src="/tech-salon-8-aop/7.jpeg"></p>
<p><img src="/tech-salon-8-aop/8.jpeg"></p>
<p><img src="/tech-salon-8-aop/9.jpeg"></p>
<p>同样的，只需要在方法添加【@NeedDecrypt】注解，</p>
<p>.now：</p>
<p>需要解密字段添加注解【@DecryptField】</p>
<p>.now：</p>
<p>调用接口，就可以看到对应的字段解密成功</p>
<p>.now：</p>
<p><img src="/tech-salon-8-aop/10.jpeg"></p>
<p><img src="/tech-salon-8-aop/11.jpeg"></p>
<p><img src="/tech-salon-8-aop/12.jpeg"></p>
<p><img src="/tech-salon-8-aop/13.jpeg"></p>
<p>脱敏也是如此，在方法添加【@NeedDesensitized】注解，需要脱敏的字段添加注解【@DesensitizedField】</p>
<p>.now：</p>
<p>最后调用接口可以看到返回脱敏成功</p>
<p><img src="/tech-salon-8-aop/14.jpeg"></p>
<p><img src="/tech-salon-8-aop/15.jpeg"></p>
<p><img src="/tech-salon-8-aop/16.jpeg"></p>
<p>.now：</p>
<p>由于都是一样的原理，就没有对解密跟脱敏的切面类做截图说明了，具体的规则可以根据需求自由发挥下</p>
<p>.now：</p>
<p>这样，我们就在不修改service层源代码的前提下，去为系统的重要用户信息添加了加解密脱敏通用功能，提高程序的可重用性，同时提高了开发的效率。</p>
<p>.now：</p>
<p>我的介绍就到这里了，介绍的比较简单，但我觉得在日常开发中还是比较实用的，希望能对大家有所帮助。</p>
]]></content>
      <categories>
        <category>应用</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第八期-低代码</title>
    <url>/tech-salon-8-lowcode/</url>
    <content><![CDATA[<p>渲染：</p>
<p>给大家分享下低代码。也是最近我看到的一篇从实现上讲低代码的好文，整理了下，加上了一些自己的看法，分享给大家</p>
<p>渲染：</p>
<p>那么什么是低代码呢？低代码是指通过少量代码就可以快速生成应用程序的开发平台。.通过可视化进行应用程序开发的方法，使具有不同经验水平的开发人员可以通过图形化的用户界面，使用拖拽组件和模型驱动的逻辑来创建网页和移动应用程序。</p>
<p>渲染：</p>
<p>也就是说相对明确的一点是 可视化 是低代码唯一不可缺少的功能</p>
<p>渲染：</p>
<p>那么对于实现可视化编辑来说，必要条件又是什么呢？</p>
<p>渲染：</p>
<p>是声明式</p>
<p>渲染：</p>
<p>与声明式相对的，还有一种编码模式叫命令式。下面先来解释下什么是声明式、什么是命令式？</p>
<p>渲染：</p>
<p>比如我们要实现一个蓝色的方块，就拿我所用的前端的语言来说吧，HTML、CSS就是声明式的，来实现它，我只需要这么写</p>
<p>渲染：</p>
<p><img src="/tech-salon-8-lowcode/1.jpeg"></p>
<p>渲染：</p>
<p>而如果换成使用命令式的javascript来实现，则可能会这么写</p>
<p>渲染：</p>
<p><img src="/tech-salon-8-lowcode/2.jpeg"></p>
<p>渲染：</p>
<p>两种方式最终展现效果是一样的，但我们可以看到，这两种代码在实现思路上有本质区别</p>
<p>渲染：</p>
<p>声明式直接描述最终效果，不关心如何实现。<br>而命令式则关注如何实现，明确怎么一步步达到这个效果。</p>
<p>渲染：</p>
<p>回到可视化编辑器的角度看，它们的最大区别是：声明式可以直接从展现结果反向推导回源码，而命令式则无法做到反向推导</p>
<p>渲染：</p>
<p>反向推导是编辑器必备功能，比如编辑器里的常见操作是点选这个区块，然后修改它的颜色，在这两种代码中如何实现？</p>
<p>渲染：</p>
<p>如果是声明式的 HTML CSS，可以直接改style的background值，而基于 Canvas 的命令式代码则无法实现这个功能，因为无法从展现找到实现它的代码，因为命令式代码实现同样效果的可能方式是无数的，除了前面的示例，下面这段代码也可以实现一样的效果：画一条长100，粗100的线段，在最终视觉呈现上也可以看做是一个矩形。</p>
<p>渲染：</p>
<p><img src="/tech-salon-8-lowcode/3.jpeg"></p>
<p>渲染：</p>
<p>因此我可以简单得到一下结论：命令式代码无法实现可视化编辑，而可视化编辑是低代码唯一不可少的功能，所以所有低代码平台必然只能采用声明式代码，这也是为什么所有低代码平台都会有内置的DSL。</p>
<p>渲染：</p>
<p>这些声明式语言有以下优点：<br>1、容易上手，因为描述的是结果，语法可以做得简单，非研发也能快速上手 HTML 及 SQL。<br>2、支持可视化编辑，微软的 HTML 可视化编辑 FrontPage 在 1995 年就有了，现在各种 BI 软件可以认为是 SQL 的可视化编辑。<br>3、容易优化性能，无论是浏览器还是数据库都在不断优化，比如可以自动改成并行执行，这是命令式语言无法自动实现的。<br>4、容易移植，容易向下兼容，现在的浏览器能轻松渲染 30 年前的HTML，而现在的编译器没法编译 30 年前的浏览器引擎代码。</p>
<p>渲染：</p>
<p>而这些语言的缺点是：<br>1、只适合特定领域，命令式的语言比如 JavaScript 可以用在各种领域，但 HTML CSS 只适合渲染文档及界面，SQL 只适合做查询。<br>2、灵活性差，比如 SQL 虽然内置了很多函数，但想只靠它实现业务是远远不够的<br>3、调试困难，遇到问题时如缺乏工具会难以排查，如果你在Firefox出现前开发过页面就会知道，由于IE6没有开发工具，编写复杂页面体验很差，遇到问题要看很久代码才发现是某个标签没闭合或者 CSS 类名写错了。</p>
<p>渲染：</p>
<p>4、强依赖运行环境，因为声明式只描述结果而不关注实现，因此强依赖运行环境，但这也带来了以下问题：</p>
<ul>
<li>1)、功能取决于运行环境，比如浏览器对 CSS 的支持程度决定某个属性是否有人用，虽然出现了新的CSS提案，但 Firefox 和 Safari 都不支持，而且上手成本太高，预计以后也不会流行。</li>
<li>2)、性能取决于运行环境，比如同一个 SQL 在不同数据库下性能有很大区别。</li>
<li>3)、对使用者是黑盒，使用者难以知道最终实现，就像很少人知道数据库及浏览器的实现细节，完全当成黑盒来使用，一旦遇到性能问题可能就不知所了。</li>
<li>4)、技术锁定，因为即便是最开放的 HTML 也无法解决，很多年前许多网站只支持 IE，现在又变成了只支持 Chrome，微软和 Opera 在挣扎了很多年后也干脆直接转向用 Chromium。同样的即便有 SQL 标准，现在用的 Oracle&#x2F;SQL Server 应用也没法轻松迁移到 Postgres&#x2F;MySQL 上。低代码行业未来也一样，即便出了标准也解决不了锁定问题，更有可能是像小程序标准那样发展缓慢，功能远落后于微信。</li>
</ul>
<p>渲染：</p>
<p>因为低代码就是一种声明式编程，所以这些声明式优缺点，其实就是低代码的优缺点。</p>
<p>渲染：</p>
<p>了解了声明式，下面来说说低代码的实现方案</p>
<p>渲染：</p>
<p>以前端的实现来说，其核心是界面渲染。前面提到前端 HTML CSS 可以看成一种描述界面的低代码 DSL，因此前端界面实现低代码会比较容易，只需要对 HTML CSS 进行更进一步封装，定义JSON schema。比如用类似如下的方式来描述页面：</p>
<p>渲染：</p>
<p><img src="/tech-salon-8-lowcode/4.jpeg"></p>
<p>渲染：</p>
<p>这里大家几乎全都使用 JSON主要是两方面原因：</p>
<ol>
<li>低代码平台编辑器几乎都是基于 Web 实现，JavaScript 可以方便操作 JSON。</li>
<li>JSON 可以支持双向编辑，它的读取和写入是一一对应的。</li>
</ol>
<p>渲染：</p>
<p>因此界面呈现上的低代码实现起来，我们只需要丰富物料库，通过拖拽这些物料拼出想要的东西后生成json描述即可。</p>
<p>渲染：</p>
<p>再来说说交互逻辑的实现。</p>
<p>渲染：</p>
<p>前面说到前端界面低代码是比较容易，但交互及逻辑处理却很难低代码化，目前常见实现有三种方案：</p>
<p>1、使用图形化编程；</p>
<p>2、固化交互行为；</p>
<p>3、使用 JavaScript；</p>
<p>渲染：</p>
<p>先说第一种图形化编程，这是非常自然的想法，既然低代码的关键是可视化，那直接使用图形化的方式编程不就行了？</p>
<p>渲染：</p>
<p>但我们发现这么做局限性很大，本质的原因是命令式的代码无法可视化。即便我们将循环、分支判断、或操作符等等这些抽象为一块块的积木，我们也难以像拼接积木一样得到我们想要的东西，因为积木拼接这种方式只适合用来实现简单的逻辑，对于复杂的交互逻辑非常难以实现。</p>
<p>渲染：</p>
<p>再来说固化交互行为，如果是面向特定领域，低代码平台可以先将这个领域难以图形化的逻辑预置好，让使用者只需做简单的处理，使用的时候只需要调整参数就行。当然这个方案最大的缺点是灵活性很低。</p>
<p>渲染：</p>
<p>因此要实现更灵活的控制，还是得支持第三个方案：JavaScript，目前很多低代码平台只在界面编辑提供可视化编辑，一旦涉及到交互就得写 JavaScript，但该方案脱离了低代码范畴、不是低代码。</p>
<p>渲染：</p>
<p>下面来看个实例，以阿里的datav中的蓝图编辑器为例，它就是同时支持了3种方案进行互补：</p>
<p>渲染：</p>
<p><img src="/tech-salon-8-lowcode/5.jpeg"></p>
<p>渲染：</p>
<p>一些简单逻辑用户可以自己通过蓝图编辑器去添加然后串并联这些节点来实现。对于使用场景较多的一些较复杂的行为可以内置固话。而对于较复杂的逻辑用户可以自己通过js处理。</p>
<p>渲染：</p>
<p>最后总结一下吧。其实可以看出在定制化较强的业务中低代码可以说是毫无用武之地，但基于特定领域或方向的低代码平台还是很有意义的。将其作为一类工具，趁手时就用。</p>
]]></content>
      <categories>
        <category>应用</category>
      </categories>
  </entry>
  <entry>
    <title>技术沙龙第九期</title>
    <url>/tech-salon-9/</url>
    <content><![CDATA[<p>绚磊：</p>
<p>做一个简单的开发过程中遇到的使用Mybatis plus更新统计数据，存在线程安全问题，也是分布式事务问题的分享吧。[呲牙]</p>
<p>绚磊：</p>
<p>前提：<br>在我们开发中常常会遇到统计需求。例如，针对一个文件的查看次数的统计。统计的逻辑，挺简单就是数据库数据自增。<br>在我们的系统中，现在大部分使用的ORM框架是Mybatis plus。Mybatis plus的特点是，将对数据库操作的sql封装成java<br>程序员熟悉的class对象方法。<br>通过Mybatis plus来对 数据库自增往往会有如下的操作。</p>
<p>绚磊：</p>
<p>假设数据存在t_file表，创建File对象如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Data</span><br><span class="line">@TableName(&quot;t_file&quot;)</span><br><span class="line">@AllArgsConstructor</span><br><span class="line">@NoArgsConstructor</span><br><span class="line">public class File &#123;</span><br><span class="line"></span><br><span class="line">    private Integer id;</span><br><span class="line"></span><br><span class="line">    private String filename;</span><br><span class="line"></span><br><span class="line">    private Integer times;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>绚磊：</p>
<p>生成Mybatis plus的 mapper 接口类</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Mapper</span><br><span class="line">public interface FileMapper extends BaseMapper&lt;File&gt; &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>绚磊：</p>
<p>生成service层对象类</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Service</span><br><span class="line">public class FileServiceImpl extends ServiceImpl&lt;FileMapper, File&gt; implements FileService &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public Integer increase()&#123;</span><br><span class="line">        //获取 文件 在数据库存储的信息</span><br><span class="line">        File file = this.baseMapper.selectById(1);</span><br><span class="line">        Integer times = file.getTimes();</span><br><span class="line">        //对文件的获取次数进行 增加 1 次</span><br><span class="line">        file.setTimes(file.getTimes() 1);</span><br><span class="line">        //更新数据库数据</span><br><span class="line">        this.baseMapper.updateById(file);</span><br><span class="line">        //返回增加后的获取次数</span><br><span class="line">        return file.getTimes();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>绚磊：</p>
<p>咋一看可能没发现问题，通过简单的单个请求测试也会发现功能正常。但实际上，大量请求并发执行时，会造成统计数据小于实际请求数据量。</p>
<p>绚磊：</p>
<p>下面进行下问题复现吧，我这边新建了一个测试项目。</p>
<p>绚磊：</p>
<p><img src="/tech-salon-9/1.jpeg"></p>
<p><img src="/tech-salon-9/9.jpeg"></p>
<p>启动项目，打上断点 。断点挂在 Thread线程上</p>
<p>绚磊：</p>
<p><img src="/tech-salon-9/3.jpeg"></p>
<p>表里 初始数据</p>
<p>绚磊：</p>
<p><img src="/tech-salon-9/4.jpeg"></p>
<p>从浏览器发送 两次请求</p>
<p>绚磊：</p>
<p><img src="/tech-salon-9/5.jpeg"></p>
<p>可以看到 两个请求进来了。</p>
<p>绚磊：</p>
<p><img src="/tech-salon-9/6.jpeg"></p>
<p><img src="/tech-salon-9/7.jpeg"></p>
<p>后续的结果很明显了，http-nio-7777-exec-1和http-nio-7777-exec-2获取到的 id&#x3D;1 的times 都是0</p>
<p><img src="/tech-salon-9/8.jpeg"></p>
<p>绚磊：</p>
<p>后续进行了 times&#x3D; times 1，然后更新表后，会发现虽然进行了两次请求，但实际数据库表里的数据为1次。</p>
<p>绚磊：</p>
<p>整理下逻辑：</p>
<p>绚磊：</p>
<p>1.当A请求，请求到服务器，开启线程1执行 File file &#x3D; this.baseMapper.selectById(1);语句完成后，进入阻塞，此时file.getTimes() 的值为 0。<br>2.此时B请求，请求到服务器，开启线程2，也执行到File file &#x3D; this.baseMapper.selectById(1);语句完成后，进入阻塞，此时file.getTimes() 的值也为 0。<br>3.此时可以明显发现问题出现了<br>4.后续A请求，B请求无论哪个先执行完，更新到数据库中的file.times 的 值 只会是1 【但请求数是2】</p>
<p>绚磊：</p>
<p>接下详细分析出现问题的原因：</p>
<p>绚磊：</p>
<p>这是一个很明显的线程安全问题，多个线程操作同一个资源。此外，由于这是对数据库记录的操作，这也涉及到数据库的事务，事务隔离级别，我们发现上面的代码，没有使用到数据库事务，<br>那么是否有可能加上事务就会解决问题了呢，这里先把结论说出来吧，实际上加上事务也是会存在问题的。总结下，上面线程安全问题的原因是，多线程通过了一系列非原子性的操作，修改了<br>相同的资源。接下来我们分下下数据库层面，默认情况下，MySQL。默认情况下，MySQL 的 innodb数据引擎的事务隔离级别是 RR，也就是Repeatable Read，<br>rr的原理是 MVCC多版本并发控制 悲观锁 乐观锁。从数据库事务角度分析上面问题，其实是事务A通过select * from file 获取到的对象 file id &#x3D;1; name&#x3D;”file”,times&#x3D;1【这是一个快照snapshot】。<br>事务B 获取到的也是一个快照，获取到的也是 file.times &#x3D; 1。所以也最终导致了最后 统计数据 小于 实际请求数量。</p>
<p>绚磊：</p>
<p>说了这么多，还没说解决方法。接下去说说几种解决方法，根据问题分析得出造成问题的原因有两点：1.多线程 2.非原子性操作。<br>那么可以从两个维度来解决问题：1.对多线程操作的方法加锁 2.将操作修改成 原子性操作。<br>先说说第一个维度的几个方案，加锁：分别从数据库，java单个应用，分布式多个应用做出方案。<br>首先由于这个场景是针对数据库的数据做统计自增操作，上面也分析了，也有数据库事务隔离级别的影响，那么是否能考虑将事务隔离级别调整下，原本默认是REPEATABLE_READ，调整成SERIALIZABLE。然后测试下看。</p>
<p>绚磊：</p>
<p>修改 FileServiceImpl类</p>
<p><img src="/tech-salon-9/10.jpeg"></p>
<p>绚磊：</p>
<p>测试结果，两个请求获取到的仍然是0</p>
<p><img src="/tech-salon-9/11.jpeg"></p>
<p>绚磊：</p>
<p>第一个请求，update数据库成功，第二个请求，报死锁错误。</p>
<p><img src="/tech-salon-9/12.jpeg"></p>
<p>绚磊：</p>
<p>这里可能涉及到 mysql 锁机制，后续我再测试分析下。</p>
<p>绚磊：</p>
<p>总结此方法：能实现数据的安全。但是只能有成功一个请求，实际编码中不推荐此方法。</p>
<p>绚磊：</p>
<p>第二个方案：加synchronized 或者 ReentrantLock 来解决，多线程问题。</p>
<p>绚磊：</p>
<p>修改Service类</p>
<p><img src="/tech-salon-9/13.jpeg"></p>
<p>绚磊：</p>
<p>测试打上断点，两个请求并发执行，会发现第一个请求进入了断点，第二个请求没有进入断点，但是浏览器上第二个请求阻塞了。</p>
<p><img src="/tech-salon-9/14.jpeg"></p>
<p>绚磊：</p>
<p>最后的执行结果：正确</p>
<p><img src="/tech-salon-9/15.jpeg"></p>
<p><img src="/tech-salon-9/16.jpeg"></p>
<p>绚磊：</p>
<p>但是 此方法：仅适用于单节点应用，现在基本上都是前后端分离的分布式系统。可以想象下，启动多个节点服务，此时两个请求分别打在两个服务上。实际上并不存在，线程竞争。此时问题变成了，分布式事务问题。</p>
<p>绚磊：</p>
<p>所以 就有个第三个方案：方案加分布式锁，例如利用redis加分布式锁。这里就不演示了，此方法肯定是可行的，但是增加了逻辑上的复杂度，带来了分布式锁的加锁和解锁问题，还涉及到等待请求的公平与非公平。</p>
<p>绚磊：</p>
<p>最后 思考下，是否还有其他方案呢，其实从原子性操作角度分析，由于我们在进行自增操作的时候是先获取了数据库的值，然后再进行java逻辑层的自增，再更新到mysql数据库中。这样获取值和更新不是一个原子操作。通过自定义 sql 。 不使用mybatis plus提供的方法，来做操作 。这样就能保证自增的原子性了。</p>
<p>绚磊：</p>
<p><img src="/tech-salon-9/17.jpeg"></p>
<p>修改方法为：</p>
<p><img src="/tech-salon-9/18.jpeg"></p>
<p>绚磊：</p>
<p>测试结果：</p>
<p><img src="/tech-salon-9/19.jpeg"></p>
<p><img src="/tech-salon-9/20.jpeg"></p>
<p>绚磊：</p>
<p>分析：有同学可能会问，两个事务下线程A和线程B同时阻塞在，update操作；然后线程A先commit，接着线程B再commit，是否会有问题。</p>
<p>由于update 在本身就是X锁排他锁，线程A，线程B并发执行，只有一个线程能获取到锁。假设线程B获取到锁，那么线程A就进入等待。</p>
<p>线程B执行完 update 语句后，但是未提交，生成一条redo log；</p>
<p>线程A执行完 update 语句后，但是未提交，生成一条redo log；</p>
<p>通过sql可以查询到获取锁的sql</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT * FROM performance_schema.events_statements_history WHERE thread_id IN(</span><br><span class="line">SELECT b.`THREAD_ID` FROM sys.`innodb_lock_waits` AS a , performance_schema.threads AS b</span><br><span class="line">WHERE a.`blocking_pid` = b.`PROCESSLIST_ID`)</span><br></pre></td></tr></table></figure>

<p><img src="/tech-salon-9/21.jpeg"></p>
<p>绚磊：</p>
<ul>
<li><p>此时执行select times from t_vc where id&#x3D;5 for update;会阻塞。因为在采用INNODB的MySQL中，更新操作默认会加行级锁.所以窗口二这里会卡住。</p>
</li>
<li><p>然后线程A commit，释放锁；线程Bcommit，释放锁。</p>
</li>
</ul>
<p>绚磊：</p>
<p>从执行日志看：</p>
<p><img src="/tech-salon-9/22.jpeg"></p>
<p>绚磊：</p>
<p>先后执行了更新操作。</p>
<p>绚磊：</p>
<p>最后总结下： 通过自定义sql update table set a&#x3D;a 1 where id&#x3D;1 来解决 统计数据的自增问题，最为合适</p>
]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
  </entry>
  <entry>
    <title>在自建Kubernetes集群中创建LoadBalancer</title>
    <url>/use-loadbalancer-in-self-built-k8s-cluster/</url>
    <content><![CDATA[<h2 id="Service的分类"><a href="#Service的分类" class="headerlink" title="Service的分类"></a>Service的分类</h2><p>在Kubernetes中，经常用到的有这些类型的Service：ClusterIP，NodePort，LoadBalancer</p>
<h3 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a>ClusterIP</h3><p>ClusterIP主要用来集群内应用的互相访问，比如你有个数据库服务暴露了一个名为db-service的Service ，那么你的应用可以使用如下的方式连接：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">&lt;service&gt;.&lt;namespace&gt;:&lt;service-port&gt;</span></span><br><span class="line">spring.datasource.url=jdbc:mysql://db-service.default:3306/test?characterEncoding=utf8&amp;useSSL=true</span><br></pre></td></tr></table></figure>

<h3 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h3><p>NodePort为我们给集群外暴露服务提供了一种方式。NodePort顾名思义，这种Service在每个Kubernetes的节点暴露一个Port(默认为30000-32767)。</p>
<p>注意，这里是每个节点都会暴露一个Port。</p>
<p>所以，你可以在集群外所用任意节点的IP+Port连接到集群内的服务。</p>
<p>一般情况下，这种方式只会在测试阶段使用，所以不建议在生产环境中使用NodePort方式对外暴露服务，有如下缺点：</p>
<ul>
<li>端口范围有限制，只能是30000-32767 </li>
<li>直接对外暴露云主机的IP并不安全 </li>
<li>由于客户端固定使用其中一个云主机实例的IP连接，若该云主机故障，则整个多副本应用都不能对外提供服务，除非自己在客户端层面实现故障检测机制。 </li>
<li>一些厂商的云主机的公有IP会变，如AWS的EC2，关闭后再重启，可能导致IP不一致</li>
</ul>
<h3 id="LoadBalancer"><a href="#LoadBalancer" class="headerlink" title="LoadBalancer"></a>LoadBalancer</h3><p>为解决以上问题，传统云环境下，我们都会用云厂商的LoadBalancer来实现负载均衡和服务的高可用。如：AWS的ELB，阿里云的SLB等。顾名思义，LoadBalancer可以将流量导入后端多个实例，实现请求服务的负载均衡。</p>
<p>Kubernetes作为云原生时代的基础设施，必然会用到一些公有云资源，如LoadBalancer、云盘等，来适应云环境下的各种场景（当然私有云环境也会有与公有云相对应的资源或服务，这里不在讨论范围）。</p>
<p>Kubernetes中的LoadBalancer类型的Service为我们创建云上LoadBalancer的服务提供了便利，你可以使用如下配置：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-service-lb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx-net</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>
<p>如上，我在我自己的集群上使用了名为nginx-service-lb的LoadBalancer类型的Service来暴露后端的Nginx服务。不过会遇到如下错误状态:<br><img src="/use-loadbalancer-in-self-built-k8s-cluster/pending-service.png"></p>
<p>可以看到，nginx-service-lb会一直处于pending状态。不过，仔细一想也会知道，我的自建集群并未告知Kubernetes相关云服务的账号信息，怎么会为我们创建对应的云资源呢。</p>
<h2 id="Cloud-Provider与Cloud-Controller-Manager"><a href="#Cloud-Provider与Cloud-Controller-Manager" class="headerlink" title="Cloud Provider与Cloud Controller Manager"></a>Cloud Provider与Cloud Controller Manager</h2><p>其实，在Kubernetes创建公有云资源，还需要告诉Kubernetes所处于的云环境，Kubernetes把创建云资源的事交给了叫做Cloud Provider或Cloud Controller Manager的组件来做。</p>
<p>以上提及的组件存在两种形式：</p>
<ul>
<li>in-tree方式：Kubernetes v1.6之前的做法，也就是Cloud Provider。该方式各Providers的代码集成在Kubernetes主代码中，各核心组件通过–cloud-provider参数来启动对应的Provider（如AWS&#x2F;Azure&#x2F;GCE等），由于和Kubernetes代码耦合，当前这种方法已被官方弃用。</li>
<li>out-of-tree方式：需要在集群中安装一个独立组件，也叫Cloud Controller Manager，并通过设置–cloud-provider&#x3D;external的方式来告诉各核心组件通过外部组件来完成云资源的创建和管理，如AWS的 <a href="https://github.com/kubernetes/cloud-provider-aws">cloud-provider-aws</a> ，阿里云的 <a href="https://github.com/kubernetes/cloud-provider-alibaba-cloud">cloud-provider-alibaba-cloud</a> ，它们均被托管在Kubernetes官方仓库下。这种方式独立于Kubernetes核心代码的开发、构建和发布，是官方推荐的做法。</li>
</ul>
<h3 id="In-tree方式的Cloud-Provider初始化分析"><a href="#In-tree方式的Cloud-Provider初始化分析" class="headerlink" title="In-tree方式的Cloud Provider初始化分析"></a>In-tree方式的Cloud Provider初始化分析</h3><blockquote>
<p>虽说已弃用，但又不是不能用</p>
</blockquote>
<p>现在，我在AWS的EC2实例上自建了Kubernetes集群，并在初始化集群的时候设置–cloud-provider&#x3D;aws参数（in-tree方式，生产环境并不建议用，此处做实验用）来告诉Kubernetes初始化AWS的Provider:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 你可以使用kubeadm init --config=如下配置来初始化集群</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta3</span></span><br><span class="line"><span class="attr">bootstrapTokens:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">groups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system:bootstrappers:kubeadm:default-node-token</span></span><br><span class="line">  <span class="attr">token:</span> <span class="string">abcdef.0123456789abcdef</span></span><br><span class="line">  <span class="attr">ttl:</span> <span class="string">24h0m0s</span></span><br><span class="line">  <span class="attr">usages:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">signing</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">authentication</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">InitConfiguration</span></span><br><span class="line"><span class="attr">localAPIEndpoint:</span></span><br><span class="line">  <span class="attr">advertiseAddress:</span> <span class="number">10.0</span><span class="number">.0</span><span class="number">.216</span></span><br><span class="line">  <span class="attr">bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line">  <span class="attr">criSocket:</span> <span class="string">/var/run/dockershim.sock</span></span><br><span class="line">  <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ip-10-0-0-216.ap-east-1.compute.internal</span></span><br><span class="line">  <span class="attr">taints:</span> <span class="literal">null</span></span><br><span class="line">  <span class="attr">kubeletExtraArgs:</span></span><br><span class="line">    <span class="attr">cloud-provider:</span> <span class="string">&quot;aws&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line">  <span class="attr">timeoutForControlPlane:</span> <span class="string">4m0s</span></span><br><span class="line">  <span class="attr">extraArgs:</span></span><br><span class="line">    <span class="attr">cloud-provider:</span> <span class="string">&quot;aws&quot;</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta3</span></span><br><span class="line"><span class="attr">certificatesDir:</span> <span class="string">/etc/kubernetes/pki</span></span><br><span class="line"><span class="attr">clusterName:</span> <span class="string">kubernetes</span></span><br><span class="line"><span class="attr">controllerManager:</span></span><br><span class="line">  <span class="attr">extraArgs:</span></span><br><span class="line">    <span class="attr">cloud-provider:</span> <span class="string">&quot;aws&quot;</span></span><br><span class="line"><span class="attr">dns:</span> &#123;&#125;</span><br><span class="line"><span class="attr">etcd:</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">dataDir:</span> <span class="string">/var/lib/etcd</span></span><br><span class="line"><span class="attr">imageRepository:</span> <span class="string">k8s.gcr.io</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterConfiguration</span></span><br><span class="line"><span class="attr">kubernetesVersion:</span> <span class="number">1.23</span><span class="number">.0</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="attr">dnsDomain:</span> <span class="string">cluster.local</span></span><br><span class="line">  <span class="attr">serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span><span class="string">/12</span></span><br><span class="line">  <span class="attr">podSubnet:</span> <span class="number">10.244</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line"><span class="attr">scheduler:</span> &#123;&#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeletConfiguration</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubelet.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">cgroupDriver:</span> <span class="string">cgroupfs</span></span><br></pre></td></tr></table></figure>

<p>我们可以在Kubernetes的代码中(以v1.23.3为例)看到初始化Cloud Provider的相关逻辑：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">createCloudProvider</span><span class="params">(cloudProvider <span class="type">string</span>, externalCloudVolumePlugin <span class="type">string</span>, cloudConfigFile <span class="type">string</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                         allowUntaggedCloud <span class="type">bool</span>, sharedInformers informers.SharedInformerFactory)</span></span> (cloudprovider.Interface, ControllerLoopMode, <span class="type">error</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> cloud cloudprovider.Interface</span><br><span class="line">    <span class="keyword">var</span> loopMode ControllerLoopMode</span><br><span class="line">    <span class="keyword">var</span> err <span class="type">error</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> utilfeature.DefaultFeatureGate.Enabled(features.DisableCloudProviders) &amp;&amp; cloudprovider.IsDeprecatedInternal(cloudProvider) &#123;</span><br><span class="line">        cloudprovider.DisableWarningForProvider(cloudProvider)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, ExternalLoops, fmt.Errorf(</span><br><span class="line">            <span class="string">&quot;cloud provider %q was specified, but built-in cloud providers are disabled. Please set --cloud-provider=external and migrate to an external cloud provider&quot;</span>,</span><br><span class="line">            cloudProvider)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 判断是否是external参数</span></span><br><span class="line">    <span class="keyword">if</span> cloudprovider.IsExternal(cloudProvider) &#123;</span><br><span class="line">        loopMode = ExternalLoops</span><br><span class="line">        <span class="keyword">if</span> externalCloudVolumePlugin == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">            <span class="comment">// externalCloudVolumePlugin is temporary until we split all cloud providers out.</span></span><br><span class="line">            <span class="comment">// So we just tell the caller that we need to run ExternalLoops without any cloud provider.</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span>, loopMode, <span class="literal">nil</span></span><br><span class="line">        &#125;</span><br><span class="line">        cloud, err = cloudprovider.InitCloudProvider(externalCloudVolumePlugin, cloudConfigFile)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 输出弃用信息</span></span><br><span class="line">        cloudprovider.DeprecationWarningForProvider(cloudProvider)</span><br><span class="line">        </span><br><span class="line">        loopMode = IncludeCloudLoops</span><br><span class="line">        <span class="comment">// 初始化相应的云厂商provider</span></span><br><span class="line">        cloud, err = cloudprovider.InitCloudProvider(cloudProvider, cloudConfigFile)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, loopMode, fmt.Errorf(<span class="string">&quot;cloud provider could not be initialized: %v&quot;</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> cloud != <span class="literal">nil</span> &amp;&amp; !cloud.HasClusterID() &#123;</span><br><span class="line">        <span class="keyword">if</span> allowUntaggedCloud &#123;</span><br><span class="line">            klog.Warning(<span class="string">&quot;detected a cluster without a ClusterID.  A ClusterID will be required in the future.  Please tag your cluster to avoid any future issues&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span>, loopMode, fmt.Errorf(<span class="string">&quot;no ClusterID Found.  A ClusterID is required for the cloud provider to function properly.  This check can be bypassed by setting the allow-untagged-cloud option&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 设置Informoer</span></span><br><span class="line">    <span class="keyword">if</span> informerUserCloud, ok := cloud.(cloudprovider.InformerUser); ok &#123;</span><br><span class="line">        informerUserCloud.SetInformers(sharedInformers)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> cloud, loopMode, err</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="创建LoadBalancer"><a href="#创建LoadBalancer" class="headerlink" title="创建LoadBalancer"></a>创建LoadBalancer</h2><p>现在我们可以在集群中创建Nginx的Deployment和Service:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">hello</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="comment"># ELB&#x27;s port</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">hello</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">hello</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure>

<p>你可以在的控制台上看到，一个LoadBalancer资源被自动创建了，且负载均衡的80端口对应的后端实例端口为31977。<br><img src="/use-loadbalancer-in-self-built-k8s-cluster/ec2-elb.png"></p>
<p>我们再来看下刚才创建的LoadBalancer Service：<br><img src="/use-loadbalancer-in-self-built-k8s-cluster/get-service.png"></p>
<p><img src="/use-loadbalancer-in-self-built-k8s-cluster/des-service.png"></p>
<p>和NodePort的Service类似，LoadBalancer类型的Service也会有一个NodePort端口暴露，这块的逻辑对于这两种Service是一样的，也就是kube-proxy在每个节点开放了31977这个端口，这里不在赘述。</p>
<p>登录浏览器，你可以使用External-IP访问到Nginx主页面。</p>
<p>以上，我们通过创建LoadBalancer类型的Service来暴露我们集群内部的服务。创建LoadBalancer时Kubernetes主要为我们做了这些事情：</p>
<ul>
<li>在所有节点上开放一个相同的端口，并做流量的负载均衡（由kube-proxy完成）</li>
<li>自动为我们创建云上LoadBalancer资源，并且完成对应的实例端口映射（由service-controller完成）</li>
</ul>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>Cloud Provider中的service-controller的新建代码:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// New returns a new service controller to keep cloud provider service resources</span></span><br><span class="line"><span class="comment">// (like load balancers) in sync with the registry.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">	cloud cloudprovider.Interface,</span></span></span><br><span class="line"><span class="params"><span class="function">	kubeClient clientset.Interface,</span></span></span><br><span class="line"><span class="params"><span class="function">	serviceInformer coreinformers.ServiceInformer,</span></span></span><br><span class="line"><span class="params"><span class="function">	nodeInformer coreinformers.NodeInformer,</span></span></span><br><span class="line"><span class="params"><span class="function">	clusterName <span class="type">string</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	featureGate featuregate.FeatureGate,</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span> (*Controller, <span class="type">error</span>) &#123;</span><br><span class="line">	broadcaster := record.NewBroadcaster()</span><br><span class="line">	broadcaster.StartStructuredLogging(<span class="number">0</span>)</span><br><span class="line">	broadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeClient.CoreV1().Events(<span class="string">&quot;&quot;</span>)&#125;)</span><br><span class="line">	recorder := broadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: <span class="string">&quot;service-controller&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> kubeClient != <span class="literal">nil</span> &amp;&amp; kubeClient.CoreV1().RESTClient().GetRateLimiter() != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">if</span> err := ratelimiter.RegisterMetricAndTrackRateLimiterUsage(subSystemName, kubeClient.CoreV1().RESTClient().GetRateLimiter()); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	registerMetrics()</span><br><span class="line">	s := &amp;Controller&#123;</span><br><span class="line">		cloud:            cloud,</span><br><span class="line">		knownHosts:       []*v1.Node&#123;&#125;,</span><br><span class="line">		kubeClient:       kubeClient,</span><br><span class="line">		clusterName:      clusterName,</span><br><span class="line">		cache:            &amp;serviceCache&#123;serviceMap: <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="type">string</span>]*cachedService)&#125;,</span><br><span class="line">		eventBroadcaster: broadcaster,</span><br><span class="line">		eventRecorder:    recorder,</span><br><span class="line">		nodeLister:       nodeInformer.Lister(),</span><br><span class="line">		nodeListerSynced: nodeInformer.Informer().HasSynced,</span><br><span class="line">		queue:            workqueue.NewNamedRateLimitingQueue(workqueue.NewItemExponentialFailureRateLimiter(minRetryDelay, maxRetryDelay), <span class="string">&quot;service&quot;</span>),</span><br><span class="line">		<span class="comment">// nodeSyncCh has a size 1 buffer. Only one pending sync signal would be cached.</span></span><br><span class="line">		nodeSyncCh: <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">interface</span>&#123;&#125;, <span class="number">1</span>),</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	serviceInformer.Informer().AddEventHandlerWithResyncPeriod(</span><br><span class="line">		cache.ResourceEventHandlerFuncs&#123;</span><br><span class="line">            <span class="comment">// 添加事件回调函数</span></span><br><span class="line">			AddFunc: <span class="function"><span class="keyword">func</span><span class="params">(cur <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">				svc, ok := cur.(*v1.Service)</span><br><span class="line">				<span class="comment">// Check cleanup here can provide a remedy when controller failed to handle</span></span><br><span class="line">				<span class="comment">// changes before it exiting (e.g. crashing, restart, etc.).</span></span><br><span class="line">				<span class="keyword">if</span> ok &amp;&amp; (wantsLoadBalancer(svc) || needsCleanup(svc)) &#123;</span><br><span class="line">					s.enqueueService(cur)</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;,</span><br><span class="line">            <span class="comment">// 更新事件回调函数</span></span><br><span class="line">			UpdateFunc: <span class="function"><span class="keyword">func</span><span class="params">(old, cur <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">				oldSvc, ok1 := old.(*v1.Service)</span><br><span class="line">				curSvc, ok2 := cur.(*v1.Service)</span><br><span class="line">				<span class="keyword">if</span> ok1 &amp;&amp; ok2 &amp;&amp; (s.needsUpdate(oldSvc, curSvc) || needsCleanup(curSvc)) &#123;</span><br><span class="line">					s.enqueueService(cur)</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;,</span><br><span class="line">			<span class="comment">// No need to handle deletion event because the deletion would be handled by</span></span><br><span class="line">			<span class="comment">// the update path when the deletion timestamp is added.</span></span><br><span class="line">		&#125;,</span><br><span class="line">		serviceSyncPeriod,</span><br><span class="line">	)</span><br><span class="line">	s.serviceLister = serviceInformer.Lister()</span><br><span class="line">	s.serviceListerSynced = serviceInformer.Informer().HasSynced</span><br><span class="line"></span><br><span class="line">	...</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> err := s.init(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> s, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LoadBalance类型</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">wantsLoadBalancer</span><span class="params">(service *v1.Service)</span></span> <span class="type">bool</span> &#123;</span><br><span class="line">	<span class="comment">// if LoadBalancerClass is set, the user does not want the default cloud-provider Load Balancer</span></span><br><span class="line">	<span class="keyword">return</span> service.Spec.Type == v1.ServiceTypeLoadBalancer &amp;&amp; service.Spec.LoadBalancerClass == <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，在Cloud Provider中的serviceInformer只关注了LoadBalancer类型的Service的动作事件。当相对应的事件入队时，控制循环会执行相应的调谐逻辑。</p>
<p>客户端到我们的业务实例请求流程如下：<br><img src="/use-loadbalancer-in-self-built-k8s-cluster/loadbanlance-arch.png"></p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>本文简单介绍了暴露服务的主要几种方式，并着重介绍了Cloud Provider，并通过AWS的Cloud Provider创建了一个LoadBalancer。当然还有以下要注意的点：</p>
<ul>
<li>LoadBalancer是Kubernetes在NodePort类型Service上的功能加成，他们的基础均是ClusterIP</li>
<li>公有云上的容器服务一般都会使用自家的out-of-tree的cloud-controller-manager组件</li>
<li>微服务架构下，每个服务使用都使用一个LoadBalancer反而带来了额外的开销和管理成本，可以搭配Ingress等7层LoadBalancer一起使用</li>
</ul>
]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
</search>
